# ü§ñ DontREADME - Guide LangChain & RAG

> *Document technique centr√© sur l'architecture LangChain, la base vectorielle et les patterns RAG*

## üèóÔ∏è Architecture LangChain

### Vue d'Ensemble RAG (Retrieval-Augmented Generation)

```mermaid
graph TD
    A[Document Upload] --> B[Text Extraction]
    B --> C[Smart Text Splitting]
    C --> D[HuggingFace Embeddings]
    D --> E[ChromaDB Vector Store]
    
    F[User Query] --> G[Query Embedding]
    G --> H[Similarity Search]
    H --> I[Context Retrieval]
    I --> J[Prompt Template]
    J --> K[Mistral AI LLM]
    K --> L[Enhanced Response]
    
    E --> H
```

## üîß Composants LangChain Utilis√©s

### 1. **Embeddings & Vector Store**

```python
# Architecture embeddings avec fallback
class HuggingFaceAPIEmbeddings:
    """Embeddings via API HuggingFace (sans installation locale)"""
    
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2"):
        self.api_url = f"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_name}"
        self.dimension = 384
    
    def embed_documents(self, texts: List[str]):
        # Appel API avec retry et fallback
        return self._call_api(texts)
    
    def embed_query(self, text: str):
        # Embedding pour requ√™te utilisateur
        return self._call_api([text])[0]

# Int√©gration ChromaDB via LangChain
from langchain_chroma import Chroma

vectorstore = Chroma(
    client=chromadb_client,
    collection_name="documents",
    embedding_function=embeddings
)
```

### 2. **Text Splitting Intelligent**

```python
class SmartTextSplitter:
    """D√©coupage contextuel selon le type de document"""
    
    def split_documents_with_metadata(self, text, filename, chunk_size, chunk_overlap):
        # 1. D√©tection automatique du type de document
        doc_type = self._detect_document_type(text)
        
        # 2. S√©parateurs optimis√©s par type
        separators = self._get_optimized_separators(doc_type)
        
        # 3. D√©coupage avec pr√©servation de structure
        chunks = self._smart_split(text, separators, chunk_size, chunk_overlap)
        
        # 4. Enrichissement des m√©tadonn√©es
        documents = []
        for i, chunk in enumerate(chunks):
            metadata = {
                "filename": filename,
                "chunk_id": i,
                "document_type": doc_type,
                "keywords": self._extract_keywords(chunk),
                "chunk_position": f"{i+1}/{len(chunks)}",
                "contains_structure": self._has_structure(chunk)
            }
            documents.append(Document(page_content=chunk, metadata=metadata))
        
        return documents
```

### 3. **Prompt Templates Adaptatifs**

```python
class PromptTemplateManager:
    """Gestion des templates selon le contexte"""
    
    def __init__(self):
        self.templates = {
            "general": self._create_general_template(),
            "academic": self._create_academic_template(),
            "technical": self._create_technical_template(),
            "legal": self._create_legal_template()
        }
    
    def get_optimized_template(self, document_type, query_type):
        """S√©lection automatique du meilleur template"""
        template_key = self._select_best_template(document_type, query_type)
        return self.templates[template_key]
    
    def _create_academic_template(self):
        return PromptTemplate(
            input_variables=["context", "question", "chat_history"],
            template="""
            Vous √™tes un assistant sp√©cialis√© dans l'analyse de documents acad√©miques.
            
            Contexte acad√©mique:
            {context}
            
            Historique: {chat_history}
            
            Question: {question}
            
            Instructions:
            - Analysez rigoureusement les sources acad√©miques
            - Citez les r√©f√©rences pr√©cises
            - Identifiez les m√©thodologies utilis√©es
            - Mentionnez les limitations √©ventuelles
            
            R√©ponse structur√©e:
            """
        )
```

### 4. **ConversationalRetrievalChain**

```python
class EnhancedChatEngine:
    """Moteur de conversation avec RAG avanc√©"""
    
    def setup_chain_with_template(self, k_documents=3, template_type='auto'):
        # 1. D√©tection automatique du template optimal
        if template_type == 'auto':
            sample_docs = self.vectorstore_manager.search_with_metadata("test", k=1)
            document_type = sample_docs[0]['metadata'].get('document_type', 'general')
            template_type = document_type
        
        # 2. Template contextualis√©
        prompt_template = self.prompt_manager.get_optimized_template(
            document_type=template_type,
            query_type='general'
        )
        
        # 3. M√©moire de conversation
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # 4. Cha√Æne RAG compl√®te
        self.chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vectorstore_manager.get_retriever(k=k_documents),
            memory=memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": prompt_template}
        )
    
    def process_question_enhanced(self, question: str):
        """Traitement avanc√© avec m√©tadonn√©es enrichies"""
        # 1. Validation et nettoyage
        question = InputValidator.sanitize_input(question)
        
        # 2. Ex√©cution de la cha√Æne RAG
        result = self.chain({"question": question})
        
        # 3. Extraction enrichie des sources
        sources = []
        source_metadata = []
        
        for doc in result.get("source_documents", []):
            filename = doc.metadata.get("filename", "Document")
            chunk_id = doc.metadata.get("chunk_id", 0)
            keywords = doc.metadata.get("keywords", [])
            
            sources.append(f"{filename} (section {chunk_id + 1})")
            source_metadata.append({
                "filename": filename,
                "chunk_id": chunk_id,
                "keywords": keywords,
                "relevance_preview": doc.page_content[:100] + "..."
            })
        
        # 4. M√©tadonn√©es de r√©ponse
        response_metadata = {
            "template_used": self.current_template_type,
            "sources_count": len(sources),
            "source_details": source_metadata,
            "performance": self.performance_monitor.get_metrics_summary()
        }
        
        return result["answer"], sources, response_metadata
```

## üóÑÔ∏è Base Vectorielle ChromaDB

### Configuration Avanc√©e

```python
class EnhancedVectorStoreManager:
    """Gestionnaire ChromaDB avec fonctionnalit√©s avanc√©es"""
    
    def initialize_vectorstore(self, collection_name="documents"):
        # Configuration ChromaDB persistante
        client = chromadb.PersistentClient(path="./data/vectorstore")
        
        # Collection avec m√©tadonn√©es
        self.vectorstore = Chroma(
            client=client,
            collection_name=collection_name,
            embedding_function=self.embeddings
        )
    
    def add_documents_enhanced(self, text, filename, chunk_size, chunk_overlap):
        """Ajout intelligent avec enrichissement m√©tadonn√©es"""
        
        # 1. D√©coupage intelligent
        documents = self.text_splitter.split_documents_with_metadata(
            text, filename, chunk_size, chunk_overlap
        )
        
        # 2. Filtrage m√©tadonn√©es pour ChromaDB
        filtered_documents = self._filter_complex_metadata(documents)
        
        # 3. Ajout √† la base vectorielle
        self.vectorstore.add_documents(filtered_documents)
        
        # 4. Informations de traitement
        doc_info = {
            "total_chunks": len(documents),
            "document_type": documents[0].metadata.get("document_type"),
            "average_chunk_size": sum(len(doc.page_content) for doc in documents) / len(documents),
            "keywords_extracted": any("keywords" in doc.metadata for doc in documents)
        }
        
        return len(documents), doc_info
    
    def search_with_metadata(self, query: str, k: int = 3):
        """Recherche avec m√©tadonn√©es enrichies"""
        documents = self.vectorstore.similarity_search(query, k=k)
        
        enriched_results = []
        for doc in documents:
            result = {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "chunk_info": {
                    "position": doc.metadata.get("chunk_position"),
                    "keywords": doc.metadata.get("keywords", []),
                    "document_type": doc.metadata.get("document_type")
                }
            }
            enriched_results.append(result)
        
        return enriched_results
```

### Gestion des M√©tadonn√©es ChromaDB

```python
def _filter_complex_metadata(self, documents):
    """ChromaDB n'accepte que les types simples (str, int, float, bool, None)"""
    filtered_docs = []
    
    for doc in documents:
        filtered_metadata = {}
        for key, value in doc.metadata.items():
            if isinstance(value, (str, int, float, bool)) or value is None:
                filtered_metadata[key] = value
            elif isinstance(value, list):
                # Convertir listes en string
                filtered_metadata[key] = ', '.join(map(str, value))
            elif isinstance(value, dict):
                # Convertir dicts en JSON string
                filtered_metadata[key] = json.dumps(value)
            else:
                filtered_metadata[key] = str(value)
        
        filtered_docs.append(Document(
            page_content=doc.page_content,
            metadata=filtered_metadata
        ))
    
    return filtered_docs
```

## ü§ñ Patterns d'Agents (Non Impl√©ment√©s)

> *Note: DontREADME utilise une approche RAG classique, mais voici comment int√©grer des agents*

### Agent Conceptuel avec LangGraph

```python
# Exemple d'architecture d'agent pour extension future
from langgraph import StateGraph, END

class DocumentAnalysisAgent:
    """Agent autonome pour analyse documentaire avanc√©e"""
    
    def __init__(self):
        self.graph = self._create_agent_graph()
    
    def _create_agent_graph(self):
        workflow = StateGraph({
            "documents": list,
            "query": str,
            "analysis_type": str,
            "results": dict
        })
        
        # N≈ìuds d'agent
        workflow.add_node("document_classifier", self._classify_documents)
        workflow.add_node("context_retriever", self._retrieve_context)
        workflow.add_node("answer_generator", self._generate_answer)
        workflow.add_node("quality_checker", self._check_quality)
        
        # Flux conditionnel
        workflow.add_edge("document_classifier", "context_retriever")
        workflow.add_edge("context_retriever", "answer_generator")
        
        workflow.add_conditional_edges(
            "answer_generator",
            self._should_improve,
            {
                "improve": "quality_checker",
                "done": END
            }
        )
        
        workflow.set_entry_point("document_classifier")
        return workflow.compile()
    
    def _classify_documents(self, state):
        """Classification automatique des documents"""
        # Analyse du type et complexit√©
        return {"analysis_type": "technical"}
    
    def _retrieve_context(self, state):
        """R√©cup√©ration contextuelle avanc√©e"""
        # Recherche multi-√©tapes avec reformulation
        return {"context": "enhanced_context"}
    
    def _should_improve(self, state):
        """D√©cision d'am√©lioration"""
        confidence = state.get("confidence", 0.8)
        return "improve" if confidence < 0.9 else "done"
```

### Fonctions d'Agent (Tools)

```python
# Outils pour agents futurs
from langchain.tools import Tool

class DocumentTools:
    """Outils sp√©cialis√©s pour agents documentaires"""
    
    @staticmethod
    def create_summary_tool():
        return Tool(
            name="document_summarizer",
            description="R√©sume un document selon sa complexit√©",
            func=lambda doc: DocumentTools._smart_summarize(doc)
        )
    
    @staticmethod
    def create_extraction_tool():
        return Tool(
            name="entity_extractor", 
            description="Extrait entit√©s et concepts cl√©s",
            func=lambda text: DocumentTools._extract_entities(text)
        )
    
    @staticmethod
    def create_validation_tool():
        return Tool(
            name="fact_checker",
            description="V√©rifie la coh√©rence des informations",
            func=lambda claims: DocumentTools._validate_facts(claims)
        )
```

## üìä M√©triques et Monitoring

### Performance RAG

```python
class RAGMetrics:
    """M√©triques sp√©cifiques au RAG"""
    
    def __init__(self):
        self.metrics = {
            "retrieval_precision": [],
            "answer_relevance": [],
            "source_diversity": [],
            "response_time": []
        }
    
    def measure_retrieval_quality(self, query, retrieved_docs, expected_topics):
        """Mesure la qualit√© de r√©cup√©ration"""
        precision = self._calculate_precision(retrieved_docs, expected_topics)
        diversity = self._calculate_diversity(retrieved_docs)
        
        self.metrics["retrieval_precision"].append(precision)
        self.metrics["source_diversity"].append(diversity)
    
    def evaluate_answer_relevance(self, question, answer, context):
        """√âvalue la pertinence de la r√©ponse"""
        relevance = self._semantic_similarity(question, answer)
        context_usage = self._context_utilization(answer, context)
        
        return {
            "relevance_score": relevance,
            "context_usage": context_usage,
            "answer_length": len(answer.split())
        }
```

## üéØ Optimisations Impl√©ment√©es

### 1. **Chunking Adaptatif**
- S√©parateurs optimis√©s par type de document
- Pr√©servation de la structure logique
- M√©tadonn√©es enrichies automatiquement

### 2. **Templates Contextuels**
- S√©lection automatique selon le domaine
- Instructions sp√©cialis√©es par type de requ√™te
- Formatage adapt√© au contenu

### 3. **Embeddings Robustes**
- API HuggingFace avec fallback
- Gestion des timeouts et retry
- Compatibilit√© sans installation locale

### 4. **M√©moire Intelligente**
- R√©initialisation automatique entre documents
- Historique contextuel pour la conversation
- Gestion des m√©tadonn√©es de session

## üîÆ Extensions Futures Possibles

### LangGraph Integration
```python
# Architecture d'agent multi-√©tapes
workflow = StateGraph(DocumentAnalysisState)
workflow.add_node("analyzer", DocumentAnalyzer())
workflow.add_node("synthesizer", ContentSynthesizer()) 
workflow.add_node("validator", ResponseValidator())
# ... flux conditionnel avanc√©
```

### Multi-Modal RAG
```python
# Support images et tableaux
class MultiModalProcessor:
    def process_document(self, file_path):
        if file_path.endswith('.pdf'):
            text, images, tables = self._extract_all_content(file_path)
            return self._create_multimodal_chunks(text, images, tables)
```

### RAG H√≠brido
```python
# Combinaison dense + sparse retrieval
class HybridRetriever:
    def __init__(self):
        self.dense_retriever = ChromaRetriever()
        self.sparse_retriever = BM25Retriever()
    
    def retrieve(self, query, k=5):
        dense_docs = self.dense_retriever.get_relevant_documents(query)
        sparse_docs = self.sparse_retriever.get_relevant_documents(query)
        return self._rerank_and_merge(dense_docs, sparse_docs, k)
```

---

## üìù Conclusion Technique

**DontREADME** impl√©mente un **RAG moderne et robuste** avec LangChain, centr√© sur :

‚úÖ **Architecture modulaire** avec composants sp√©cialis√©s  
‚úÖ **Embeddings adaptatifs** via API externe  
‚úÖ **Chunking intelligent** selon le contexte  
‚úÖ **Templates optimis√©s** par domaine  
‚úÖ **Base vectorielle persistante** ChromaDB  
‚úÖ **Monitoring int√©gr√©** des performances  

Le syst√®me est **pr√™t pour extensions** vers des architectures d'agents plus complexes avec LangGraph, tout en maintenant une base RAG solide et performante.

---