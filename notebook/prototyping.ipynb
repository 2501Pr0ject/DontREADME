{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DontREADME**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture du Projet ChatBot Documentaire\n",
    "\n",
    "## üèóÔ∏è Structure des dossiers\n",
    "\n",
    "```\n",
    "document-chatbot/\n",
    "‚îú‚îÄ‚îÄ üìÅ app/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # Interface Gradio principale\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Configuration et constantes\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ components/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ file_processor.py      # Traitement des fichiers (PDF, DOCX, TXT)\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ vectorstore.py         # Gestion ChromaDB et embeddings\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ chat_engine.py         # Logique de conversation et RAG\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ memory.py              # Gestion de l'historique\n",
    "‚îú‚îÄ‚îÄ üìÅ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ uploads/                   # Fichiers upload√©s temporaires\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ vectorstore/               # Base ChromaDB persistante\n",
    "‚îú‚îÄ‚îÄ üìÅ utils/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ text_splitter.py          # D√©coupage intelligent du texte\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ prompt_templates.py       # Templates de prompts pour le LLM\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ .env.example\n",
    "‚îú‚îÄ‚îÄ .gitignore\n",
    "‚îî‚îÄ‚îÄ README.md\n",
    "```\n",
    "\n",
    "## üîß Architecture technique\n",
    "\n",
    "### 1. Interface Utilisateur (Gradio)\n",
    "```python\n",
    "# Structure de l'interface\n",
    "Interface Gradio:\n",
    "‚îú‚îÄ‚îÄ Onglet \"Configuration\"\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Input: Cl√© API OpenAI/Mistral\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ File Upload: PDF/DOCX/TXT\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Slider: Chunk Size (100-2000)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Slider: Nombre de documents K (1-10)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Button: \"Traiter le document\"\n",
    "‚îú‚îÄ‚îÄ Onglet \"Chat\"\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Chatbot: Historique des conversations\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Textbox: Question utilisateur\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Button: \"Envoyer\"\n",
    "‚îî‚îÄ‚îÄ Onglet \"Informations\"\n",
    "    ‚îú‚îÄ‚îÄ Display: Statut du traitement\n",
    "    ‚îú‚îÄ‚îÄ Display: Nombre de chunks cr√©√©s\n",
    "    ‚îî‚îÄ‚îÄ Display: Mod√®le utilis√©\n",
    "```\n",
    "\n",
    "### 2. Pipeline de traitement\n",
    "```mermaid\n",
    "graph TD\n",
    "A[Upload Fichier] --> B[Extraction Texte]\n",
    "B --> C[D√©coupage en Chunks]\n",
    "C --> D[G√©n√©ration Embeddings]\n",
    "D --> E[Stockage ChromaDB]\n",
    "E --> F[Pr√™t pour Questions]\n",
    "\n",
    "G[Question Utilisateur] --> H[Recherche Similarit√©]\n",
    "H --> I[R√©cup√©ration Contexte]\n",
    "I --> J[G√©n√©ration R√©ponse LLM]\n",
    "J --> K[Mise √† jour Historique]\n",
    "```\n",
    "\n",
    "## üß© Composants principaux\n",
    "\n",
    "### 1. **FileProcessor** (`file_processor.py`)\n",
    "- **Responsabilit√©** : Extraction de texte des fichiers\n",
    "- **Formats support√©s** : PDF (PyPDF2), DOCX (python-docx), TXT\n",
    "- **Fonctions** :\n",
    "  - `extract_text_from_pdf()`\n",
    "  - `extract_text_from_docx()`\n",
    "  - `extract_text_from_txt()`\n",
    "  - `process_uploaded_file()`\n",
    "\n",
    "### 2. **VectorStore** (`vectorstore.py`)\n",
    "- **Responsabilit√©** : Gestion des embeddings et ChromaDB\n",
    "- **Fonctions** :\n",
    "  - `initialize_vectorstore()`\n",
    "  - `add_documents_to_vectorstore()`\n",
    "  - `search_similar_documents()`\n",
    "  - `clear_vectorstore()`\n",
    "\n",
    "### 3. **ChatEngine** (`chat_engine.py`)\n",
    "- **Responsabilit√©** : Logique RAG et interaction LLM\n",
    "- **Fonctions** :\n",
    "  - `setup_retrieval_chain()`\n",
    "  - `process_question()`\n",
    "  - `generate_response()`\n",
    "  - `format_context()`\n",
    "\n",
    "### 4. **Memory** (`memory.py`)\n",
    "- **Responsabilit√©** : Historique des conversations\n",
    "- **Fonctions** :\n",
    "  - `add_message()`\n",
    "  - `get_conversation_history()`\n",
    "  - `clear_history()`\n",
    "  - `format_history_for_display()`\n",
    "\n",
    "## üîÑ Flow de donn√©es\n",
    "\n",
    "### √âtape 1: Initialisation\n",
    "1. L'utilisateur saisit sa cl√© API\n",
    "2. Configure les param√®tres (chunk_size, k)\n",
    "3. Upload un document\n",
    "\n",
    "### √âtape 2: Traitement du document\n",
    "1. **FileProcessor** extrait le texte\n",
    "2. **TextSplitter** d√©coupe en chunks\n",
    "3. **VectorStore** g√©n√®re les embeddings via OpenAI/Mistral\n",
    "4. Stockage dans ChromaDB\n",
    "\n",
    "### √âtape 3: Conversation\n",
    "1. L'utilisateur pose une question\n",
    "2. **VectorStore** recherche les chunks pertinents\n",
    "3. **ChatEngine** combine contexte + historique + question\n",
    "4. LLM g√©n√®re la r√©ponse\n",
    "5. **Memory** sauvegarde l'√©change\n",
    "\n",
    "## üõ†Ô∏è Stack technique\n",
    "\n",
    "- **Interface** : Gradio (plus simple que Streamlit pour ce cas)\n",
    "- **LLM Framework** : LangChain\n",
    "- **Vector Database** : ChromaDB (local, pas besoin de serveur)\n",
    "- **LLM** : OpenAI GPT-3.5-turbo (ou Mistral AI en alternative gratuite)\n",
    "- **Embeddings** : OpenAI text-embedding-ada-002 (ou sentence-transformers gratuit)\n",
    "- **Document Processing** : PyPDF2, python-docx\n",
    "- **Text Splitting** : LangChain RecursiveCharacterTextSplitter\n",
    "\n",
    "## üì¶ D√©pendances principales\n",
    "\n",
    "```txt\n",
    "gradio>=4.0.0\n",
    "langchain>=0.1.0\n",
    "langchain-openai\n",
    "langchain-community\n",
    "chromadb>=0.4.0\n",
    "openai>=1.0.0\n",
    "PyPDF2>=3.0.0\n",
    "python-docx>=0.8.11\n",
    "sentence-transformers  # Alternative gratuite aux embeddings OpenAI\n",
    "python-dotenv\n",
    "```\n",
    "\n",
    "## üéØ Fonctionnalit√©s avanc√©es (optionnelles)\n",
    "\n",
    "- **Multi-documents** : Traiter plusieurs fichiers simultan√©ment\n",
    "- **Export conversation** : Sauvegarder l'historique en JSON/PDF\n",
    "- **M√©triques** : Temps de r√©ponse, similarit√© des chunks\n",
    "- **Streaming** : R√©ponses en temps r√©el\n",
    "- **Templates personnalis√©s** : Diff√©rents styles de r√©ponse\n",
    "\n",
    "Cette architecture modulaire permet une maintenance facile et des extensions futures. Voulez-vous que je commence par impl√©menter un composant sp√©cifique ou pr√©f√©rez-vous que je cr√©e l'application compl√®te ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt\n",
    "\"\"\"\n",
    "gradio>=4.0.0\n",
    "langchain>=0.1.0\n",
    "langchain-community\n",
    "langchain-mistralai\n",
    "chromadb>=0.4.0\n",
    "mistralai>=0.1.0\n",
    "PyPDF2>=3.0.0\n",
    "python-docx>=0.8.11\n",
    "sentence-transformers>=2.2.0\n",
    "python-dotenv>=1.0.0\n",
    "tiktoken>=0.5.0\n",
    "\"\"\"\n",
    "\n",
    "# app/config.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class Config:\n",
    "    # Mistral AI Configuration\n",
    "    MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"\")\n",
    "    MISTRAL_MODEL = \"mistral-tiny\"  # ou \"mistral-small\", \"mistral-medium\"\n",
    "    \n",
    "    # ChromaDB Configuration\n",
    "    CHROMADB_PATH = \"./data/vectorstore\"\n",
    "    COLLECTION_NAME = \"document_embeddings\"\n",
    "    \n",
    "    # Text Processing\n",
    "    DEFAULT_CHUNK_SIZE = 1000\n",
    "    DEFAULT_CHUNK_OVERLAP = 200\n",
    "    DEFAULT_K_DOCUMENTS = 3\n",
    "    \n",
    "    # Supported file types\n",
    "    SUPPORTED_EXTENSIONS = ['.pdf', '.docx', '.txt']\n",
    "    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\n",
    "\n",
    "# app/components/file_processor.py\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "\n",
    "class FileProcessor:\n",
    "    \"\"\"Traitement des fichiers upload√©s\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_pdf(file_path: str) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier PDF\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text.strip()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erreur lors de la lecture du PDF: {str(e)}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_docx(file_path: str) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier DOCX\"\"\"\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            text = \"\"\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erreur lors de la lecture du DOCX: {str(e)}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_txt(file_path: str) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier TXT\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                return file.read().strip()\n",
    "        except UnicodeDecodeError:\n",
    "            # Essayer avec d'autres encodages\n",
    "            for encoding in ['latin-1', 'cp1252', 'iso-8859-1']:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding=encoding) as file:\n",
    "                        return file.read().strip()\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            raise Exception(\"Impossible de d√©coder le fichier texte\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erreur lors de la lecture du TXT: {str(e)}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def process_uploaded_file(cls, file_obj) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Traite un fichier upload√© et retourne le texte extrait\n",
    "        Returns: (text_content, filename)\n",
    "        \"\"\"\n",
    "        if file_obj is None:\n",
    "            raise ValueError(\"Aucun fichier fourni\")\n",
    "        \n",
    "        # Sauvegarder temporairement le fichier\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file_obj.name).suffix) as tmp_file:\n",
    "            tmp_file.write(file_obj.read() if hasattr(file_obj, 'read') else file_obj)\n",
    "            tmp_path = tmp_file.name\n",
    "        \n",
    "        try:\n",
    "            file_extension = Path(file_obj.name).suffix.lower()\n",
    "            filename = Path(file_obj.name).name\n",
    "            \n",
    "            if file_extension == '.pdf':\n",
    "                text = cls.extract_text_from_pdf(tmp_path)\n",
    "            elif file_extension == '.docx':\n",
    "                text = cls.extract_text_from_docx(tmp_path)\n",
    "            elif file_extension == '.txt':\n",
    "                text = cls.extract_text_from_txt(tmp_path)\n",
    "            else:\n",
    "                raise ValueError(f\"Format de fichier non support√©: {file_extension}\")\n",
    "            \n",
    "            if not text.strip():\n",
    "                raise ValueError(\"Le fichier ne contient pas de texte extractible\")\n",
    "            \n",
    "            return text, filename\n",
    "            \n",
    "        finally:\n",
    "            # Nettoyer le fichier temporaire\n",
    "            if os.path.exists(tmp_path):\n",
    "                os.unlink(tmp_path)\n",
    "\n",
    "# app/components/vectorstore.py\n",
    "import os\n",
    "from typing import List, Optional\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "from app.config import Config\n",
    "\n",
    "class VectorStoreManager:\n",
    "    \"\"\"Gestion de la base de donn√©es vectorielle ChromaDB\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = None\n",
    "        self.vectorstore = None\n",
    "        self.text_splitter = None\n",
    "        self._setup_embeddings()\n",
    "    \n",
    "    def _setup_embeddings(self):\n",
    "        \"\"\"Configure les embeddings avec sentence-transformers (gratuit)\"\"\"\n",
    "        # Utilise un mod√®le multilingue fran√ßais/anglais\n",
    "        model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={'device': 'cpu'},  # Utilise CPU pour la compatibilit√©\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "    \n",
    "    def setup_text_splitter(self, chunk_size: int = Config.DEFAULT_CHUNK_SIZE, \n",
    "                           chunk_overlap: int = Config.DEFAULT_CHUNK_OVERLAP):\n",
    "        \"\"\"Configure le d√©coupeur de texte\"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def initialize_vectorstore(self, collection_name: str = Config.COLLECTION_NAME):\n",
    "        \"\"\"Initialise la base vectorielle ChromaDB\"\"\"\n",
    "        # Cr√©er le dossier s'il n'existe pas\n",
    "        os.makedirs(Config.CHROMADB_PATH, exist_ok=True)\n",
    "        \n",
    "        # Configuration ChromaDB\n",
    "        client = chromadb.PersistentClient(path=Config.CHROMADB_PATH)\n",
    "        \n",
    "        # Supprimer la collection existante si elle existe\n",
    "        try:\n",
    "            client.delete_collection(collection_name)\n",
    "        except ValueError:\n",
    "            pass  # Collection n'existe pas\n",
    "        \n",
    "        self.vectorstore = Chroma(\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=self.embeddings,\n",
    "        )\n",
    "    \n",
    "    def add_documents(self, text: str, filename: str, chunk_size: int, chunk_overlap: int) -> int:\n",
    "        \"\"\"\n",
    "        Ajoute des documents √† la base vectorielle\n",
    "        Returns: nombre de chunks cr√©√©s\n",
    "        \"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"VectorStore non initialis√©\")\n",
    "        \n",
    "        # Configuration du text splitter\n",
    "        self.setup_text_splitter(chunk_size, chunk_overlap)\n",
    "        \n",
    "        # D√©coupage du texte\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        # Cr√©ation des documents avec m√©tadonn√©es\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"filename\": filename,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"total_chunks\": len(chunks)\n",
    "                }\n",
    "            )\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "        \n",
    "        # Ajout √† la base vectorielle\n",
    "        self.vectorstore.add_documents(documents)\n",
    "        \n",
    "        return len(chunks)\n",
    "    \n",
    "    def search_similar_documents(self, query: str, k: int = Config.DEFAULT_K_DOCUMENTS) -> List[Document]:\n",
    "        \"\"\"Recherche les documents similaires √† la requ√™te\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return []\n",
    "        \n",
    "        return self.vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    def get_retriever(self, k: int = Config.DEFAULT_K_DOCUMENTS):\n",
    "        \"\"\"Retourne un retriever pour LangChain\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"VectorStore non initialis√©\")\n",
    "        \n",
    "        return self.vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# app/components/memory.py\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "class ConversationMemory:\n",
    "    \"\"\"Gestion de l'historique des conversations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversation_history: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def add_exchange(self, question: str, answer: str, sources: List[str] = None):\n",
    "        \"\"\"Ajoute un √©change question-r√©ponse √† l'historique\"\"\"\n",
    "        exchange = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources or []\n",
    "        }\n",
    "        self.conversation_history.append(exchange)\n",
    "    \n",
    "    def get_recent_history(self, n_exchanges: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Retourne l'historique r√©cent format√© pour le contexte LLM\n",
    "        \"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return \"\"\n",
    "        \n",
    "        recent = self.conversation_history[-n_exchanges:]\n",
    "        history_text = \"Historique de la conversation:\\n\"\n",
    "        \n",
    "        for exchange in recent:\n",
    "            history_text += f\"Q: {exchange['question']}\\n\"\n",
    "            history_text += f\"R: {exchange['answer']}\\n\\n\"\n",
    "        \n",
    "        return history_text\n",
    "    \n",
    "    def get_formatted_history(self) -> List[List[str]]:\n",
    "        \"\"\"Retourne l'historique format√© pour l'affichage Gradio\"\"\"\n",
    "        formatted = []\n",
    "        for exchange in self.conversation_history:\n",
    "            formatted.append([exchange[\"question\"], exchange[\"answer\"]])\n",
    "        return formatted\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Efface l'historique\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# app/components/chat_engine.py\n",
    "from typing import Optional, Tuple, List\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from app.components.vectorstore import VectorStoreManager\n",
    "from app.components.memory import ConversationMemory\n",
    "\n",
    "class ChatEngine:\n",
    "    \"\"\"Moteur de conversation avec RAG\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.vectorstore_manager = VectorStoreManager()\n",
    "        self.memory = ConversationMemory()\n",
    "        self.chain = None\n",
    "        \n",
    "        # Template de prompt en fran√ßais\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "            template=\"\"\"Tu es un assistant IA sp√©cialis√© dans l'analyse de documents. \n",
    "R√©ponds aux questions en te basant UNIQUEMENT sur le contexte fourni.\n",
    "\n",
    "Contexte des documents:\n",
    "{context}\n",
    "\n",
    "Historique de la conversation:\n",
    "{chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- R√©ponds de mani√®re pr√©cise et concise\n",
    "- Si l'information n'est pas dans le contexte, dis clairement \"Je ne trouve pas cette information dans le document fourni\"\n",
    "- Cite des passages sp√©cifiques quand c'est pertinent\n",
    "- Reste factuel et objectif\n",
    "\n",
    "R√©ponse:\"\"\"\n",
    "        )\n",
    "    \n",
    "    def setup_llm(self, api_key: str, model: str = \"mistral-tiny\"):\n",
    "        \"\"\"Configure le mod√®le Mistral AI\"\"\"\n",
    "        if not api_key:\n",
    "            raise ValueError(\"Cl√© API Mistral requise\")\n",
    "        \n",
    "        self.llm = ChatMistralAI(\n",
    "            mistral_api_key=api_key,\n",
    "            model=model,\n",
    "            temperature=0.1,  # R√©ponses plus d√©terministes\n",
    "            max_tokens=1000\n",
    "        )\n",
    "    \n",
    "    def setup_chain(self, k_documents: int = 3):\n",
    "        \"\"\"Configure la cha√Æne de traitement RAG\"\"\"\n",
    "        if not self.llm:\n",
    "            raise ValueError(\"LLM non configur√©\")\n",
    "        \n",
    "        if not self.vectorstore_manager.vectorstore:\n",
    "            raise ValueError(\"VectorStore non initialis√©\")\n",
    "        \n",
    "        # M√©moire pour la conversation\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            output_key=\"answer\"\n",
    "        )\n",
    "        \n",
    "        # Cha√Æne conversationnelle avec retrieval\n",
    "        self.chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=self.llm,\n",
    "            retriever=self.vectorstore_manager.get_retriever(k=k_documents),\n",
    "            memory=memory,\n",
    "            return_source_documents=True,\n",
    "            combine_docs_chain_kwargs={\"prompt\": self.prompt_template}\n",
    "        )\n",
    "    \n",
    "    def process_question(self, question: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Traite une question et retourne la r√©ponse avec les sources\n",
    "        Returns: (answer, sources)\n",
    "        \"\"\"\n",
    "        if not self.chain:\n",
    "            return \"Erreur: Syst√®me non configur√©\", []\n",
    "        \n",
    "        if not question.strip():\n",
    "            return \"Veuillez poser une question.\", []\n",
    "        \n",
    "        try:\n",
    "            # Ex√©cution de la cha√Æne\n",
    "            result = self.chain({\"question\": question})\n",
    "            \n",
    "            answer = result.get(\"answer\", \"Pas de r√©ponse g√©n√©r√©e\")\n",
    "            \n",
    "            # Extraction des sources\n",
    "            sources = []\n",
    "            if \"source_documents\" in result:\n",
    "                for doc in result[\"source_documents\"]:\n",
    "                    filename = doc.metadata.get(\"filename\", \"Document\")\n",
    "                    chunk_id = doc.metadata.get(\"chunk_id\", 0)\n",
    "                    sources.append(f\"{filename} (section {chunk_id + 1})\")\n",
    "            \n",
    "            # Sauvegarde dans l'historique\n",
    "            self.memory.add_exchange(question, answer, sources)\n",
    "            \n",
    "            return answer, sources\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Erreur lors du traitement: {str(e)}\"\n",
    "            return error_msg, []\n",
    "    \n",
    "    def get_conversation_history(self):\n",
    "        \"\"\"Retourne l'historique format√© pour Gradio\"\"\"\n",
    "        return self.memory.get_formatted_history()\n",
    "    \n",
    "    def clear_conversation(self):\n",
    "        \"\"\"Efface l'historique de conversation\"\"\"\n",
    "        self.memory.clear_history()\n",
    "        if self.chain and hasattr(self.chain, 'memory'):\n",
    "            self.chain.memory.clear()\n",
    "\n",
    "# app/main.py\n",
    "import gradio as gr\n",
    "import os\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "from app.components.file_processor import FileProcessor\n",
    "from app.components.chat_engine import ChatEngine\n",
    "from app.config import Config\n",
    "\n",
    "class DocumentChatBot:\n",
    "    \"\"\"Application principale du ChatBot documentaire\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chat_engine = ChatEngine()\n",
    "        self.current_document = None\n",
    "        self.document_processed = False\n",
    "        \n",
    "    def process_document(self, \n",
    "                        api_key: str, \n",
    "                        file_obj, \n",
    "                        chunk_size: int, \n",
    "                        k_documents: int) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Traite le document upload√©\n",
    "        Returns: (status_message, document_info)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validation des inputs\n",
    "            if not api_key.strip():\n",
    "                return \"‚ùå Veuillez saisir votre cl√© API Mistral\", \"\"\n",
    "            \n",
    "            if file_obj is None:\n",
    "                return \"‚ùå Veuillez s√©lectionner un fichier\", \"\"\n",
    "            \n",
    "            # Configuration du LLM\n",
    "            self.chat_engine.setup_llm(api_key)\n",
    "            \n",
    "            # Traitement du fichier\n",
    "            text_content, filename = FileProcessor.process_uploaded_file(file_obj)\n",
    "            \n",
    "            # Initialisation de la base vectorielle\n",
    "            self.chat_engine.vectorstore_manager.initialize_vectorstore()\n",
    "            \n",
    "            # Ajout des documents\n",
    "            chunk_overlap = max(50, chunk_size // 5)  # 20% de chevauchement\n",
    "            num_chunks = self.chat_engine.vectorstore_manager.add_documents(\n",
    "                text_content, filename, chunk_size, chunk_overlap\n",
    "            )\n",
    "            \n",
    "            # Configuration de la cha√Æne de traitement\n",
    "            self.chat_engine.setup_chain(k_documents)\n",
    "            \n",
    "            # √âtat mis √† jour\n",
    "            self.current_document = filename\n",
    "            self.document_processed = True\n",
    "            \n",
    "            status = f\"‚úÖ Document trait√© avec succ√®s!\"\n",
    "            doc_info = f\"\"\"\n",
    "üìÑ **Fichier**: {filename}\n",
    "üìä **Chunks cr√©√©s**: {num_chunks}\n",
    "‚öôÔ∏è **Taille des chunks**: {chunk_size} caract√®res\n",
    "üîç **Documents r√©cup√©r√©s**: {k_documents}\n",
    "ü§ñ **Mod√®le**: Mistral AI (mistral-tiny)\n",
    "            \"\"\".strip()\n",
    "            \n",
    "            return status, doc_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Erreur: {str(e)}\"\n",
    "            return error_msg, \"\"\n",
    "    \n",
    "    def chat_with_document(self, message: str, history: List[List[str]]) -> Tuple[str, List[List[str]]]:\n",
    "        \"\"\"\n",
    "        Traite une question du chat\n",
    "        Returns: (\"\", updated_history)\n",
    "        \"\"\"\n",
    "        if not self.document_processed:\n",
    "            history.append([message, \"‚ö†Ô∏è Veuillez d'abord traiter un document dans l'onglet Configuration.\"])\n",
    "            return \"\", history\n",
    "        \n",
    "        if not message.strip():\n",
    "            return \"\", history\n",
    "        \n",
    "        # Traitement de la question\n",
    "        answer, sources = self.chat_engine.process_question(message)\n",
    "        \n",
    "        # Formatage de la r√©ponse avec sources\n",
    "        if sources:\n",
    "            formatted_answer = f\"{answer}\\n\\nüìö **Sources**: {', '.join(sources)}\"\n",
    "        else:\n",
    "            formatted_answer = answer\n",
    "        \n",
    "        # Mise √† jour de l'historique\n",
    "        history.append([message, formatted_answer])\n",
    "        \n",
    "        return \"\", history\n",
    "    \n",
    "    def clear_chat(self) -> List[List[str]]:\n",
    "        \"\"\"Efface l'historique du chat\"\"\"\n",
    "        if self.chat_engine:\n",
    "            self.chat_engine.clear_conversation()\n",
    "        return []\n",
    "    \n",
    "    def create_interface(self):\n",
    "        \"\"\"Cr√©e l'interface Gradio\"\"\"\n",
    "        \n",
    "        # CSS personnalis√©\n",
    "        css = \"\"\"\n",
    "        .gradio-container {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        }\n",
    "        .tab-nav button {\n",
    "            font-size: 16px;\n",
    "            font-weight: 500;\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        with gr.Blocks(css=css, title=\"ChatBot Documentaire - Mistral AI\", theme=gr.themes.Soft()) as interface:\n",
    "            \n",
    "            gr.Markdown(\"\"\"\n",
    "            # üìÑ ChatBot Documentaire avec Mistral AI\n",
    "            \n",
    "            Posez des questions sur vos documents PDF, DOCX et TXT gr√¢ce √† l'IA !\n",
    "            \"\"\")\n",
    "            \n",
    "            with gr.Tabs():\n",
    "                \n",
    "                # Onglet Configuration\n",
    "                with gr.Tab(\"‚öôÔ∏è Configuration\", id=\"config\"):\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        with gr.Column(scale=2):\n",
    "                            api_key_input = gr.Textbox(\n",
    "                                label=\"üîë Cl√© API Mistral AI\",\n",
    "                                placeholder=\"Saisissez votre cl√© API Mistral...\",\n",
    "                                type=\"password\",\n",
    "                                info=\"Obtenez votre cl√© gratuite sur https://console.mistral.ai/\"\n",
    "                            )\n",
    "                            \n",
    "                            file_upload = gr.File(\n",
    "                                label=\"üìÅ Fichier √† analyser\",\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"],\n",
    "                                type=\"binary\"\n",
    "                            )\n",
    "                            \n",
    "                            with gr.Row():\n",
    "                                chunk_size_slider = gr.Slider(\n",
    "                                    minimum=200,\n",
    "                                    maximum=2000,\n",
    "                                    value=Config.DEFAULT_CHUNK_SIZE,\n",
    "                                    step=100,\n",
    "                                    label=\"üìè Taille des chunks\",\n",
    "                                    info=\"Taille des segments de texte\"\n",
    "                                )\n",
    "                                \n",
    "                                k_documents_slider = gr.Slider(\n",
    "                                    minimum=1,\n",
    "                                    maximum=10,\n",
    "                                    value=Config.DEFAULT_K_DOCUMENTS,\n",
    "                                    step=1,\n",
    "                                    label=\"üîç Nombre de documents r√©cup√©r√©s\",\n",
    "                                    info=\"Nombre de segments utilis√©s pour r√©pondre\"\n",
    "                                )\n",
    "                            \n",
    "                            process_btn = gr.Button(\"üöÄ Traiter le document\", variant=\"primary\", size=\"lg\")\n",
    "                        \n",
    "                        with gr.Column(scale=1):\n",
    "                            status_output = gr.Textbox(\n",
    "                                label=\"üìä Statut\",\n",
    "                                interactive=False,\n",
    "                                lines=2\n",
    "                            )\n",
    "                            \n",
    "                            doc_info_output = gr.Markdown(\n",
    "                                label=\"‚ÑπÔ∏è Informations du document\"\n",
    "                            )\n",
    "                \n",
    "                # Onglet Chat\n",
    "                with gr.Tab(\"üí¨ Chat\", id=\"chat\"):\n",
    "                    \n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"Conversation avec votre document\",\n",
    "                        height=500,\n",
    "                        show_copy_button=True\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg_input = gr.Textbox(\n",
    "                            label=\"Votre question\",\n",
    "                            placeholder=\"Posez une question sur votre document...\",\n",
    "                            scale=4\n",
    "                        )\n",
    "                        \n",
    "                        with gr.Column(scale=1):\n",
    "                            send_btn = gr.Button(\"üì§ Envoyer\", variant=\"primary\")\n",
    "                            clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "                \n",
    "                # Onglet Aide\n",
    "                with gr.Tab(\"‚ùì Aide\", id=\"help\"):\n",
    "                    gr.Markdown(\"\"\"\n",
    "                    ## üöÄ Comment utiliser cette application ?\n",
    "                    \n",
    "                    ### 1. Configuration\n",
    "                    - **Obtenez une cl√© API Mistral AI** (gratuite) sur [console.mistral.ai](https://console.mistral.ai/)\n",
    "                    - **Uploadez votre document** (PDF, DOCX ou TXT, max 10MB)\n",
    "                    - **Ajustez les param√®tres** selon vos besoins\n",
    "                    - **Cliquez sur \"Traiter le document\"**\n",
    "                    \n",
    "                    ### 2. Chat\n",
    "                    - **Posez vos questions** en langage naturel\n",
    "                    - L'IA r√©pondra en se basant **uniquement** sur votre document\n",
    "                    - Les **sources** sont indiqu√©es pour chaque r√©ponse\n",
    "                    \n",
    "                    ### 3. Conseils\n",
    "                    - **Questions pr√©cises** = meilleures r√©ponses\n",
    "                    - **Reformulez** si la r√©ponse ne vous convient pas\n",
    "                    - **Historique** conserv√© pendant la session\n",
    "                    \n",
    "                    ### üîß Param√®tres avanc√©s\n",
    "                    - **Taille des chunks**: Plus petit = plus pr√©cis, plus grand = plus de contexte\n",
    "                    - **Nombre de documents**: Plus = plus de contexte, mais peut diluer la r√©ponse\n",
    "                    \n",
    "                    ### üÜì Mod√®le utilis√©\n",
    "                    **Mistral AI (mistral-tiny)** - Mod√®le fran√ßais performant et gratuit !\n",
    "                    \"\"\")\n",
    "            \n",
    "            # √âv√©nements\n",
    "            process_btn.click(\n",
    "                fn=self.process_document,\n",
    "                inputs=[api_key_input, file_upload, chunk_size_slider, k_documents_slider],\n",
    "                outputs=[status_output, doc_info_output]\n",
    "            )\n",
    "            \n",
    "            send_btn.click(\n",
    "                fn=self.chat_with_document,\n",
    "                inputs=[msg_input, chatbot],\n",
    "                outputs=[msg_input, chatbot]\n",
    "            )\n",
    "            \n",
    "            msg_input.submit(\n",
    "                fn=self.chat_with_document,\n",
    "                inputs=[msg_input, chatbot],\n",
    "                outputs=[msg_input, chatbot]\n",
    "            )\n",
    "            \n",
    "            clear_btn.click(\n",
    "                fn=self.clear_chat,\n",
    "                outputs=[chatbot]\n",
    "            )\n",
    "        \n",
    "        return interface\n",
    "\n",
    "def main():\n",
    "    \"\"\"Point d'entr√©e principal\"\"\"\n",
    "    # Cr√©er les dossiers n√©cessaires\n",
    "    os.makedirs(\"./data/uploads\", exist_ok=True)\n",
    "    os.makedirs(\"./data/vectorstore\", exist_ok=True)\n",
    "    \n",
    "    # Lancer l'application\n",
    "    app = DocumentChatBot()\n",
    "    interface = app.create_interface()\n",
    "    \n",
    "    interface.launch(\n",
    "        server_name=\"0.0.0.0\",  # Accessible depuis le r√©seau\n",
    "        server_port=7860,\n",
    "        share=False,  # Mettre True pour un lien public temporaire\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie utils :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/__init__.py\n",
    "\"\"\"\n",
    "Package utilitaires pour le ChatBot documentaire\n",
    "\"\"\"\n",
    "\n",
    "from .text_splitter import SmartTextSplitter\n",
    "from .prompt_templates import PromptTemplateManager\n",
    "from .validators import FileValidator, InputValidator\n",
    "from .performance import PerformanceMonitor\n",
    "\n",
    "__all__ = [\n",
    "    'SmartTextSplitter',\n",
    "    'PromptTemplateManager', \n",
    "    'FileValidator',\n",
    "    'InputValidator',\n",
    "    'PerformanceMonitor'\n",
    "]\n",
    "\n",
    "# utils/text_splitter.py\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class SmartTextSplitter:\n",
    "    \"\"\"\n",
    "    D√©coupeur de texte intelligent avec optimisations pour diff√©rents types de documents\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.separators = {\n",
    "            'default': [\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"],\n",
    "            'academic': [\"\\n\\n\", \"\\n\", \". \", \"; \", \", \", \" \", \"\"],\n",
    "            'technical': [\"\\n\\n\", \"\\n\", \".\\n\", \". \", \":\\n\", \": \", \" \", \"\"],\n",
    "            'legal': [\"\\n\\n\", \"\\n\", \". \", \"; \", \" - \", \" \", \"\"]\n",
    "        }\n",
    "    \n",
    "    def detect_document_type(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        D√©tecte le type de document pour optimiser le d√©coupage\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Mots-cl√©s pour diff√©rents types\n",
    "        academic_keywords = ['abstract', 'r√©sum√©', 'introduction', 'conclusion', 'r√©f√©rences', 'bibliographie']\n",
    "        technical_keywords = ['api', 'fonction', 'class', 'def ', 'import', 'documentation', 'manuel']\n",
    "        legal_keywords = ['article', 'clause', 'alin√©a', 'consid√©rant', 'attendu', 'arr√™t√©']\n",
    "        \n",
    "        # Compter les occurrences\n",
    "        academic_score = sum(1 for keyword in academic_keywords if keyword in text_lower)\n",
    "        technical_score = sum(1 for keyword in technical_keywords if keyword in text_lower)\n",
    "        legal_score = sum(1 for keyword in legal_keywords if keyword in text_lower)\n",
    "        \n",
    "        # D√©terminer le type\n",
    "        scores = {\n",
    "            'academic': academic_score,\n",
    "            'technical': technical_score,\n",
    "            'legal': legal_score\n",
    "        }\n",
    "        \n",
    "        max_score = max(scores.values())\n",
    "        if max_score >= 2:  # Seuil minimum\n",
    "            return max(scores, key=scores.get)\n",
    "        \n",
    "        return 'default'\n",
    "    \n",
    "    def create_splitter(self, \n",
    "                       chunk_size: int = 1000,\n",
    "                       chunk_overlap: int = 200,\n",
    "                       document_type: str = 'auto') -> RecursiveCharacterTextSplitter:\n",
    "        \"\"\"\n",
    "        Cr√©e un text splitter optimis√© selon le type de document\n",
    "        \"\"\"\n",
    "        if document_type == 'auto':\n",
    "            # Le type sera d√©tect√© lors du split_text\n",
    "            separators = self.separators['default']\n",
    "        else:\n",
    "            separators = self.separators.get(document_type, self.separators['default'])\n",
    "        \n",
    "        return RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=separators,\n",
    "            keep_separator=True\n",
    "        )\n",
    "    \n",
    "    def smart_split(self, \n",
    "                   text: str, \n",
    "                   chunk_size: int = 1000,\n",
    "                   chunk_overlap: int = 200,\n",
    "                   preserve_structure: bool = True) -> List[str]:\n",
    "        \"\"\"\n",
    "        D√©coupage intelligent qui pr√©serve la structure du document\n",
    "        \"\"\"\n",
    "        # D√©tection automatique du type\n",
    "        doc_type = self.detect_document_type(text)\n",
    "        \n",
    "        # Pr√©processing pour pr√©server la structure\n",
    "        if preserve_structure:\n",
    "            text = self._preprocess_text(text)\n",
    "        \n",
    "        # Cr√©ation du splitter adapt√©\n",
    "        splitter = self.create_splitter(chunk_size, chunk_overlap, doc_type)\n",
    "        \n",
    "        # D√©coupage\n",
    "        chunks = splitter.split_text(text)\n",
    "        \n",
    "        # Post-processing pour nettoyer les chunks\n",
    "        cleaned_chunks = [self._clean_chunk(chunk) for chunk in chunks]\n",
    "        \n",
    "        return [chunk for chunk in cleaned_chunks if chunk.strip()]\n",
    "    \n",
    "    def split_documents_with_metadata(self,\n",
    "                                    text: str,\n",
    "                                    filename: str,\n",
    "                                    chunk_size: int = 1000,\n",
    "                                    chunk_overlap: int = 200) -> List[Document]:\n",
    "        \"\"\"\n",
    "        D√©coupe le texte et cr√©e des documents avec m√©tadonn√©es enrichies\n",
    "        \"\"\"\n",
    "        chunks = self.smart_split(text, chunk_size, chunk_overlap)\n",
    "        doc_type = self.detect_document_type(text)\n",
    "        \n",
    "        documents = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # M√©tadonn√©es enrichies\n",
    "            metadata = {\n",
    "                \"filename\": filename,\n",
    "                \"chunk_id\": i,\n",
    "                \"total_chunks\": len(chunks),\n",
    "                \"document_type\": doc_type,\n",
    "                \"chunk_size\": len(chunk),\n",
    "                \"chunk_position\": \"start\" if i == 0 else (\"end\" if i == len(chunks) - 1 else \"middle\"),\n",
    "                \"contains_structure\": self._has_structure_markers(chunk)\n",
    "            }\n",
    "            \n",
    "            # Ajout de mots-cl√©s pour ce chunk\n",
    "            keywords = self._extract_keywords(chunk)\n",
    "            if keywords:\n",
    "                metadata[\"keywords\"] = keywords\n",
    "            \n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Pr√©processing pour am√©liorer le d√©coupage\"\"\"\n",
    "        # Normaliser les espaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Pr√©server les sauts de ligne importants\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        \n",
    "        # Am√©liorer la d√©tection des phrases\n",
    "        text = re.sub(r'([.!?])\\s*([A-Z])', r'\\1\\n\\2', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _clean_chunk(self, chunk: str) -> str:\n",
    "        \"\"\"Nettoie un chunk apr√®s d√©coupage\"\"\"\n",
    "        # Supprimer les espaces en d√©but/fin\n",
    "        chunk = chunk.strip()\n",
    "        \n",
    "        # Supprimer les lignes vides multiples\n",
    "        chunk = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', chunk)\n",
    "        \n",
    "        # S'assurer qu'on ne commence pas par un s√©parateur\n",
    "        chunk = re.sub(r'^[.!?;:,\\s]+', '', chunk)\n",
    "        \n",
    "        return chunk\n",
    "    \n",
    "    def _has_structure_markers(self, chunk: str) -> bool:\n",
    "        \"\"\"D√©tecte si le chunk contient des marqueurs structurels\"\"\"\n",
    "        structure_patterns = [\n",
    "            r'^\\d+\\.',  # Num√©rotation\n",
    "            r'^[A-Z][.]',  # Sections A., B., etc.\n",
    "            r'^-\\s',  # Listes √† puces\n",
    "            r'^\\*\\s',  # Listes √©toiles\n",
    "            r'^\\w+:\\s',  # Titre: contenu\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, chunk, re.MULTILINE) for pattern in structure_patterns)\n",
    "    \n",
    "    def _extract_keywords(self, chunk: str, max_keywords: int = 5) -> List[str]:\n",
    "        \"\"\"Extrait les mots-cl√©s principaux d'un chunk\"\"\"\n",
    "        # Mots vides fran√ßais et anglais\n",
    "        stop_words = {\n",
    "            'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'et', 'ou', '√†', 'dans', 'sur', 'pour', 'par',\n",
    "            'avec', 'sans', 'sous', 'que', 'qui', 'quoi', 'dont', 'o√π', 'ce', 'cette', 'ces', 'est', 'sont',\n",
    "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are'\n",
    "        }\n",
    "        \n",
    "        # Extraction des mots\n",
    "        words = re.findall(r'\\b[a-zA-Z√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√ß√Ä√Ç√Ñ√â√à√ä√ã√è√é√î√ñ√ô√õ√ú≈∏√á]{3,}\\b', chunk.lower())\n",
    "        \n",
    "        # Filtrage et comptage\n",
    "        word_freq = {}\n",
    "        for word in words:\n",
    "            if word not in stop_words and len(word) > 2:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        # Retourner les mots les plus fr√©quents\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [word for word, _ in sorted_words[:max_keywords]]\n",
    "\n",
    "# utils/prompt_templates.py\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Dict, List\n",
    "\n",
    "class PromptTemplateManager:\n",
    "    \"\"\"\n",
    "    Gestionnaire de templates de prompts optimis√©s pour diff√©rents cas d'usage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.templates = self._initialize_templates()\n",
    "    \n",
    "    def _initialize_templates(self) -> Dict[str, PromptTemplate]:\n",
    "        \"\"\"Initialise les diff√©rents templates de prompts\"\"\"\n",
    "        \n",
    "        templates = {}\n",
    "        \n",
    "        # Template g√©n√©ral (par d√©faut)\n",
    "        templates['general'] = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "            template=\"\"\"Tu es un assistant IA sp√©cialis√© dans l'analyse de documents. \n",
    "R√©ponds aux questions en te basant UNIQUEMENT sur le contexte fourni.\n",
    "\n",
    "Contexte des documents:\n",
    "{context}\n",
    "\n",
    "Historique de la conversation:\n",
    "{chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- R√©ponds de mani√®re pr√©cise et concise\n",
    "- Si l'information n'est pas dans le contexte, dis clairement \"Je ne trouve pas cette information dans le document fourni\"\n",
    "- Cite des passages sp√©cifiques quand c'est pertinent\n",
    "- Reste factuel et objectif\n",
    "\n",
    "R√©ponse:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Template pour documents acad√©miques\n",
    "        templates['academic'] = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "            template=\"\"\"Tu es un assistant de recherche acad√©mique sp√©cialis√© dans l'analyse de publications scientifiques.\n",
    "Analyse le contexte fourni pour r√©pondre √† la question de mani√®re rigoureuse.\n",
    "\n",
    "Contexte du document acad√©mique:\n",
    "{context}\n",
    "\n",
    "Historique de la conversation:\n",
    "{chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Fournis une r√©ponse acad√©mique rigoureuse\n",
    "- Cite les auteurs et sections sp√©cifiques si mentionn√©s\n",
    "- Distingue clairement les faits des hypoth√®ses\n",
    "- Si des donn√©es ou statistiques sont pr√©sentes, inclus-les dans ta r√©ponse\n",
    "- Indique les limites de l'information disponible\n",
    "- Si l'information n'est pas dans le document, dis \"Cette information n'est pas pr√©sente dans le document analys√©\"\n",
    "\n",
    "R√©ponse acad√©mique:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Template pour documents techniques\n",
    "        templates['technical'] = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "            template=\"\"\"Tu es un assistant technique expert en documentation et manuels.\n",
    "Analyse la documentation technique pour fournir une r√©ponse pr√©cise et applicable.\n",
    "\n",
    "Documentation technique:\n",
    "{context}\n",
    "\n",
    "Historique de la conversation:\n",
    "{chat_history}\n",
    "\n",
    "Question technique: {question}\n",
    "\n",
    "Instructions:\n",
    "- Fournis une r√©ponse technique pr√©cise et actionnable\n",
    "- Inclus les √©tapes, commandes ou configurations si pertinentes\n",
    "- Mentionne les pr√©requis ou limitations\n",
    "- Utilise la terminologie technique appropri√©e\n",
    "- Si des exemples de code sont pr√©sents, inclus-les\n",
    "- Si l'information technique n'est pas disponible, dis \"Cette information technique n'est pas document√©e dans ce manuel\"\n",
    "\n",
    "R√©ponse technique:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Template pour documents l√©gaux\n",
    "        templates['legal'] = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "            template=\"\"\"Tu es un assistant sp√©cialis√© dans l'analyse de documents juridiques.\n",
    "Analyse le texte l√©gal pour r√©pondre de mani√®re pr√©cise et structur√©e.\n",
    "\n",
    "Texte l√©gal:\n",
    "{context}\n",
    "\n",
    "Historique de la conversation:\n",
    "{chat_history}\n",
    "\n",
    "Question juridique: {question}\n",
    "\n",
    "Instructions:\n",
    "- Fournis une r√©ponse structur√©e et pr√©cise\n",
    "- Cite les articles, clauses ou sections sp√©cifiques\n",
    "- Respecte la terminologie juridique\n",
    "- Distingue les obligations, droits et proc√©dures\n",
    "- Ne fournis pas de conseil juridique, seulement l'analyse du texte\n",
    "- Si l'information n'est pas dans le document, dis \"Cette disposition n'est pas couverte dans ce texte\"\n",
    "\n",
    "Analyse juridique:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Template pour r√©sum√©\n",
    "        templates['summary'] = PromptTemplate(\n",
    "            input_variables=[\"context\"],\n",
    "            template=\"\"\"Tu dois cr√©er un r√©sum√© structur√© et complet du document fourni.\n",
    "\n",
    "Contenu du document:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- Cr√©e un r√©sum√© structur√© avec des sections claires\n",
    "- Inclus les points cl√©s et informations importantes\n",
    "- Utilise des puces pour les listes d'√©l√©ments\n",
    "- Maintiens l'objectivit√© et la pr√©cision\n",
    "- Indique s'il s'agit d'un r√©sum√© partiel si le document semble incomplet\n",
    "\n",
    "Structure sugg√©r√©e:\n",
    "## R√©sum√© du document\n",
    "\n",
    "### Points principaux\n",
    "- [Point 1]\n",
    "- [Point 2]\n",
    "- [etc.]\n",
    "\n",
    "### Informations cl√©s\n",
    "- [Information 1]\n",
    "- [Information 2]\n",
    "- [etc.]\n",
    "\n",
    "### Conclusion\n",
    "[Synth√®se globale]\n",
    "\n",
    "R√©sum√©:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Template pour extraction d'informations sp√©cifiques\n",
    "        templates['extraction'] = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"Tu dois extraire des informations sp√©cifiques du document selon la demande.\n",
    "\n",
    "Document:\n",
    "{context}\n",
    "\n",
    "Demande d'extraction: {question}\n",
    "\n",
    "Instructions:\n",
    "- Extrais UNIQUEMENT les informations demand√©es\n",
    "- Pr√©sente les r√©sultats de mani√®re structur√©e\n",
    "- Si l'information n'est pas pr√©sente, indique \"Information non trouv√©e\"\n",
    "- Utilise des listes ou tableaux si appropri√©\n",
    "- Reste fid√®le au texte original sans interpr√©ter\n",
    "\n",
    "Extraction:\"\"\"\n",
    "        )\n",
    "        \n",
    "        return templates\n",
    "    \n",
    "    def get_template(self, template_type: str = 'general') -> PromptTemplate:\n",
    "        \"\"\"\n",
    "        Retourne le template demand√©\n",
    "        \"\"\"\n",
    "        return self.templates.get(template_type, self.templates['general'])\n",
    "    \n",
    "    def get_available_templates(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retourne la liste des templates disponibles\n",
    "        \"\"\"\n",
    "        return list(self.templates.keys())\n",
    "    \n",
    "    def create_custom_template(self, \n",
    "                             template_name: str,\n",
    "                             template_content: str,\n",
    "                             input_variables: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Ajoute un template personnalis√©\n",
    "        \"\"\"\n",
    "        self.templates[template_name] = PromptTemplate(\n",
    "            input_variables=input_variables,\n",
    "            template=template_content\n",
    "        )\n",
    "    \n",
    "    def get_optimized_template(self, document_type: str, query_type: str = 'general') -> PromptTemplate:\n",
    "        \"\"\"\n",
    "        Retourne le template le mieux adapt√© selon le type de document et de requ√™te\n",
    "        \"\"\"\n",
    "        # Logique de s√©lection du template optimal\n",
    "        if query_type == 'summary':\n",
    "            return self.get_template('summary')\n",
    "        elif query_type == 'extraction':\n",
    "            return self.get_template('extraction')\n",
    "        elif document_type in ['academic', 'technical', 'legal']:\n",
    "            return self.get_template(document_type)\n",
    "        else:\n",
    "            return self.get_template('general')\n",
    "\n",
    "# utils/validators.py\n",
    "import os\n",
    "import magic\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, List\n",
    "import re\n",
    "\n",
    "class FileValidator:\n",
    "    \"\"\"\n",
    "    Validateur pour les fichiers upload√©s\n",
    "    \"\"\"\n",
    "    \n",
    "    SUPPORTED_EXTENSIONS = {'.pdf', '.docx', '.txt', '.doc'}\n",
    "    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\n",
    "    MIN_FILE_SIZE = 100  # 100 bytes\n",
    "    \n",
    "    @classmethod\n",
    "    def validate_file(cls, file_obj) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Valide un fichier upload√©\n",
    "        Returns: (is_valid, error_message)\n",
    "        \"\"\"\n",
    "        if file_obj is None:\n",
    "            return False, \"Aucun fichier fourni\"\n",
    "        \n",
    "        # V√©rification de l'extension\n",
    "        if hasattr(file_obj, 'name'):\n",
    "            file_extension = Path(file_obj.name).suffix.lower()\n",
    "            if file_extension not in cls.SUPPORTED_EXTENSIONS:\n",
    "                return False, f\"Format non support√©. Formats accept√©s: {', '.join(cls.SUPPORTED_EXTENSIONS)}\"\n",
    "        \n",
    "        # V√©rification de la taille\n",
    "        try:\n",
    "            if hasattr(file_obj, 'size'):\n",
    "                file_size = file_obj.size\n",
    "            else:\n",
    "                # Fallback pour les objets sans attribut size\n",
    "                content = file_obj.read() if hasattr(file_obj, 'read') else file_obj\n",
    "                file_size = len(content)\n",
    "                # Reset du pointeur si possible\n",
    "                if hasattr(file_obj, 'seek'):\n",
    "                    file_obj.seek(0)\n",
    "            \n",
    "            if file_size > cls.MAX_FILE_SIZE:\n",
    "                return False, f\"Fichier trop volumineux (max {cls.MAX_FILE_SIZE // (1024*1024)}MB)\"\n",
    "            \n",
    "            if file_size < cls.MIN_FILE_SIZE:\n",
    "                return False, \"Fichier trop petit ou vide\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return False, f\"Erreur lors de la v√©rification de la taille: {str(e)}\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    @classmethod\n",
    "    def detect_file_type(cls, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        D√©tecte le type MIME d'un fichier\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mime_type = magic.from_file(file_path, mime=True)\n",
    "            return mime_type\n",
    "        except:\n",
    "            # Fallback bas√© sur l'extension\n",
    "            extension = Path(file_path).suffix.lower()\n",
    "            mime_mapping = {\n",
    "                '.pdf': 'application/pdf',\n",
    "                '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
    "                '.doc': 'application/msword',\n",
    "                '.txt': 'text/plain'\n",
    "            }\n",
    "            return mime_mapping.get(extension, 'unknown')\n",
    "\n",
    "class InputValidator:\n",
    "    \"\"\"\n",
    "    Validateur pour les entr√©es utilisateur\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_api_key(api_key: str, provider: str = 'mistral') -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Valide une cl√© API\n",
    "        \"\"\"\n",
    "        if not api_key or not api_key.strip():\n",
    "            return False, \"Cl√© API manquante\"\n",
    "        \n",
    "        api_key = api_key.strip()\n",
    "        \n",
    "        if provider.lower() == 'mistral':\n",
    "            # Format attendu pour Mistral: commence g√©n√©ralement par des caract√®res sp√©cifiques\n",
    "            if len(api_key) < 20:\n",
    "                return False, \"Cl√© API Mistral trop courte\"\n",
    "            \n",
    "            # V√©rification basique du format\n",
    "            if not re.match(r'^[a-zA-Z0-9_-]+$', api_key):\n",
    "                return False, \"Format de cl√© API invalide\"\n",
    "        \n",
    "        elif provider.lower() == 'openai':\n",
    "            # Format OpenAI: sk-...\n",
    "            if not api_key.startswith('sk-'):\n",
    "                return False, \"Cl√© API OpenAI doit commencer par 'sk-'\"\n",
    "            \n",
    "            if len(api_key) < 45:\n",
    "                return False, \"Cl√© API OpenAI trop courte\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_chunk_size(chunk_size: int) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Valide la taille des chunks\n",
    "        \"\"\"\n",
    "        if not isinstance(chunk_size, int):\n",
    "            return False, \"La taille des chunks doit √™tre un nombre entier\"\n",
    "        \n",
    "        if chunk_size < 100:\n",
    "            return False, \"Taille des chunks trop petite (minimum 100)\"\n",
    "        \n",
    "        if chunk_size > 4000:\n",
    "            return False, \"Taille des chunks trop grande (maximum 4000)\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_k_documents(k: int) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Valide le nombre de documents √† r√©cup√©rer\n",
    "        \"\"\"\n",
    "        if not isinstance(k, int):\n",
    "            return False, \"Le nombre de documents doit √™tre un entier\"\n",
    "        \n",
    "        if k < 1:\n",
    "            return False, \"Nombre de documents minimum: 1\"\n",
    "        \n",
    "        if k > 20:\n",
    "            return False, \"Nombre de documents maximum: 20\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_question(question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Valide une question utilisateur\n",
    "        \"\"\"\n",
    "        if not question or not question.strip():\n",
    "            return False, \"Question vide\"\n",
    "        \n",
    "        question = question.strip()\n",
    "        \n",
    "        if len(question) < 3:\n",
    "            return False, \"Question trop courte (minimum 3 caract√®res)\"\n",
    "        \n",
    "        if len(question) > 1000:\n",
    "            return False, \"Question trop longue (maximum 1000 caract√®res)\"\n",
    "        \n",
    "        # V√©rification de caract√®res suspects\n",
    "        suspicious_patterns = [\n",
    "            r'<script',\n",
    "            r'javascript:',\n",
    "            r'eval\\(',\n",
    "            r'exec\\('\n",
    "        ]\n",
    "        \n",
    "        for pattern in suspicious_patterns:\n",
    "            if re.search(pattern, question, re.IGNORECASE):\n",
    "                return False, \"Question contient des √©l√©ments suspects\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def sanitize_input(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Nettoie et s√©curise une entr√©e texte\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Supprime les caract√®res de contr√¥le\n",
    "        text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
    "        \n",
    "        # Normalise les espaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Supprime les espaces en d√©but/fin\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "# utils/performance.py\n",
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "from typing import Dict, List, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Structure pour stocker les m√©triques de performance\"\"\"\n",
    "    timestamp: datetime\n",
    "    operation: str\n",
    "    duration: float\n",
    "    memory_usage: float\n",
    "    cpu_usage: float\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Moniteur de performance pour l'application\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_history: int = 100):\n",
    "        self.metrics_history: List[PerformanceMetrics] = []\n",
    "        self.max_history = max_history\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def measure_performance(self, operation_name: str):\n",
    "        \"\"\"\n",
    "        D√©corateur pour mesurer les performances d'une fonction\n",
    "        \"\"\"\n",
    "        def decorator(func: Callable):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                start_time = time.time()\n",
    "                start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "                start_cpu = psutil.cpu_percent()\n",
    "                \n",
    "                success = True\n",
    "                error_message = None\n",
    "                result = None\n",
    "                \n",
    "                try:\n",
    "                    result = func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    success = False\n",
    "                    error_message = str(e)\n",
    "                    raise\n",
    "                finally:\n",
    "                    end_time = time.time()\n",
    "                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "                    duration = end_time - start_time\n",
    "                    \n",
    "                    # Mesure CPU pendant l'op√©ration (approximation)\n",
    "                    cpu_usage = psutil.cpu_percent()\n",
    "                    \n",
    "                    metrics = PerformanceMetrics(\n",
    "                        timestamp=datetime.now(),\n",
    "                        operation=operation_name,\n",
    "                        duration=duration,\n",
    "                        memory_usage=end_memory - start_memory,\n",
    "                        cpu_usage=cpu_usage,\n",
    "                        success=success,\n",
    "                        error_message=error_message\n",
    "                    )\n",
    "                    \n",
    "                    self._add_metrics(metrics)\n",
    "                \n",
    "                return result\n",
    "            return wrapper\n",
    "        return decorator\n",
    "    \n",
    "    def _add_metrics(self, metrics: PerformanceMetrics):\n",
    "        \"\"\"Ajoute des m√©triques √† l'historique\"\"\"\n",
    "        with self._lock:\n",
    "            self.metrics_history.append(metrics)\n",
    "            \n",
    "            # Limiter la taille de l'historique\n",
    "            if len(self.metrics_history) > self.max_history:\n",
    "                self.metrics_history = self.metrics_history[-self.max_history:]\n",
    "    \n",
    "    def get_metrics_summary(self, \n",
    "                           operation: Optional[str] = None,\n",
    "                           last_n_minutes: Optional[int] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Retourne un r√©sum√© des m√©triques\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            filtered_metrics = self.metrics_history.copy()\n",
    "        \n",
    "        # Filtrage par op√©ration\n",
    "        if operation:\n",
    "            filtered_metrics = [m for m in filtered_metrics if m.operation == operation]\n",
    "        \n",
    "        # Filtrage temporel\n",
    "        if last_n_minutes:\n",
    "            cutoff_time = datetime.now() - timedelta(minutes=last_n_minutes)\n",
    "            filtered_metrics = [m for m in filtered_metrics if m.timestamp >= cutoff_time]\n",
    "        \n",
    "        if not filtered_metrics:\n",
    "            return {\"message\": \"Aucune m√©trique disponible\"}\n",
    "        \n",
    "        # Calculs statistiques\n",
    "        durations = [m.duration for m in filtered_metrics]\n",
    "        memory_usages = [m.memory_usage for m in filtered_metrics]\n",
    "        success_count = sum(1 for m in filtered_metrics if m.success)\n",
    "        \n",
    "        summary = {\n",
    "            \"total_operations\": len(filtered_metrics),\n",
    "            \"success_rate\": success_count / len(filtered_metrics) * 100,\n",
    "            \"avg_duration\": sum(durations) / len(durations),\n",
    "            \"max_duration\": max(durations),\n",
    "            \"min_duration\": min(durations),\n",
    "            \"avg_memory_usage\": sum(memory_usages) / len(memory_usages),\n",
    "            \"max_memory_usage\": max(memory_usages),\n",
    "            \"recent_errors\": [\n",
    "                {\"operation\": m.operation, \"error\": m.error_message, \"timestamp\": m.timestamp}\n",
    "                for m in filtered_metrics[-5:] if not m.success\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def get_system_info(self) -> Dict:\n",
    "        \"\"\"Retourne les informations syst√®me actuelles\"\"\"\n",
    "        return {\n",
    "            \"cpu_percent\": psutil.cpu_percent(),\n",
    "            \"memory_percent\": psutil.virtual_memory().percent,\n",
    "            \"memory_available_gb\": psutil.virtual_memory().available / 1024 / 1024 / 1024,\n",
    "            \"disk_usage_percent\": psutil.disk_usage('/').percent,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def export_metrics(self, filepath: str):\n",
    "        \"\"\"Exporte les m√©triques vers un fichier JSON\"\"\"\n",
    "        import json\n",
    "        \n",
    "        with self._lock:\n",
    "            data = {\n",
    "                \"export_timestamp\": datetime.now().isoformat(),\n",
    "                \"metrics\": [\n",
    "                    {\n",
    "                        \"timestamp\": m.timestamp.isoformat(),\n",
    "                        \"operation\": m.operation,\n",
    "                        \"duration\": m.duration,\n",
    "                        \"memory_usage\": m.memory_usage,\n",
    "                        \"cpu_usage\": m.cpu_usage,\n",
    "                        \"success\": m.success,\n",
    "                        \"error_message\": m.error_message\n",
    "                    }\n",
    "                    for m in self.metrics_history\n",
    "                ]\n",
    "            }\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def clear_metrics(self):\n",
    "        \"\"\"Efface l'historique des m√©triques\"\"\"\n",
    "        with self._lock:\n",
    "            self.metrics_history.clear()\n",
    "\n",
    "# Exemple d'utilisation du moniteur de performance\n",
    "def create_performance_monitor():\n",
    "    \"\"\"Factory function pour cr√©er un moniteur de performance\"\"\"\n",
    "    return PerformanceMonitor(max_history=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìã R√©capitulatif des scripts cr√©√©s :\n",
    "1. text_splitter.py - D√©coupage intelligent\n",
    "\n",
    "SmartTextSplitter : D√©tection automatique du type de document\n",
    "Types support√©s : acad√©mique, technique, l√©gal, g√©n√©ral\n",
    "Optimisations : pr√©servation de structure, extraction de mots-cl√©s\n",
    "M√©tadonn√©es enrichies pour chaque chunk\n",
    "\n",
    "2. prompt_templates.py - Templates de prompts optimis√©s\n",
    "\n",
    "6 templates sp√©cialis√©s : g√©n√©ral, acad√©mique, technique, l√©gal, r√©sum√©, extraction\n",
    "S√©lection automatique selon le type de document\n",
    "Templates personnalisables\n",
    "Optimisation contextuelle\n",
    "\n",
    "3. validators.py - Validation et s√©curit√©\n",
    "\n",
    "FileValidator : validation fichiers (taille, format, MIME)\n",
    "InputValidator : validation des entr√©es (API keys, param√®tres, questions)\n",
    "S√©curisation : d√©tection de patterns suspects, nettoyage\n",
    "Support multi-providers (Mistral, OpenAI)\n",
    "\n",
    "4. performance.py - Monitoring des performances\n",
    "\n",
    "PerformanceMonitor : mesure automatique des performances\n",
    "M√©triques : temps, m√©moire, CPU, taux de succ√®s\n",
    "D√©corateur pour instrumenter les fonctions\n",
    "Export et historique des m√©triques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .env.example :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .env.example\n",
    "MISTRAL_API_KEY=your_mistral_api_key_here\n",
    "\n",
    "# Installation et lancement\n",
    "# 1. Cr√©er un environnement virtuel\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # Linux/Mac\n",
    "# ou\n",
    "venv\\Scripts\\activate  # Windows\n",
    "\n",
    "# 2. Installer les d√©pendances\n",
    "pip install gradio>=4.0.0 langchain>=0.1.0 langchain-community langchain-mistralai chromadb>=0.4.0 mistralai>=0.1.0 PyPDF2>=3.0.0 python-docx>=0.8.11 sentence-transformers>=2.2.0 python-dotenv>=1.0.0 tiktoken>=0.5.0\n",
    "\n",
    "# 3. Cr√©er le fichier .env avec votre cl√© API\n",
    "cp .env.example .env\n",
    "# Puis √©diter .env avec votre vraie cl√© API Mistral\n",
    "\n",
    "# 4. Lancer l'application\n",
    "python app/main.py\n",
    "\n",
    "# Structure des dossiers √† cr√©er\n",
    "mkdir -p app/components\n",
    "mkdir -p data/uploads\n",
    "mkdir -p data/vectorstore\n",
    "mkdir -p utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-r√©cap :\n",
    "\n",
    "### ‚ú® Fonctionnalit√©s principales :\n",
    "- **Interface Gradio moderne** avec 3 onglets (Configuration, Chat, Aide)\n",
    "- **Mistral AI int√©gr√©** (mistral-tiny pour commencer, gratuit)\n",
    "- **Embeddings gratuits** avec sentence-transformers (multilingue FR/EN)\n",
    "- **Support PDF, DOCX, TXT** avec extraction robuste\n",
    "- **ChromaDB local** pour la persistance\n",
    "- **Historique de conversation** avec sources\n",
    "- **Configuration flexible** des param√®tres\n",
    "\n",
    "### üöÄ Pour lancer l'application :\n",
    "\n",
    "1. **Cr√©ez la structure** :\n",
    "```bash\n",
    "mkdir document-chatbot && cd document-chatbot\n",
    "mkdir -p app/components data/uploads data/vectorstore\n",
    "```\n",
    "\n",
    "2. **Copiez le code** dans les fichiers appropri√©s\n",
    "3. **Installez les d√©pendances** avec pip\n",
    "4. **Obtenez votre cl√© API** Mistral (gratuite) sur [console.mistral.ai](https://console.mistral.ai/)\n",
    "5. **Lancez** : `python app/main.py`\n",
    "\n",
    "### üîß Points forts de cette impl√©mentation :\n",
    "\n",
    "- **100% gratuit** (sauf limite API Mistral gratuite)\n",
    "- **Multilingue** (fran√ßais/anglais optimis√©)\n",
    "- **Robuste** avec gestion d'erreurs compl√®te\n",
    "- **Modulaire** et extensible\n",
    "- **Interface intuitive** avec aide int√©gr√©e\n",
    "\n",
    "L'application sera accessible sur `http://localhost:7860` et ressemblera √† SecInsights.ai avec des fonctionnalit√©s similaires !\n",
    "\n",
    "Voulez-vous que je vous aide avec l'installation ou que j'ajoute des fonctionnalit√©s sp√©cifiques ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok NEW GAME :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Guide d'installation et utilisation compl√®te\n",
    "\n",
    "## üì¶ Installation\n",
    "\n",
    "### 1. Cr√©er la structure du projet\n",
    "```bash\n",
    "mkdir document-chatbot\n",
    "cd document-chatbot\n",
    "\n",
    "# Cr√©er l'arborescence\n",
    "mkdir -p app/components\n",
    "mkdir -p utils\n",
    "mkdir -p data/uploads\n",
    "mkdir -p data/vectorstore\n",
    "mkdir -p data/exports\n",
    "```\n",
    "\n",
    "### 2. Installer les d√©pendances\n",
    "```bash\n",
    "# Cr√©er un environnement virtuel\n",
    "python -m venv venv\n",
    "\n",
    "# Activer l'environnement\n",
    "# Linux/Mac:\n",
    "source venv/bin/activate\n",
    "# Windows:\n",
    "venv\\Scripts\\activate\n",
    "\n",
    "# Installer les packages\n",
    "pip install gradio>=4.0.0\n",
    "pip install langchain>=0.1.0\n",
    "pip install langchain-community\n",
    "pip install langchain-mistralai\n",
    "pip install chromadb>=0.4.0\n",
    "pip install mistralai>=0.1.0\n",
    "pip install PyPDF2>=3.0.0\n",
    "pip install python-docx>=0.8.11\n",
    "pip install sentence-transformers>=2.2.0\n",
    "pip install python-dotenv>=1.0.0\n",
    "pip install tiktoken>=0.5.0\n",
    "pip install psutil>=5.9.0\n",
    "pip install python-magic>=0.4.27\n",
    "\n",
    "# Alternative pour python-magic sur Windows :\n",
    "# pip install python-magic-bin==0.4.14\n",
    "```\n",
    "\n",
    "### 3. Configuration\n",
    "```bash\n",
    "# Cr√©er le fichier d'environnement\n",
    "cp .env.example .env\n",
    "\n",
    "# √âditer .env avec votre cl√© API Mistral\n",
    "echo \"MISTRAL_API_KEY=votre_cle_api_ici\" > .env\n",
    "```\n",
    "\n",
    "## üéØ Utilisation\n",
    "\n",
    "### Version Simple (originale)\n",
    "```bash\n",
    "python app/main.py\n",
    "```\n",
    "\n",
    "### Version Avanc√©e (avec tous les utilitaires)\n",
    "```bash\n",
    "python app/main_enhanced.py\n",
    "```\n",
    "\n",
    "## üÜï Nouvelles fonctionnalit√©s de la version am√©lior√©e\n",
    "\n",
    "### 1. **D√©coupage intelligent de documents**\n",
    "- **D√©tection automatique** du type de document\n",
    "- **Types support√©s** : \n",
    "  - üìö Acad√©mique (articles, th√®ses, rapports de recherche)\n",
    "  - üîß Technique (manuels, documentation, guides)\n",
    "  - ‚öñÔ∏è L√©gal (contrats, r√®glements, lois)\n",
    "  - üìÑ G√©n√©ral (tous autres documents)\n",
    "\n",
    "- **Optimisations par type** :\n",
    "  - S√©parateurs sp√©cialis√©s\n",
    "  - Pr√©servation de la structure\n",
    "  - Extraction de mots-cl√©s\n",
    "  - M√©tadonn√©es enrichies\n",
    "\n",
    "### 2. **Templates de prompts optimis√©s**\n",
    "- **S√©lection automatique** du template selon le document\n",
    "- **Templates disponibles** :\n",
    "  - `general` : Usage standard\n",
    "  - `academic` : Analyse rigoureuse, citations\n",
    "  - `technical` : R√©ponses pratiques, √©tapes\n",
    "  - `legal` : Analyse structur√©e, articles\n",
    "  - `summary` : R√©sum√©s structur√©s\n",
    "  - `extraction` : Extraction d'informations\n",
    "\n",
    "### 3. **Validation et s√©curit√© renforc√©es**\n",
    "- **Validation des fichiers** :\n",
    "  - V√©rification du format et de la taille\n",
    "  - D√©tection du type MIME\n",
    "  - Protection contre les fichiers malveillants\n",
    "\n",
    "- **Validation des entr√©es** :\n",
    "  - Cl√©s API (format et longueur)\n",
    "  - Param√®tres (plages valides)\n",
    "  - Questions (nettoyage et s√©curisation)\n",
    "\n",
    "### 4. **Monitoring de performance**\n",
    "- **M√©triques en temps r√©el** :\n",
    "  - Temps de traitement\n",
    "  - Utilisation m√©moire et CPU\n",
    "  - Taux de succ√®s des op√©rations\n",
    "\n",
    "- **Historique et export** :\n",
    "  - Sauvegarde des m√©triques\n",
    "  - Export des sessions\n",
    "  - Analyse des performances\n",
    "\n",
    "## üéõÔ∏è Interface utilisateur am√©lior√©e\n",
    "\n",
    "### Onglet \"Configuration Avanc√©e\"\n",
    "1. **Cl√© API** avec validation automatique\n",
    "2. **Upload de fichier** avec v√©rification en temps r√©el\n",
    "3. **Param√®tres avanc√©s** :\n",
    "   - Taille des chunks (optimis√©e automatiquement)\n",
    "   - Nombre de documents r√©cup√©r√©s\n",
    "   - **Type de template** (auto-d√©tection ou manuel)\n",
    "4. **Informations d√©taill√©es** :\n",
    "   - Type de document d√©tect√©\n",
    "   - Nombre de chunks cr√©√©s\n",
    "   - Mots-cl√©s extraits\n",
    "   - M√©triques de performance\n",
    "\n",
    "### Onglet \"Chat Intelligent\"\n",
    "1. **Conversation am√©lior√©e** avec :\n",
    "   - Sources d√©taill√©es avec aper√ßu\n",
    "   - Mots-cl√©s des chunks utilis√©s\n",
    "   - Template de prompt utilis√©\n",
    "2. **M√©tadonn√©es en temps r√©el** :\n",
    "   - Nombre de sources consult√©es\n",
    "   - Temps de r√©ponse\n",
    "   - Template utilis√©\n",
    "3. **Validation automatique** des questions\n",
    "\n",
    "### Onglet \"Monitoring\"\n",
    "1. **Statut du syst√®me** :\n",
    "   - Configuration actuelle\n",
    "   - Ressources syst√®me (CPU, m√©moire, disque)\n",
    "   - Templates disponibles\n",
    "2. **Export de session** :\n",
    "   - Historique des conversations\n",
    "   - M√©triques de performance\n",
    "   - Configuration utilis√©e\n",
    "\n",
    "## üîç Exemples d'utilisation\n",
    "\n",
    "### Document acad√©mique\n",
    "```\n",
    "Document : Article de recherche sur l'IA\n",
    "Type d√©tect√© : academic\n",
    "Template utilis√© : academic\n",
    "R√©ponse : \"Selon l'√©tude pr√©sent√©e dans ce document, les auteurs d√©montrent que...\"\n",
    "```\n",
    "\n",
    "### Documentation technique\n",
    "```\n",
    "Document : Manuel d'installation logiciel\n",
    "Type d√©tect√© : technical\n",
    "Template utilis√© : technical\n",
    "R√©ponse : \"Pour installer le logiciel, suivez ces √©tapes : 1. T√©l√©charger...\"\n",
    "```\n",
    "\n",
    "### Document l√©gal\n",
    "```\n",
    "Document : Contrat de travail\n",
    "Type d√©tect√© : legal\n",
    "Template utilis√© : legal\n",
    "R√©ponse : \"L'article 3.2 du contrat stipule que...\"\n",
    "```\n",
    "\n",
    "## üìä M√©triques et performance\n",
    "\n",
    "### M√©triques collect√©es\n",
    "- **Temps de traitement** par op√©ration\n",
    "- **Utilisation m√©moire** (avant/apr√®s)\n",
    "- **CPU** pendant le traitement\n",
    "- **Taux de succ√®s** des op√©rations\n",
    "- **Erreurs** avec d√©tails\n",
    "\n",
    "### Export des donn√©es\n",
    "```json\n",
    "{\n",
    "  \"document\": \"mon_document.pdf\",\n",
    "  \"system_status\": {\n",
    "    \"template_used\": \"academic\",\n",
    "    \"conversation_length\": 5,\n",
    "    \"performance_summary\": {...}\n",
    "  },\n",
    "  \"conversation_history\": [...],\n",
    "  \"performance_metrics\": {...}\n",
    "}\n",
    "```\n",
    "\n",
    "## üéØ Avantages de la version am√©lior√©e\n",
    "\n",
    "### üöÄ Performance\n",
    "- **D√©coupage optimis√©** selon le type de document\n",
    "- **Templates sp√©cialis√©s** pour de meilleures r√©ponses\n",
    "- **Monitoring en temps r√©el** des performances\n",
    "\n",
    "### üîí S√©curit√©\n",
    "- **Validation robuste** des entr√©es\n",
    "- **Nettoyage automatique** des donn√©es\n",
    "- **Protection** contre les injections\n",
    "\n",
    "### üß† Intelligence\n",
    "- **D√©tection automatique** du contexte\n",
    "- **Adaptation dynamique** des r√©ponses\n",
    "- **M√©tadonn√©es enrichies** pour plus de pr√©cision\n",
    "\n",
    "### üìà Observabilit√©\n",
    "- **M√©triques d√©taill√©es** en temps r√©el\n",
    "- **Historique complet** des performances\n",
    "- **Export facile** des donn√©es\n",
    "\n",
    "## üîß Personnalisation\n",
    "\n",
    "### Ajouter un nouveau template\n",
    "```python\n",
    "from utils.prompt_templates import PromptTemplateManager\n",
    "\n",
    "manager = PromptTemplateManager()\n",
    "manager.create_custom_template(\n",
    "    template_name=\"medical\",\n",
    "    template_content=\"Tu es un assistant m√©dical...\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "```\n",
    "\n",
    "### Personnaliser la d√©tection de type\n",
    "```python\n",
    "from utils.text_splitter import SmartTextSplitter\n",
    "\n",
    "splitter = SmartTextSplitter()\n",
    "# Ajouter des mots-cl√©s pour un nouveau type\n",
    "splitter.separators['medical'] = [\"\\n\\n\", \"\\n\", \". \", \"; \", \" \", \"\"]\n",
    "```\n",
    "\n",
    "## üêõ D√©pannage\n",
    "\n",
    "### Probl√®mes courants\n",
    "\n",
    "1. **Erreur d'installation de python-magic** :\n",
    "   ```bash\n",
    "   # Linux\n",
    "   sudo apt-get install libmagic1\n",
    "   \n",
    "   # macOS\n",
    "   brew install libmagic\n",
    "   \n",
    "   # Windows\n",
    "   pip install python-magic-bin\n",
    "   ```\n",
    "\n",
    "2. **Erreur de cl√© API Mistral** :\n",
    "   - V√©rifiez que la cl√© commence par le bon format\n",
    "   - Consultez https://console.mistral.ai/ pour obtenir une cl√©\n",
    "\n",
    "3. **Probl√®me de m√©moire** :\n",
    "   - R√©duisez la taille des chunks\n",
    "   - Diminuez le nombre de documents r√©cup√©r√©s\n",
    "   - Surveillez l'onglet Monitoring\n",
    "\n",
    "4. **Documents non trait√©s** :\n",
    "   - V√©rifiez le format (PDF, DOCX, TXT)\n",
    "   - Assurez-vous que le fichier contient du texte\n",
    "   - V√©rifiez la taille (max 10MB)\n",
    "\n",
    "## üéâ Fonctionnalit√©s futures\n",
    "\n",
    "- üåê **Support multilingue** avanc√©\n",
    "- üîó **Int√©gration d'APIs** externes\n",
    "- üì± **Interface mobile** responsive\n",
    "- ü§ñ **Agent autonome** pour l'analyse\n",
    "- üìä **Tableaux de bord** avanc√©s\n",
    "- üíæ **Base de donn√©es** persistante\n",
    "- üîÑ **Synchronisation** multi-utilisateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Structure finale du projet :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "document-chatbot/\n",
    "‚îú‚îÄ‚îÄ app/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # Version originale\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ main_enhanced.py           # Version am√©lior√©e avec utilitaires\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ components/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ file_processor.py\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ vectorstore.py         # Version originale\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ vectorstore_enhanced.py # Version am√©lior√©e\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ chat_engine.py         # Version originale\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ chat_engine_enhanced.py # Version am√©lior√©e\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ memory.py\n",
    "‚îú‚îÄ‚îÄ utils/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # ‚úÖ Cr√©√©\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ text_splitter.py          # ‚úÖ Cr√©√©\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ prompt_templates.py       # ‚úÖ Cr√©√©\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ validators.py             # ‚úÖ Cr√©√©\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ performance.py            # ‚úÖ Cr√©√©\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ uploads/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ vectorstore/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ exports/                  # Nouveau : exports de session\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ .env.example\n",
    "‚îî‚îÄ‚îÄ README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app/components/vectorstore_enhanced.py\n",
    "# Version am√©lior√©e avec les utilitaires\n",
    "\n",
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "from app.config import Config\n",
    "from utils.text_splitter import SmartTextSplitter\n",
    "from utils.validators import InputValidator\n",
    "from utils.performance import PerformanceMonitor\n",
    "\n",
    "class EnhancedVectorStoreManager:\n",
    "    \"\"\"Version am√©lior√©e du gestionnaire de base vectorielle avec utilitaires\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = None\n",
    "        self.vectorstore = None\n",
    "        self.text_splitter = SmartTextSplitter()\n",
    "        self.performance_monitor = PerformanceMonitor()\n",
    "        self._setup_embeddings()\n",
    "    \n",
    "    def _setup_embeddings(self):\n",
    "        \"\"\"Configure les embeddings avec monitoring\"\"\"\n",
    "        @self.performance_monitor.measure_performance(\"setup_embeddings\")\n",
    "        def _setup():\n",
    "            model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs={'device': 'cpu'},\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "        \n",
    "        _setup()\n",
    "    \n",
    "    @property\n",
    "    def performance_metrics(self):\n",
    "        \"\"\"Acc√®s aux m√©triques de performance\"\"\"\n",
    "        return self.performance_monitor.get_metrics_summary()\n",
    "    \n",
    "    def initialize_vectorstore(self, collection_name: str = Config.COLLECTION_NAME):\n",
    "        \"\"\"Initialise la base vectorielle avec monitoring\"\"\"\n",
    "        @self.performance_monitor.measure_performance(\"initialize_vectorstore\")\n",
    "        def _initialize():\n",
    "            os.makedirs(Config.CHROMADB_PATH, exist_ok=True)\n",
    "            client = chromadb.PersistentClient(path=Config.CHROMADB_PATH)\n",
    "            \n",
    "            try:\n",
    "                client.delete_collection(collection_name)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "            self.vectorstore = Chroma(\n",
    "                client=client,\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=self.embeddings,\n",
    "            )\n",
    "        \n",
    "        _initialize()\n",
    "    \n",
    "    def add_documents_enhanced(self, \n",
    "                             text: str, \n",
    "                             filename: str, \n",
    "                             chunk_size: int, \n",
    "                             chunk_overlap: int) -> Tuple[int, dict]:\n",
    "        \"\"\"\n",
    "        Version am√©lior√©e d'ajout de documents avec validation et monitoring\n",
    "        Returns: (nombre_chunks, informations_d√©taill√©es)\n",
    "        \"\"\"\n",
    "        # Validation des param√®tres\n",
    "        valid_chunk, chunk_error = InputValidator.validate_chunk_size(chunk_size)\n",
    "        if not valid_chunk:\n",
    "            raise ValueError(f\"Chunk size invalide: {chunk_error}\")\n",
    "        \n",
    "        @self.performance_monitor.measure_performance(\"add_documents\")\n",
    "        def _add_documents():\n",
    "            if not self.vectorstore:\n",
    "                raise ValueError(\"VectorStore non initialis√©\")\n",
    "            \n",
    "            # D√©coupage intelligent avec m√©tadonn√©es enrichies\n",
    "            documents = self.text_splitter.split_documents_with_metadata(\n",
    "                text, filename, chunk_size, chunk_overlap\n",
    "            )\n",
    "            \n",
    "            # Ajout √† la base vectorielle\n",
    "            self.vectorstore.add_documents(documents)\n",
    "            \n",
    "            # Informations d√©taill√©es sur le traitement\n",
    "            doc_info = {\n",
    "                \"total_chunks\": len(documents),\n",
    "                \"document_type\": documents[0].metadata.get(\"document_type\", \"unknown\") if documents else \"unknown\",\n",
    "                \"average_chunk_size\": sum(len(doc.page_content) for doc in documents) / len(documents) if documents else 0,\n",
    "                \"keywords_extracted\": any(\"keywords\" in doc.metadata for doc in documents),\n",
    "                \"structure_preserved\": sum(1 for doc in documents if doc.metadata.get(\"contains_structure\", False))\n",
    "            }\n",
    "            \n",
    "            return len(documents), doc_info\n",
    "        \n",
    "        return _add_documents()\n",
    "    \n",
    "    def search_with_metadata(self, query: str, k: int = Config.DEFAULT_K_DOCUMENTS) -> List[dict]:\n",
    "        \"\"\"Recherche avec m√©tadonn√©es d√©taill√©es\"\"\"\n",
    "        # Validation de la requ√™te\n",
    "        valid_query, query_error = InputValidator.validate_question(query)\n",
    "        if not valid_query:\n",
    "            raise ValueError(f\"Requ√™te invalide: {query_error}\")\n",
    "        \n",
    "        @self.performance_monitor.measure_performance(\"search_documents\")\n",
    "        def _search():\n",
    "            if not self.vectorstore:\n",
    "                return []\n",
    "            \n",
    "            documents = self.vectorstore.similarity_search(query, k=k)\n",
    "            \n",
    "            # Enrichissement des r√©sultats avec m√©tadonn√©es\n",
    "            enriched_results = []\n",
    "            for doc in documents:\n",
    "                result = {\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata,\n",
    "                    \"relevance_score\": getattr(doc, 'relevance_score', None),\n",
    "                    \"chunk_info\": {\n",
    "                        \"position\": doc.metadata.get(\"chunk_position\", \"unknown\"),\n",
    "                        \"total_chunks\": doc.metadata.get(\"total_chunks\", 0),\n",
    "                        \"chunk_id\": doc.metadata.get(\"chunk_id\", 0),\n",
    "                        \"has_structure\": doc.metadata.get(\"contains_structure\", False),\n",
    "                        \"keywords\": doc.metadata.get(\"keywords\", [])\n",
    "                    }\n",
    "                }\n",
    "                enriched_results.append(result)\n",
    "            \n",
    "            return enriched_results\n",
    "        \n",
    "        return _search()\n",
    "\n",
    "# app/components/chat_engine_enhanced.py\n",
    "# Version am√©lior√©e du moteur de chat\n",
    "\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from app.components.vectorstore_enhanced import EnhancedVectorStoreManager\n",
    "from app.components.memory import ConversationMemory\n",
    "from utils.prompt_templates import PromptTemplateManager\n",
    "from utils.validators import InputValidator\n",
    "from utils.performance import PerformanceMonitor\n",
    "\n",
    "class EnhancedChatEngine:\n",
    "    \"\"\"Moteur de conversation am√©lior√© avec utilitaires avanc√©s\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.vectorstore_manager = EnhancedVectorStoreManager()\n",
    "        self.memory = ConversationMemory()\n",
    "        self.prompt_manager = PromptTemplateManager()\n",
    "        self.performance_monitor = PerformanceMonitor()\n",
    "        self.chain = None\n",
    "        self.current_template_type = 'general'\n",
    "    \n",
    "    def setup_llm(self, api_key: str, model: str = \"mistral-tiny\"):\n",
    "        \"\"\"Configuration du LLM avec validation\"\"\"\n",
    "        # Validation de la cl√© API\n",
    "        valid_key, key_error = InputValidator.validate_api_key(api_key, 'mistral')\n",
    "        if not valid_key:\n",
    "            raise ValueError(f\"Cl√© API invalide: {key_error}\")\n",
    "        \n",
    "        @self.performance_monitor.measure_performance(\"setup_llm\")\n",
    "        def _setup():\n",
    "            self.llm = ChatMistralAI(\n",
    "                mistral_api_key=api_key,\n",
    "                model=model,\n",
    "                temperature=0.1,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "        \n",
    "        _setup()\n",
    "    \n",
    "    def setup_chain_with_template(self, \n",
    "                                 k_documents: int = 3, \n",
    "                                 template_type: str = 'auto'):\n",
    "        \"\"\"Configuration de la cha√Æne avec template intelligent\"\"\"\n",
    "        if not self.llm:\n",
    "            raise ValueError(\"LLM non configur√©\")\n",
    "        \n",
    "        if not self.vectorstore_manager.vectorstore:\n",
    "            raise ValueError(\"VectorStore non initialis√©\")\n",
    "        \n",
    "        # Validation des param√®tres\n",
    "        valid_k, k_error = InputValidator.validate_k_documents(k_documents)\n",
    "        if not valid_k:\n",
    "            raise ValueError(f\"Param√®tre k invalide: {k_error}\")\n",
    "        \n",
    "        @self.performance_monitor.measure_performance(\"setup_chain\")\n",
    "        def _setup():\n",
    "            # D√©tection automatique du type de template si n√©cessaire\n",
    "            if template_type == 'auto':\n",
    "                # R√©cup√©rer le type de document depuis les m√©tadonn√©es\n",
    "                sample_docs = self.vectorstore_manager.search_with_metadata(\"test\", k=1)\n",
    "                if sample_docs:\n",
    "                    document_type = sample_docs[0]['metadata'].get('document_type', 'general')\n",
    "                    template_type_final = document_type\n",
    "                else:\n",
    "                    template_type_final = 'general'\n",
    "            else:\n",
    "                template_type_final = template_type\n",
    "            \n",
    "            # S√©lection du template optimis√©\n",
    "            prompt_template = self.prompt_manager.get_optimized_template(\n",
    "                document_type=template_type_final,\n",
    "                query_type='general'\n",
    "            )\n",
    "            \n",
    "            self.current_template_type = template_type_final\n",
    "            \n",
    "            # Configuration de la m√©moire\n",
    "            memory = ConversationBufferMemory(\n",
    "                memory_key=\"chat_history\",\n",
    "                return_messages=True,\n",
    "                output_key=\"answer\"\n",
    "            )\n",
    "            \n",
    "            # Cr√©ation de la cha√Æne\n",
    "            self.chain = ConversationalRetrievalChain.from_llm(\n",
    "                llm=self.llm,\n",
    "                retriever=self.vectorstore_manager.get_retriever(k=k_documents),\n",
    "                memory=memory,\n",
    "                return_source_documents=True,\n",
    "                combine_docs_chain_kwargs={\"prompt\": prompt_template}\n",
    "            )\n",
    "        \n",
    "        _setup()\n",
    "    \n",
    "    def process_question_enhanced(self, question: str) -> Tuple[str, List[str], Dict]:\n",
    "        \"\"\"\n",
    "        Traitement avanc√© des questions avec m√©tadonn√©es enrichies\n",
    "        Returns: (answer, sources, metadata)\n",
    "        \"\"\"\n",
    "        # Validation et nettoyage de la question\n",
    "        valid_question, question_error = InputValidator.validate_question(question)\n",
    "        if not valid_question:\n",
    "            return f\"Question invalide: {question_error}\", [], {}\n",
    "        \n",
    "        question = InputValidator.sanitize_input(question)\n",
    "        \n",
    "        @self.performance_monitor.measure_performance(\"process_question\")\n",
    "        def _process():\n",
    "            if not self.chain:\n",
    "                return \"Erreur: Syst√®me non configur√©\", [], {}\n",
    "            \n",
    "            try:\n",
    "                # Ex√©cution de la cha√Æne\n",
    "                result = self.chain({\"question\": question})\n",
    "                \n",
    "                answer = result.get(\"answer\", \"Pas de r√©ponse g√©n√©r√©e\")\n",
    "                \n",
    "                # Extraction enrichie des sources\n",
    "                sources = []\n",
    "                source_metadata = []\n",
    "                \n",
    "                if \"source_documents\" in result:\n",
    "                    for doc in result[\"source_documents\"]:\n",
    "                        filename = doc.metadata.get(\"filename\", \"Document\")\n",
    "                        chunk_id = doc.metadata.get(\"chunk_id\", 0)\n",
    "                        chunk_position = doc.metadata.get(\"chunk_position\", \"unknown\")\n",
    "                        keywords = doc.metadata.get(\"keywords\", [])\n",
    "                        \n",
    "                        source_label = f\"{filename} (section {chunk_id + 1})\"\n",
    "                        sources.append(source_label)\n",
    "                        \n",
    "                        source_metadata.append({\n",
    "                            \"filename\": filename,\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"position\": chunk_position,\n",
    "                            \"keywords\": keywords,\n",
    "                            \"content_preview\": doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "                        })\n",
    "                \n",
    "                # M√©tadonn√©es de la r√©ponse\n",
    "                response_metadata = {\n",
    "                    \"template_used\": self.current_template_type,\n",
    "                    \"sources_count\": len(sources),\n",
    "                    \"source_details\": source_metadata,\n",
    "                    \"performance\": self.performance_monitor.get_metrics_summary(\"process_question\", last_n_minutes=1)\n",
    "                }\n",
    "                \n",
    "                # Sauvegarde dans l'historique\n",
    "                self.memory.add_exchange(question, answer, sources)\n",
    "                \n",
    "                return answer, sources, response_metadata\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Erreur lors du traitement: {str(e)}\"\n",
    "                return error_msg, [], {\"error\": str(e)}\n",
    "        \n",
    "        return _process()\n",
    "    \n",
    "    def get_system_status(self) -> Dict:\n",
    "        \"\"\"Retourne le statut d√©taill√© du syst√®me\"\"\"\n",
    "        return {\n",
    "            \"llm_configured\": self.llm is not None,\n",
    "            \"vectorstore_ready\": self.vectorstore_manager.vectorstore is not None,\n",
    "            \"chain_ready\": self.chain is not None,\n",
    "            \"current_template\": self.current_template_type,\n",
    "            \"conversation_length\": len(self.memory.conversation_history),\n",
    "            \"performance_summary\": self.performance_monitor.get_metrics_summary(),\n",
    "            \"system_info\": self.performance_monitor.get_system_info(),\n",
    "            \"available_templates\": self.prompt_manager.get_available_templates()\n",
    "        }\n",
    "    \n",
    "    def switch_template(self, new_template_type: str) -> bool:\n",
    "        \"\"\"Change le template de prompt en cours d'ex√©cution\"\"\"\n",
    "        try:\n",
    "            available_templates = self.prompt_manager.get_available_templates()\n",
    "            if new_template_type not in available_templates:\n",
    "                return False\n",
    "            \n",
    "            # Reconfigurer la cha√Æne avec le nouveau template\n",
    "            if self.chain:\n",
    "                k_current = getattr(self.chain.retriever, 'search_kwargs', {}).get('k', 3)\n",
    "                self.setup_chain_with_template(k_current, new_template_type)\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "# app/main_enhanced.py\n",
    "# Version am√©lior√©e de l'application principale\n",
    "\n",
    "import gradio as gr\n",
    "import os\n",
    "from typing import Optional, Tuple, List\n",
    "import json\n",
    "\n",
    "from app.components.file_processor import FileProcessor\n",
    "from app.components.chat_engine_enhanced import EnhancedChatEngine\n",
    "from app.config import Config\n",
    "from utils.validators import FileValidator, InputValidator\n",
    "from utils.performance import PerformanceMonitor\n",
    "\n",
    "class EnhancedDocumentChatBot:\n",
    "    \"\"\"Application principale am√©lior√©e avec tous les utilitaires\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chat_engine = EnhancedChatEngine()\n",
    "        self.global_monitor = PerformanceMonitor()\n",
    "        self.current_document = None\n",
    "        self.document_processed = False\n",
    "        self.system_status = {}\n",
    "    \n",
    "    def process_document_enhanced(self, \n",
    "                                api_key: str, \n",
    "                                file_obj, \n",
    "                                chunk_size: int, \n",
    "                                k_documents: int,\n",
    "                                template_type: str = 'auto') -> Tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Version am√©lior√©e du traitement de documents\n",
    "        Returns: (status_message, document_info, performance_info)\n",
    "        \"\"\"\n",
    "        @self.global_monitor.measure_performance(\"full_document_processing\")\n",
    "        def _process():\n",
    "            try:\n",
    "                # Validation du fichier\n",
    "                file_valid, file_error = FileValidator.validate_file(file_obj)\n",
    "                if not file_valid:\n",
    "                    return f\"‚ùå {file_error}\", \"\", \"\"\n",
    "                \n",
    "                # Validation de la cl√© API\n",
    "                api_valid, api_error = InputValidator.validate_api_key(api_key.strip(), 'mistral')\n",
    "                if not api_valid:\n",
    "                    return f\"‚ùå {api_error}\", \"\", \"\"\n",
    "                \n",
    "                # Configuration du LLM\n",
    "                self.chat_engine.setup_llm(api_key.strip())\n",
    "                \n",
    "                # Traitement du fichier\n",
    "                text_content, filename = FileProcessor.process_uploaded_file(file_obj)\n",
    "                \n",
    "                # Initialisation de la base vectorielle\n",
    "                self.chat_engine.vectorstore_manager.initialize_vectorstore()\n",
    "                \n",
    "                # Ajout des documents avec m√©tadonn√©es enrichies\n",
    "                chunk_overlap = max(50, chunk_size // 5)\n",
    "                num_chunks, doc_details = self.chat_engine.vectorstore_manager.add_documents_enhanced(\n",
    "                    text_content, filename, chunk_size, chunk_overlap\n",
    "                )\n",
    "                \n",
    "                # Configuration de la cha√Æne avec template intelligent\n",
    "                self.chat_engine.setup_chain_with_template(k_documents, template_type)\n",
    "                \n",
    "                # Mise √† jour de l'√©tat\n",
    "                self.current_document = filename\n",
    "                self.document_processed = True\n",
    "                self.system_status = self.chat_engine.get_system_status()\n",
    "                \n",
    "                # Messages de statut\n",
    "                status = f\"‚úÖ Document trait√© avec succ√®s!\"\n",
    "                \n",
    "                doc_info = f\"\"\"\n",
    "### üìÑ Informations du document\n",
    "- **Fichier**: {filename}\n",
    "- **Type d√©tect√©**: {doc_details['document_type']}\n",
    "- **Chunks cr√©√©s**: {num_chunks}\n",
    "- **Taille moyenne des chunks**: {doc_details['average_chunk_size']:.0f} caract√®res\n",
    "- **Structure pr√©serv√©e**: {doc_details['structure_preserved']} sections\n",
    "- **Mots-cl√©s extraits**: {'‚úÖ' if doc_details['keywords_extracted'] else '‚ùå'}\n",
    "\n",
    "### ‚öôÔ∏è Configuration\n",
    "- **Template utilis√©**: {self.chat_engine.current_template_type}\n",
    "- **Documents r√©cup√©r√©s**: {k_documents}\n",
    "- **Mod√®le**: Mistral AI (mistral-tiny)\n",
    "                \"\"\".strip()\n",
    "                \n",
    "                # Informations de performance\n",
    "                perf_summary = self.global_monitor.get_metrics_summary()\n",
    "                system_info = self.global_monitor.get_system_info()\n",
    "                \n",
    "                perf_info = f\"\"\"\n",
    "### üìä Performance\n",
    "- **Temps de traitement**: {perf_summary.get('avg_duration', 0):.2f}s\n",
    "- **Utilisation m√©moire**: {system_info['memory_percent']:.1f}%\n",
    "- **CPU**: {system_info['cpu_percent']:.1f}%\n",
    "- **Taux de succ√®s**: {perf_summary.get('success_rate', 100):.1f}%\n",
    "                \"\"\".strip()\n",
    "                \n",
    "                return status, doc_info, perf_info\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"‚ùå Erreur: {str(e)}\"\n",
    "                return error_msg, \"\", \"\"\n",
    "        \n",
    "        return _process()\n",
    "    \n",
    "    def chat_enhanced(self, \n",
    "                     message: str, \n",
    "                     history: List[List[str]]) -> Tuple[str, List[List[str]], str]:\n",
    "        \"\"\"\n",
    "        Version am√©lior√©e du chat avec m√©tadonn√©es\n",
    "        Returns: (\"\", updated_history, metadata_info)\n",
    "        \"\"\"\n",
    "        if not self.document_processed:\n",
    "            history.append([message, \"‚ö†Ô∏è Veuillez d'abord traiter un document dans l'onglet Configuration.\"])\n",
    "            return \"\", history, \"\"\n",
    "        \n",
    "        if not message.strip():\n",
    "            return \"\", history, \"\"\n",
    "        \n",
    "        # Traitement am√©lior√© de la question\n",
    "        answer, sources, metadata = self.chat_engine.process_question_enhanced(message)\n",
    "        \n",
    "        # Formatage de la r√©ponse avec sources d√©taill√©es\n",
    "        if sources:\n",
    "            source_details = []\n",
    "            for i, source in enumerate(sources):\n",
    "                if i < len(metadata.get('source_details', [])):\n",
    "                    detail = metadata['source_details'][i]\n",
    "                    keywords_str = ', '.join(detail['keywords'][:3]) if detail['keywords'] else 'N/A'\n",
    "                    source_details.append(f\"üìÑ {source} | üè∑Ô∏è Mots-cl√©s: {keywords_str}\")\n",
    "                else:\n",
    "                    source_details.append(f\"üìÑ {source}\")\n",
    "            \n",
    "            formatted_answer = f\"{answer}\\n\\n**Sources consult√©es:**\\n\" + \"\\n\".join(source_details)\n",
    "        else:\n",
    "            formatted_answer = answer\n",
    "        \n",
    "        # Informations de m√©tadonn√©es pour affichage\n",
    "        metadata_info = f\"\"\"\n",
    "**Template**: {metadata.get('template_used', 'N/A')} | **Sources**: {metadata.get('sources_count', 0)} | **Performance**: {metadata.get('performance', {}).get('avg_duration', 0):.2f}s\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        # Mise √† jour de l'historique\n",
    "        history.append([message, formatted_answer])\n",
    "        \n",
    "        return \"\", history, metadata_info\n",
    "    \n",
    "    def get_detailed_status(self) -> str:\n",
    "        \"\"\"Retourne un statut d√©taill√© du syst√®me\"\"\"\n",
    "        if not self.document_processed:\n",
    "            return \"üî¥ **Statut**: Aucun document trait√©\"\n",
    "        \n",
    "        status = self.chat_engine.get_system_status()\n",
    "        \n",
    "        status_text = f\"\"\"\n",
    "### üü¢ Syst√®me op√©rationnel\n",
    "\n",
    "**Configuration actuelle:**\n",
    "- Template: {status['current_template']}\n",
    "- Conversations: {status['conversation_length']} √©changes\n",
    "- Performance moyenne: {status['performance_summary'].get('avg_duration', 0):.2f}s\n",
    "\n",
    "**Ressources syst√®me:**\n",
    "- M√©moire: {status['system_info']['memory_percent']:.1f}%\n",
    "- CPU: {status['system_info']['cpu_percent']:.1f}%\n",
    "- Espace disque: {status['system_info']['disk_usage_percent']:.1f}%\n",
    "\n",
    "**Templates disponibles:** {', '.join(status['available_templates'])}\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        return status_text\n",
    "    \n",
    "    def export_session_data(self) -> str:\n",
    "        \"\"\"Exporte les donn√©es de la session\"\"\"\n",
    "        if not self.document_processed:\n",
    "            return \"Aucune donn√©e √† exporter\"\n",
    "        \n",
    "        try:\n",
    "            # Donn√©es √† exporter\n",
    "            session_data = {\n",
    "                \"document\": self.current_document,\n",
    "                \"system_status\": self.chat_engine.get_system_status(),\n",
    "                \"conversation_history\": self.chat_engine.memory.conversation_history,\n",
    "                \"performance_metrics\": self.global_monitor.get_metrics_summary()\n",
    "            }\n",
    "            \n",
    "            # Sauvegarde dans un fichier\n",
    "            filename = f\"session_export_{self.current_document}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(session_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "            \n",
    "            return f\"‚úÖ Session export√©e: {filename}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Erreur d'export: {str(e)}\"\n",
    "\n",
    "# Integration dans l'interface Gradio\n",
    "def create_enhanced_interface():\n",
    "    \"\"\"Cr√©e l'interface Gradio am√©lior√©e\"\"\"\n",
    "    \n",
    "    app = EnhancedDocumentChatBot()\n",
    "    \n",
    "    # CSS am√©lior√©\n",
    "    css = \"\"\"\n",
    "    .gradio-container {\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        max-width: 1200px;\n",
    "        margin: 0 auto;\n",
    "    }\n",
    "    .tab-nav button {\n",
    "        font-size: 16px;\n",
    "        font-weight: 500;\n",
    "    }\n",
    "    .performance-info {\n",
    "        background: #f0f0f0;\n",
    "        padding: 10px;\n",
    "        border-radius: 5px;\n",
    "        font-size: 12px;\n",
    "        color: #666;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(css=css, title=\"ChatBot Documentaire Avanc√© - Mistral AI\", theme=gr.themes.Soft()) as interface:\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        # üìÑ ChatBot Documentaire Avanc√© avec Mistral AI\n",
    "        \n",
    "        **Nouvelle version** avec d√©coupage intelligent, templates optimis√©s et monitoring de performance !\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            # Onglet Configuration Avanc√©\n",
    "            with gr.Tab(\"‚öôÔ∏è Configuration Avanc√©e\", id=\"config\"):\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        api_key_input = gr.Textbox(\n",
    "                            label=\"üîë Cl√© API Mistral AI\",\n",
    "                            placeholder=\"Saisissez votre cl√© API Mistral...\",\n",
    "                            type=\"password\",\n",
    "                            info=\"Validation automatique de la cl√©\"\n",
    "                        )\n",
    "                        \n",
    "                        file_upload = gr.File(\n",
    "                            label=\"üìÅ Fichier √† analyser\",\n",
    "                            file_types=[\".pdf\", \".docx\", \".txt\"],\n",
    "                            type=\"binary\"\n",
    "                        )\n",
    "                        \n",
    "                        with gr.Row():\n",
    "                            chunk_size_slider = gr.Slider(\n",
    "                                minimum=200,\n",
    "                                maximum=2000,\n",
    "                                value=Config.DEFAULT_CHUNK_SIZE,\n",
    "                                step=100,\n",
    "                                label=\"üìè Taille des chunks\",\n",
    "                                info=\"D√©coupage intelligent automatique\"\n",
    "                            )\n",
    "                            \n",
    "                            k_documents_slider = gr.Slider(\n",
    "                                minimum=1,\n",
    "                                maximum=10,\n",
    "                                value=Config.DEFAULT_K_DOCUMENTS,\n",
    "                                step=1,\n",
    "                                label=\"üîç Documents r√©cup√©r√©s\"\n",
    "                            )\n",
    "                        \n",
    "                        template_dropdown = gr.Dropdown(\n",
    "                            choices=['auto', 'general', 'academic', 'technical', 'legal'],\n",
    "                            value='auto',\n",
    "                            label=\"üéØ Type de template\",\n",
    "                            info=\"Auto = d√©tection automatique\"\n",
    "                        )\n",
    "                        \n",
    "                        process_btn = gr.Button(\"üöÄ Traiter le document\", variant=\"primary\", size=\"lg\")\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        status_output = gr.Textbox(\n",
    "                            label=\"üìä Statut\",\n",
    "                            interactive=False,\n",
    "                            lines=2\n",
    "                        )\n",
    "                        \n",
    "                        doc_info_output = gr.Markdown(\n",
    "                            label=\"‚ÑπÔ∏è Informations d√©taill√©es\"\n",
    "                        )\n",
    "                        \n",
    "                        perf_info_output = gr.Markdown(\n",
    "                            label=\"‚ö° Performance\",\n",
    "                            elem_classes=[\"performance-info\"]\n",
    "                        )\n",
    "            \n",
    "            # Onglet Chat Am√©lior√©\n",
    "            with gr.Tab(\"üí¨ Chat Intelligent\", id=\"chat\"):\n",
    "                \n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"Conversation avec intelligence augment√©e\",\n",
    "                    height=500,\n",
    "                    show_copy_button=True\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    msg_input = gr.Textbox(\n",
    "                        label=\"Votre question\",\n",
    "                        placeholder=\"Question intelligente avec validation automatique...\",\n",
    "                        scale=4\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        send_btn = gr.Button(\"üì§ Envoyer\", variant=\"primary\")\n",
    "                        clear_btn = gr.Button(\"üóëÔ∏è Effacer\", variant=\"secondary\")\n",
    "                \n",
    "                # Informations de m√©tadonn√©es en temps r√©el\n",
    "                metadata_display = gr.Textbox(\n",
    "                    label=\"üîç M√©tadonn√©es de la r√©ponse\",\n",
    "                    interactive=False,\n",
    "                    lines=1,\n",
    "                    elem_classes=[\"performance-info\"]\n",
    "                )\n",
    "            \n",
    "            # Onglet Monitoring\n",
    "            with gr.Tab(\"üìä Monitoring\", id=\"monitoring\"):\n",
    "                \n",
    "                with gr.Row():\n",
    "                    refresh_btn = gr.Button(\"üîÑ Actualiser\", variant=\"secondary\")\n",
    "                    export_btn = gr.Button(\"üíæ Exporter Session\", variant=\"primary\")\n",
    "                \n",
    "                system_status_display = gr.Markdown(\n",
    "                    label=\"Statut du syst√®me\",\n",
    "                    value=\"Aucune donn√©e disponible\"\n",
    "                )\n",
    "                \n",
    "                export_result_display = gr.Textbox(\n",
    "                    label=\"R√©sultat d'export\",\n",
    "                    interactive=False\n",
    "                )\n",
    "        \n",
    "        # √âv√©nements\n",
    "        process_btn.click(\n",
    "            fn=app.process_document_enhanced,\n",
    "            inputs=[api_key_input, file_upload, chunk_size_slider, k_documents_slider, template_dropdown],\n",
    "            outputs=[status_output, doc_info_output, perf_info_output]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            fn=app.chat_enhanced,\n",
    "            inputs=[msg_input, chatbot],\n",
    "            outputs=[msg_input, chatbot, metadata_display]\n",
    "        )\n",
    "        \n",
    "        msg_input.submit(\n",
    "            fn=app.chat_enhanced,\n",
    "            inputs=[msg_input, chatbot],\n",
    "            outputs=[msg_input, chatbot, metadata_display]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            fn=lambda: ([], \"\"),\n",
    "            outputs=[chatbot, metadata_display]\n",
    "        )\n",
    "        \n",
    "        refresh_btn.click(\n",
    "            fn=app.get_detailed_status,\n",
    "            outputs=[system_status_display]\n",
    "        )\n",
    "        \n",
    "        export_btn.click(\n",
    "            fn=app.export_session_data,\n",
    "            outputs=[export_result_display]\n",
    "        )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from datetime import datetime\n",
    "    os.makedirs(\"./data/uploads\", exist_ok=True)\n",
    "    os.makedirs(\"./data/vectorstore\", exist_ok=True)\n",
    "    \n",
    "    interface = create_enhanced_interface()\n",
    "    interface.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "        share=False,\n",
    "        debug=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ Int√©gration compl√®te des utilitaires !\n",
    "Voici comment les utilitaires sont int√©gr√©s dans votre application :\n",
    "‚ú® Nouvelles fonctionnalit√©s ajout√©es :\n",
    "1. D√©coupage intelligent (SmartTextSplitter)\n",
    "\n",
    "D√©tection automatique du type de document (acad√©mique, technique, l√©gal)\n",
    "Pr√©servation de structure avec mots-cl√©s extraits\n",
    "M√©tadonn√©es enrichies pour chaque chunk\n",
    "\n",
    "2. Templates de prompts optimis√©s (PromptTemplateManager)\n",
    "\n",
    "6 templates sp√©cialis√©s selon le type de contenu\n",
    "S√©lection automatique ou manuelle\n",
    "Optimisation contextuelle des r√©ponses\n",
    "\n",
    "3. Validation robuste (Validators)\n",
    "\n",
    "Validation des fichiers : taille, format, s√©curit√©\n",
    "Validation des entr√©es : cl√©s API, param√®tres, questions\n",
    "Nettoyage automatique des inputs utilisateur\n",
    "\n",
    "4. Monitoring de performance (PerformanceMonitor)\n",
    "\n",
    "M√©triques en temps r√©el : temps, m√©moire, CPU\n",
    "Historique des performances avec export\n",
    "Surveillance syst√®me automatique\n",
    "\n",
    "üÜï Interface am√©lior√©e :\n",
    "Nouveaux onglets :\n",
    "\n",
    "Configuration Avanc√©e : s√©lection de templates, validation en temps r√©el\n",
    "Chat Intelligent : m√©tadonn√©es des r√©ponses, validation des questions\n",
    "Monitoring : statut syst√®me, export de session\n",
    "\n",
    "Nouvelles informations affich√©es :\n",
    "\n",
    "Type de document d√©tect√© automatiquement\n",
    "Template de prompt utilis√©\n",
    "Mots-cl√©s extraits des chunks\n",
    "M√©triques de performance en temps r√©el\n",
    "Sources d√©taill√©es avec pr√©visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "document-chatbot/\n",
    "‚îú‚îÄ‚îÄ app/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # Version originale\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ main_enhanced.py           # Version am√©lior√©e avec utilitaires\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ components/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ file_processor.py\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ vectorstore.py         # Version originale\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ vectorstore_enhanced.py # Version am√©lior√©e\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ chat_engine.py         # Version originale\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ chat_engine_enhanced.py # Version am√©lior√©e\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ memory.py\n",
    "‚îú‚îÄ‚îÄ utils/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # ‚úÖ Cr√©√©\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ text_splitter.py          # ‚úÖ Cr√©√©\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ prompt_templates.py       # ‚úÖ Cr√©√©\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ validators.py             # ‚úÖ Cr√©√©\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ performance.py            # ‚úÖ Cr√©√©\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ uploads/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ vectorstore/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ exports/                  # Nouveau : exports de session\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ .env.example\n",
    "‚îî‚îÄ‚îÄ README.md\n",
    "```\n",
    "\n",
    "## üîß **Installation et utilisation :**## üéØ **R√©sum√© de votre projet complet**\n",
    "\n",
    "Vous avez maintenant **deux versions** de votre ChatBot documentaire :\n",
    "\n",
    "### üì± **Version Simple** (`main.py`)\n",
    "- Interface Gradio basique\n",
    "- Fonctionnalit√©s essentielles\n",
    "- Facile √† d√©ployer et utiliser\n",
    "\n",
    "### üöÄ **Version Avanc√©e** (`main_enhanced.py`)\n",
    "- **D√©coupage intelligent** avec d√©tection automatique du type\n",
    "- **Templates optimis√©s** pour chaque contexte\n",
    "- **Validation robuste** et s√©curit√© renforc√©e\n",
    "- **Monitoring complet** des performances\n",
    "- **Interface enrichie** avec 3 onglets sp√©cialis√©s\n",
    "\n",
    "## üî• **Points forts de la version avanc√©e :**\n",
    "\n",
    "1. **üß† Intelligence augment√©e** : d√©tection automatique + templates sp√©cialis√©s\n",
    "2. **‚ö° Performance optimis√©e** : monitoring en temps r√©el + m√©triques d√©taill√©es  \n",
    "3. **üîí S√©curit√© renforc√©e** : validation compl√®te + nettoyage automatique\n",
    "4. **üìä Observabilit√© totale** : export de sessions + historique complet\n",
    "5. **üéØ R√©ponses de qualit√©** : adaptation contextuelle + sources enrichies\n",
    "\n",
    "## üöÄ **Pour commencer :**\n",
    "\n",
    "1. **Choisissez votre version** (simple ou avanc√©e)\n",
    "2. **Installez les d√©pendances** avec pip\n",
    "3. **Obtenez votre cl√© Mistral AI** (gratuite)\n",
    "4. **Lancez l'application** : `python app/main_enhanced.py`\n",
    "5. **Acc√©dez √†** : `http://localhost:7860`\n",
    "\n",
    "Votre ChatBot documentaire est maintenant **pr√™t √† rivaliser avec SecInsights.ai** ! üéâ\n",
    "\n",
    "Avez-vous des questions sur l'installation ou voulez-vous que j'ajoute d'autres fonctionnalit√©s sp√©cifiques ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
