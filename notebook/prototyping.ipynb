{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DontREADME**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture du Projet ChatBot Documentaire\n",
    "\n",
    "## 🏗️ Structure des dossiers\n",
    "\n",
    "```\n",
    "document-chatbot/\n",
    "├── 📁 app/\n",
    "│   ├── __init__.py\n",
    "│   ├── main.py                    # Interface Gradio principale\n",
    "│   ├── config.py                  # Configuration et constantes\n",
    "│   └── components/\n",
    "│       ├── __init__.py\n",
    "│       ├── file_processor.py      # Traitement des fichiers (PDF, DOCX, TXT)\n",
    "│       ├── vectorstore.py         # Gestion ChromaDB et embeddings\n",
    "│       ├── chat_engine.py         # Logique de conversation et RAG\n",
    "│       └── memory.py              # Gestion de l'historique\n",
    "├── 📁 data/\n",
    "│   ├── uploads/                   # Fichiers uploadés temporaires\n",
    "│   └── vectorstore/               # Base ChromaDB persistante\n",
    "├── 📁 utils/\n",
    "│   ├── __init__.py\n",
    "│   ├── text_splitter.py          # Découpage intelligent du texte\n",
    "│   └── prompt_templates.py       # Templates de prompts pour le LLM\n",
    "├── requirements.txt\n",
    "├── .env.example\n",
    "├── .gitignore\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "## 🔧 Architecture technique\n",
    "\n",
    "### 1. Interface Utilisateur (Gradio)\n",
    "```python\n",
    "# Structure de l'interface\n",
    "Interface Gradio:\n",
    "├── Onglet \"Configuration\"\n",
    "│   ├── Input: Clé API OpenAI/Mistral\n",
    "│   ├── File Upload: PDF/DOCX/TXT\n",
    "│   ├── Slider: Chunk Size (100-2000)\n",
    "│   ├── Slider: Nombre de documents K (1-10)\n",
    "│   └── Button: \"Traiter le document\"\n",
    "├── Onglet \"Chat\"\n",
    "│   ├── Chatbot: Historique des conversations\n",
    "│   ├── Textbox: Question utilisateur\n",
    "│   └── Button: \"Envoyer\"\n",
    "└── Onglet \"Informations\"\n",
    "    ├── Display: Statut du traitement\n",
    "    ├── Display: Nombre de chunks créés\n",
    "    └── Display: Modèle utilisé\n",
    "```\n",
    "\n",
    "### 2. Pipeline de traitement\n",
    "```mermaid\n",
    "graph TD\n",
    "A[Upload Fichier] --> B[Extraction Texte]\n",
    "B --> C[Découpage en Chunks]\n",
    "C --> D[Génération Embeddings]\n",
    "D --> E[Stockage ChromaDB]\n",
    "E --> F[Prêt pour Questions]\n",
    "\n",
    "G[Question Utilisateur] --> H[Recherche Similarité]\n",
    "H --> I[Récupération Contexte]\n",
    "I --> J[Génération Réponse LLM]\n",
    "J --> K[Mise à jour Historique]\n",
    "```\n",
    "\n",
    "## 🧩 Composants principaux\n",
    "\n",
    "### 1. **FileProcessor** (`file_processor.py`)\n",
    "- **Responsabilité** : Extraction de texte des fichiers\n",
    "- **Formats supportés** : PDF (PyPDF2), DOCX (python-docx), TXT\n",
    "- **Fonctions** :\n",
    "  - `extract_text_from_pdf()`\n",
    "  - `extract_text_from_docx()`\n",
    "  - `extract_text_from_txt()`\n",
    "  - `process_uploaded_file()`\n",
    "\n",
    "### 2. **VectorStore** (`vectorstore.py`)\n",
    "- **Responsabilité** : Gestion des embeddings et ChromaDB\n",
    "- **Fonctions** :\n",
    "  - `initialize_vectorstore()`\n",
    "  - `add_documents_to_vectorstore()`\n",
    "  - `search_similar_documents()`\n",
    "  - `clear_vectorstore()`\n",
    "\n",
    "### 3. **ChatEngine** (`chat_engine.py`)\n",
    "- **Responsabilité** : Logique RAG et interaction LLM\n",
    "- **Fonctions** :\n",
    "  - `setup_retrieval_chain()`\n",
    "  - `process_question()`\n",
    "  - `generate_response()`\n",
    "  - `format_context()`\n",
    "\n",
    "### 4. **Memory** (`memory.py`)\n",
    "- **Responsabilité** : Historique des conversations\n",
    "- **Fonctions** :\n",
    "  - `add_message()`\n",
    "  - `get_conversation_history()`\n",
    "  - `clear_history()`\n",
    "  - `format_history_for_display()`\n",
    "\n",
    "## 🔄 Flow de données\n",
    "\n",
    "### Étape 1: Initialisation\n",
    "1. L'utilisateur saisit sa clé API\n",
    "2. Configure les paramètres (chunk_size, k)\n",
    "3. Upload un document\n",
    "\n",
    "### Étape 2: Traitement du document\n",
    "1. **FileProcessor** extrait le texte\n",
    "2. **TextSplitter** découpe en chunks\n",
    "3. **VectorStore** génère les embeddings via OpenAI/Mistral\n",
    "4. Stockage dans ChromaDB\n",
    "\n",
    "### Étape 3: Conversation\n",
    "1. L'utilisateur pose une question\n",
    "2. **VectorStore** recherche les chunks pertinents\n",
    "3. **ChatEngine** combine contexte + historique + question\n",
    "4. LLM génère la réponse\n",
    "5. **Memory** sauvegarde l'échange\n",
    "\n",
    "## 🛠️ Stack technique\n",
    "\n",
    "- **Interface** : Gradio (plus simple que Streamlit pour ce cas)\n",
    "- **LLM Framework** : LangChain\n",
    "- **Vector Database** : ChromaDB (local, pas besoin de serveur)\n",
    "- **LLM** : OpenAI GPT-3.5-turbo (ou Mistral AI en alternative gratuite)\n",
    "- **Embeddings** : OpenAI text-embedding-ada-002 (ou sentence-transformers gratuit)\n",
    "- **Document Processing** : PyPDF2, python-docx\n",
    "- **Text Splitting** : LangChain RecursiveCharacterTextSplitter\n",
    "\n",
    "## 📦 Dépendances principales\n",
    "\n",
    "```txt\n",
    "gradio>=4.0.0\n",
    "langchain>=0.1.0\n",
    "langchain-openai\n",
    "langchain-community\n",
    "chromadb>=0.4.0\n",
    "openai>=1.0.0\n",
    "PyPDF2>=3.0.0\n",
    "python-docx>=0.8.11\n",
    "sentence-transformers  # Alternative gratuite aux embeddings OpenAI\n",
    "python-dotenv\n",
    "```\n",
    "\n",
    "## 🎯 Fonctionnalités avancées (optionnelles)\n",
    "\n",
    "- **Multi-documents** : Traiter plusieurs fichiers simultanément\n",
    "- **Export conversation** : Sauvegarder l'historique en JSON/PDF\n",
    "- **Métriques** : Temps de réponse, similarité des chunks\n",
    "- **Streaming** : Réponses en temps réel\n",
    "- **Templates personnalisés** : Différents styles de réponse\n",
    "\n",
    "Cette architecture modulaire permet une maintenance facile et des extensions futures. Voulez-vous que je commence par implémenter un composant spécifique ou préférez-vous que je crée l'application complète ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt\n",
    "\"\"\"\n",
    "gradio>=4.0.0\n",
    "langchain>=0.1.0\n",
    "langchain-community\n",
    "langchain-mistralai\n",
    "chromadb>=0.4.0\n",
    "mistralai>=0.1.0\n",
    "PyPDF2>=3.0.0\n",
    "python-docx>=0.8.11\n",
    "sentence-transformers>=2.2.0\n",
    "python-dotenv>=1.0.0\n",
    "tiktoken>=0.5.0\n",
    "\"\"\"\n",
    "\n",
    "# app/config.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class Config:\n",
    "    # Mistral AI Configuration\n",
    "    MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"\")\n",
    "    MISTRAL_MODEL = \"mistral-tiny\"  # ou \"mistral-small\", \"mistral-medium\"\n",
    "    \n",
    "    # ChromaDB Configuration\n",
    "    CHROMADB_PATH = \"./data/vectorstore\"\n",
    "    COLLECTION_NAME = \"document_embeddings\"\n",
    "    \n",
    "    # Text Processing\n",
    "    DEFAULT_CHUNK_SIZE = 1000\n",
    "    DEFAULT_CHUNK_OVERLAP = 200\n",
    "    DEFAULT_K_DOCUMENTS = 3\n",
    "    \n",
    "    # Supported file types\n",
    "    SUPPORTED_EXTENSIONS = ['.pdf', '.docx', '.txt']\n",
    "    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\n",
    "\n",
    "# app/components/file_processor.py\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "\n",
    "class FileProcessor:\n",
    "    \"\"\"Traitement des fichiers uploadés\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_pdf(file_path: str) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier PDF\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text.strip()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erreur lors de la lecture du PDF: {str(e)}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_docx(file_path: str) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier DOCX\"\"\"\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            text = \"\"\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erreur lors de la lecture du DOCX: {str(e)}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_txt(file_path: str) -> str:\n",
    "        \"\"\"Extrait le texte d'un fichier TXT\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                return file.read().strip()\n",
    "        except UnicodeDecodeError:\n",
    "            # Essayer avec d'autres encodages\n",
    "            for encoding in ['latin-1', 'cp1252', 'iso-8859-1']:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding=encoding) as file:\n",
    "                        return file.read().strip()\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            raise Exception(\"Impossible de décoder le fichier texte\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erreur lors de la lecture du TXT: {str(e)}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def process_uploaded_file(cls, file_obj) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Traite un fichier uploadé et retourne le texte extrait\n",
    "        Returns: (text_content, filename)\n",
    "        \"\"\"\n",
    "        if file_obj is None:\n",
    "            raise ValueError(\"Aucun fichier fourni\")\n",
    "        \n",
    "        # Sauvegarder temporairement le fichier\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file_obj.name).suffix) as tmp_file:\n",
    "            tmp_file.write(file_obj.read() if hasattr(file_obj, 'read') else file_obj)\n",
    "            tmp_path = tmp_file.name\n",
    "        \n",
    "        try:\n",
    "            file_extension = Path(file_obj.name).suffix.lower()\n",
    "            filename = Path(file_obj.name).name\n",
    "            \n",
    "            if file_extension == '.pdf':\n",
    "                text = cls.extract_text_from_pdf(tmp_path)\n",
    "            elif file_extension == '.docx':\n",
    "                text = cls.extract_text_from_docx(tmp_path)\n",
    "            elif file_extension == '.txt':\n",
    "                text = cls.extract_text_from_txt(tmp_path)\n",
    "            else:\n",
    "                raise ValueError(f\"Format de fichier non supporté: {file_extension}\")\n",
    "            \n",
    "            if not text.strip():\n",
    "                raise ValueError(\"Le fichier ne contient pas de texte extractible\")\n",
    "            \n",
    "            return text, filename\n",
    "            \n",
    "        finally:\n",
    "            # Nettoyer le fichier temporaire\n",
    "            if os.path.exists(tmp_path):\n",
    "                os.unlink(tmp_path)\n",
    "\n",
    "# app/components/vectorstore.py\n",
    "import os\n",
    "from typing import List, Optional\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "from app.config import Config\n",
    "\n",
    "class VectorStoreManager:\n",
    "    \"\"\"Gestion de la base de données vectorielle ChromaDB\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = None\n",
    "        self.vectorstore = None\n",
    "        self.text_splitter = None\n",
    "        self._setup_embeddings()\n",
    "    \n",
    "    def _setup_embeddings(self):\n",
    "        \"\"\"Configure les embeddings avec sentence-transformers (gratuit)\"\"\"\n",
    "        # Utilise un modèle multilingue français/anglais\n",
    "        model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={'device': 'cpu'},  # Utilise CPU pour la compatibilité\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "    \n",
    "    def setup_text_splitter(self, chunk_size: int = Config.DEFAULT_CHUNK_SIZE, \n",
    "                           chunk_overlap: int = Config.DEFAULT_CHUNK_OVERLAP):\n",
    "        \"\"\"Configure le découpeur de texte\"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def initialize_vectorstore(self, collection_name: str = Config.COLLECTION_NAME):\n",
    "        \"\"\"Initialise la base vectorielle ChromaDB\"\"\"\n",
    "        # Créer le dossier s'il n'existe pas\n",
    "        os.makedirs(Config.CHROMADB_PATH, exist_ok=True)\n",
    "        \n",
    "        # Configuration ChromaDB\n",
    "        client = chromadb.PersistentClient(path=Config.CHROMADB_PATH)\n",
    "        \n",
    "        # Supprimer la collection existante si elle existe\n",
    "        try:\n",
    "            client.delete_collection(collection_name)\n",
    "        except ValueError:\n",
    "            pass  # Collection n'existe pas\n",
    "        \n",
    "        self.vectorstore = Chroma(\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=self.embeddings,\n",
    "        )\n",
    "    \n",
    "    def add_documents(self, text: str, filename: str, chunk_size: int, chunk_overlap: int) -> int:\n",
    "        \"\"\"\n",
    "        Ajoute des documents à la base vectorielle\n",
    "        Returns: nombre de chunks créés\n",
    "        \"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"VectorStore non initialisé\")\n",
    "        \n",
    "        # Configuration du text splitter\n",
    "        self.setup_text_splitter(chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Découpage du texte\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        # Création des documents avec métadonnées\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"filename\": filename,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"total_chunks\": len(chunks)\n",
    "                }\n",
    "            )\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "        \n",
    "        # Ajout à la base vectorielle\n",
    "        self.vectorstore.add_documents(documents)\n",
    "        \n",
    "        return len(chunks)\n",
    "    \n",
    "    def search_similar_documents(self, query: str, k: int = Config.DEFAULT_K_DOCUMENTS) -> List[Document]:\n",
    "        \"\"\"Recherche les documents similaires à la requête\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return []\n",
    "        \n",
    "        return self.vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    def get_retriever(self, k: int = Config.DEFAULT_K_DOCUMENTS):\n",
    "        \"\"\"Retourne un retriever pour LangChain\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"VectorStore non initialisé\")\n",
    "        \n",
    "        return self.vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# app/components/memory.py\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "class ConversationMemory:\n",
    "    \"\"\"Gestion de l'historique des conversations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversation_history: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def add_exchange(self, question: str, answer: str, sources: List[str] = None):\n",
    "        \"\"\"Ajoute un échange question-réponse à l'historique\"\"\"\n",
    "        exchange = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources or []\n",
    "        }\n",
    "        self.conversation_history.append(exchange)\n",
    "    \n",
    "    def get_recent_history(self, n_exchanges: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Retourne l'historique récent formaté pour le contexte LLM\n",
    "        \"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return \"\"\n",
    "        \n",
    "        recent = self.conversation_history[-n_exchanges:]\n",
    "        history_text = \"Historique de la conversation:\\n\"\n",
    "        \n",
    "        for exchange in recent:\n",
    "            history_text += f\"Q: {exchange['question']}\\n\"\n",
    "            history_text += f\"R: {exchange['answer']}\\n\\n\"\n",
    "        \n",
    "        return history_text\n",
    "    \n",
    "    def get_formatted_history(self) -> List[List[str]]:\n",
    "        \"\"\"Retourne l'historique formaté pour l'affichage Gradio\"\"\"\n",
    "        formatted = []\n",
    "        for exchange in self.conversation_history:\n",
    "            formatted.append([exchange[\"question\"], exchange[\"answer\"]])\n",
    "        return formatted\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Efface l'historique\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# app/components/chat_engine.py\n",
    "from typing import Optional, Tuple, List\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from app.components.vectorstore import VectorStoreManager\n",
    "from app.components.memory import ConversationMemory\n",
    "\n",
    "class ChatEngine:\n",
    "    \"\"\"Moteur de conversation avec RAG\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.vectorstore_manager = VectorStoreManager()\n",
    "        self.memory = ConversationMemory()\n",
    "        self.chain = None\n",
    "        \n",
    "        # Template de prompt en français\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "            template=\"\"\"Tu es un assistant IA spécialisé dans l'analyse de documents. \n",
    "Réponds aux questions en te basant UNIQUEMENT sur le contexte fourni.\n",
    "\n",
    "Contexte des documents:\n",
    "{context}\n",
    "\n",
    "Historique de la conversation:\n",
    "{chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Réponds de manière précise et concise\n",
    "- Si l'information n'est pas dans le contexte, dis clairement \"Je ne trouve pas cette information dans le document fourni\"\n",
    "- Cite des passages spécifiques quand c'est pertinent\n",
    "- Reste factuel et objectif\n",
    "\n",
    "Réponse:\"\"\"\n",
    "        )\n",
    "    \n",
    "    def setup_llm(self, api_key: str, model: str = \"mistral-tiny\"):\n",
    "        \"\"\"Configure le modèle Mistral AI\"\"\"\n",
    "        if not api_key:\n",
    "            raise ValueError(\"Clé API Mistral requise\")\n",
    "        \n",
    "        self.llm = ChatMistralAI(\n",
    "            mistral_api_key=api_key,\n",
    "            model=model,\n",
    "            temperature=0.1,  # Réponses plus déterministes\n",
    "            max_tokens=1000\n",
    "        )\n",
    "    \n",
    "    def setup_chain(self, k_documents: int = 3):\n",
    "        \"\"\"Configure la chaîne de traitement RAG\"\"\"\n",
    "        if not self.llm:\n",
    "            raise ValueError(\"LLM non configuré\")\n",
    "        \n",
    "        if not self.vectorstore_manager.vectorstore:\n",
    "            raise ValueError(\"VectorStore non initialisé\")\n",
    "        \n",
    "        # Mémoire pour la conversation\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            output_key=\"answer\"\n",
    "        )\n",
    "        \n",
    "        # Chaîne conversationnelle avec retrieval\n",
    "        self.chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=self.llm,\n",
    "            retriever=self.vectorstore_manager.get_retriever(k=k_documents),\n",
    "            memory=memory,\n",
    "            return_source_documents=True,\n",
    "            combine_docs_chain_kwargs={\"prompt\": self.prompt_template}\n",
    "        )\n",
    "    \n",
    "    def process_question(self, question: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Traite une question et retourne la réponse avec les sources\n",
    "        Returns: (answer, sources)\n",
    "        \"\"\"\n",
    "        if not self.chain:\n",
    "            return \"Erreur: Système non configuré\", []\n",
    "        \n",
    "        if not question.strip():\n",
    "            return \"Veuillez poser une question.\", []\n",
    "        \n",
    "        try:\n",
    "            # Exécution de la chaîne\n",
    "            result = self.chain({\"question\": question})\n",
    "            \n",
    "            answer = result.get(\"answer\", \"Pas de réponse générée\")\n",
    "            \n",
    "            # Extraction des sources\n",
    "            sources = []\n",
    "            if \"source_documents\" in result:\n",
    "                for doc in result[\"source_documents\"]:\n",
    "                    filename = doc.metadata.get(\"filename\", \"Document\")\n",
    "                    chunk_id = doc.metadata.get(\"chunk_id\", 0)\n",
    "                    sources.append(f\"{filename} (section {chunk_id + 1})\")\n",
    "            \n",
    "            # Sauvegarde dans l'historique\n",
    "            self.memory.add_exchange(question, answer, sources)\n",
    "            \n",
    "            return answer, sources\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Erreur lors du traitement: {str(e)}\"\n",
    "            return error_msg, []\n",
    "    \n",
    "    def get_conversation_history(self):\n",
    "        \"\"\"Retourne l'historique formaté pour Gradio\"\"\"\n",
    "        return self.memory.get_formatted_history()\n",
    "    \n",
    "    def clear_conversation(self):\n",
    "        \"\"\"Efface l'historique de conversation\"\"\"\n",
    "        self.memory.clear_history()\n",
    "        if self.chain and hasattr(self.chain, 'memory'):\n",
    "            self.chain.memory.clear()\n",
    "\n",
    "# app/main.py\n",
    "import gradio as gr\n",
    "import os\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "from app.components.file_processor import FileProcessor\n",
    "from app.components.chat_engine import ChatEngine\n",
    "from app.config import Config\n",
    "\n",
    "class DocumentChatBot:\n",
    "    \"\"\"Application principale du ChatBot documentaire\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chat_engine = ChatEngine()\n",
    "        self.current_document = None\n",
    "        self.document_processed = False\n",
    "        \n",
    "    def process_document(self, \n",
    "                        api_key: str, \n",
    "                        file_obj, \n",
    "                        chunk_size: int, \n",
    "                        k_documents: int) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Traite le document uploadé\n",
    "        Returns: (status_message, document_info)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validation des inputs\n",
    "            if not api_key.strip():\n",
    "                return \"❌ Veuillez saisir votre clé API Mistral\", \"\"\n",
    "            \n",
    "            if file_obj is None:\n",
    "                return \"❌ Veuillez sélectionner un fichier\", \"\"\n",
    "            \n",
    "            # Configuration du LLM\n",
    "            self.chat_engine.setup_llm(api_key)\n",
    "            \n",
    "            # Traitement du fichier\n",
    "            text_content, filename = FileProcessor.process_uploaded_file(file_obj)\n",
    "            \n",
    "            # Initialisation de la base vectorielle\n",
    "            self.chat_engine.vectorstore_manager.initialize_vectorstore()\n",
    "            \n",
    "            # Ajout des documents\n",
    "            chunk_overlap = max(50, chunk_size // 5)  # 20% de chevauchement\n",
    "            num_chunks = self.chat_engine.vectorstore_manager.add_documents(\n",
    "                text_content, filename, chunk_size, chunk_overlap\n",
    "            )\n",
    "            \n",
    "            # Configuration de la chaîne de traitement\n",
    "            self.chat_engine.setup_chain(k_documents)\n",
    "            \n",
    "            # État mis à jour\n",
    "            self.current_document = filename\n",
    "            self.document_processed = True\n",
    "            \n",
    "            status = f\"✅ Document traité avec succès!\"\n",
    "            doc_info = f\"\"\"\n",
    "📄 **Fichier**: {filename}\n",
    "📊 **Chunks créés**: {num_chunks}\n",
    "⚙️ **Taille des chunks**: {chunk_size} caractères\n",
    "🔍 **Documents récupérés**: {k_documents}\n",
    "🤖 **Modèle**: Mistral AI (mistral-tiny)\n",
    "            \"\"\".strip()\n",
    "            \n",
    "            return status, doc_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"❌ Erreur: {str(e)}\"\n",
    "            return error_msg, \"\"\n",
    "    \n",
    "    def chat_with_document(self, message: str, history: List[List[str]]) -> Tuple[str, List[List[str]]]:\n",
    "        \"\"\"\n",
    "        Traite une question du chat\n",
    "        Returns: (\"\", updated_history)\n",
    "        \"\"\"\n",
    "        if not self.document_processed:\n",
    "            history.append([message, \"⚠️ Veuillez d'abord traiter un document dans l'onglet Configuration.\"])\n",
    "            return \"\", history\n",
    "        \n",
    "        if not message.strip():\n",
    "            return \"\", history\n",
    "        \n",
    "        # Traitement de la question\n",
    "        answer, sources = self.chat_engine.process_question(message)\n",
    "        \n",
    "        # Formatage de la réponse avec sources\n",
    "        if sources:\n",
    "            formatted_answer = f\"{answer}\\n\\n📚 **Sources**: {', '.join(sources)}\"\n",
    "        else:\n",
    "            formatted_answer = answer\n",
    "        \n",
    "        # Mise à jour de l'historique\n",
    "        history.append([message, formatted_answer])\n",
    "        \n",
    "        return \"\", history\n",
    "    \n",
    "    def clear_chat(self) -> List[List[str]]:\n",
    "        \"\"\"Efface l'historique du chat\"\"\"\n",
    "        if self.chat_engine:\n",
    "            self.chat_engine.clear_conversation()\n",
    "        return []\n",
    "    \n",
    "    def create_interface(self):\n",
    "        \"\"\"Crée l'interface Gradio\"\"\"\n",
    "        \n",
    "        # CSS personnalisé\n",
    "        css = \"\"\"\n",
    "        .gradio-container {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        }\n",
    "        .tab-nav button {\n",
    "            font-size: 16px;\n",
    "            font-weight: 500;\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        with gr.Blocks(css=css, title=\"ChatBot Documentaire - Mistral AI\", theme=gr.themes.Soft()) as interface:\n",
    "            \n",
    "            gr.Markdown(\"\"\"\n",
    "            # 📄 ChatBot Documentaire avec Mistral AI\n",
    "            \n",
    "            Posez des questions sur vos documents PDF, DOCX et TXT grâce à l'IA !\n",
    "            \"\"\")\n",
    "            \n",
    "            with gr.Tabs():\n",
    "                \n",
    "                # Onglet Configuration\n",
    "                with gr.Tab(\"⚙️ Configuration\", id=\"config\"):\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        with gr.Column(scale=2):\n",
    "                            api_key_input = gr.Textbox(\n",
    "                                label=\"🔑 Clé API Mistral AI\",\n",
    "                                placeholder=\"Saisissez votre clé API Mistral...\",\n",
    "                                type=\"password\",\n",
    "                                info=\"Obtenez votre clé gratuite sur https://console.mistral.ai/\"\n",
    "                            )\n",
    "                            \n",
    "                            file_upload = gr.File(\n",
    "                                label=\"📁 Fichier à analyser\",\n",
    "                                file_types=[\".pdf\", \".docx\", \".txt\"],\n",
    "                                type=\"binary\"\n",
    "                            )\n",
    "                            \n",
    "                            with gr.Row():\n",
    "                                chunk_size_slider = gr.Slider(\n",
    "                                    minimum=200,\n",
    "                                    maximum=2000,\n",
    "                                    value=Config.DEFAULT_CHUNK_SIZE,\n",
    "                                    step=100,\n",
    "                                    label=\"📏 Taille des chunks\",\n",
    "                                    info=\"Taille des segments de texte\"\n",
    "                                )\n",
    "                                \n",
    "                                k_documents_slider = gr.Slider(\n",
    "                                    minimum=1,\n",
    "                                    maximum=10,\n",
    "                                    value=Config.DEFAULT_K_DOCUMENTS,\n",
    "                                    step=1,\n",
    "                                    label=\"🔍 Nombre de documents récupérés\",\n",
    "                                    info=\"Nombre de segments utilisés pour répondre\"\n",
    "                                )\n",
    "                            \n",
    "                            process_btn = gr.Button(\"🚀 Traiter le document\", variant=\"primary\", size=\"lg\")\n",
    "                        \n",
    "                        with gr.Column(scale=1):\n",
    "                            status_output = gr.Textbox(\n",
    "                                label=\"📊 Statut\",\n",
    "                                interactive=False,\n",
    "                                lines=2\n",
    "                            )\n",
    "                            \n",
    "                            doc_info_output = gr.Markdown(\n",
    "                                label=\"ℹ️ Informations du document\"\n",
    "                            )\n",
    "                \n",
    "                # Onglet Chat\n",
    "                with gr.Tab(\"💬 Chat\", id=\"chat\"):\n",
    "                    \n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"Conversation avec votre document\",\n",
    "                        height=500,\n",
    "                        show_copy_button=True\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg_input = gr.Textbox(\n",
    "                            label=\"Votre question\",\n",
    "                            placeholder=\"Posez une question sur votre document...\",\n",
    "                            scale=4\n",
    "                        )\n",
    "                        \n",
    "                        with gr.Column(scale=1):\n",
    "                            send_btn = gr.Button(\"📤 Envoyer\", variant=\"primary\")\n",
    "                            clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "                \n",
    "                # Onglet Aide\n",
    "                with gr.Tab(\"❓ Aide\", id=\"help\"):\n",
    "                    gr.Markdown(\"\"\"\n",
    "                    ## 🚀 Comment utiliser cette application ?\n",
    "                    \n",
    "                    ### 1. Configuration\n",
    "                    - **Obtenez une clé API Mistral AI** (gratuite) sur [console.mistral.ai](https://console.mistral.ai/)\n",
    "                    - **Uploadez votre document** (PDF, DOCX ou TXT, max 10MB)\n",
    "                    - **Ajustez les paramètres** selon vos besoins\n",
    "                    - **Cliquez sur \"Traiter le document\"**\n",
    "                    \n",
    "                    ### 2. Chat\n",
    "                    - **Posez vos questions** en langage naturel\n",
    "                    - L'IA répondra en se basant **uniquement** sur votre document\n",
    "                    - Les **sources** sont indiquées pour chaque réponse\n",
    "                    \n",
    "                    ### 3. Conseils\n",
    "                    - **Questions précises** = meilleures réponses\n",
    "                    - **Reformulez** si la réponse ne vous convient pas\n",
    "                    - **Historique** conservé pendant la session\n",
    "                    \n",
    "                    ### 🔧 Paramètres avancés\n",
    "                    - **Taille des chunks**: Plus petit = plus précis, plus grand = plus de contexte\n",
    "                    - **Nombre de documents**: Plus = plus de contexte, mais peut diluer la réponse\n",
    "                    \n",
    "                    ### 🆓 Modèle utilisé\n",
    "                    **Mistral AI (mistral-tiny)** - Modèle français performant et gratuit !\n",
    "                    \"\"\")\n",
    "            \n",
    "            # Événements\n",
    "            process_btn.click(\n",
    "                fn=self.process_document,\n",
    "                inputs=[api_key_input, file_upload, chunk_size_slider, k_documents_slider],\n",
    "                outputs=[status_output, doc_info_output]\n",
    "            )\n",
    "            \n",
    "            send_btn.click(\n",
    "                fn=self.chat_with_document,\n",
    "                inputs=[msg_input, chatbot],\n",
    "                outputs=[msg_input, chatbot]\n",
    "            )\n",
    "            \n",
    "            msg_input.submit(\n",
    "                fn=self.chat_with_document,\n",
    "                inputs=[msg_input, chatbot],\n",
    "                outputs=[msg_input, chatbot]\n",
    "            )\n",
    "            \n",
    "            clear_btn.click(\n",
    "                fn=self.clear_chat,\n",
    "                outputs=[chatbot]\n",
    "            )\n",
    "        \n",
    "        return interface\n",
    "\n",
    "def main():\n",
    "    \"\"\"Point d'entrée principal\"\"\"\n",
    "    # Créer les dossiers nécessaires\n",
    "    os.makedirs(\"./data/uploads\", exist_ok=True)\n",
    "    os.makedirs(\"./data/vectorstore\", exist_ok=True)\n",
    "    \n",
    "    # Lancer l'application\n",
    "    app = DocumentChatBot()\n",
    "    interface = app.create_interface()\n",
    "    \n",
    "    interface.launch(\n",
    "        server_name=\"0.0.0.0\",  # Accessible depuis le réseau\n",
    "        server_port=7860,\n",
    "        share=False,  # Mettre True pour un lien public temporaire\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie utils :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/__init__.py\n",
    "\"\"\"\n",
    "Package utilitaires pour le ChatBot documentaire\n",
    "\"\"\"\n",
    "\n",
    "from .text_splitter import SmartTextSplitter\n",
    "from .prompt_templates import PromptTemplateManager\n",
    "from .validators import FileValidator, InputValidator\n",
    "from .performance import PerformanceMonitor\n",
    "\n",
    "__all__ = [\n",
    "    'SmartTextSplitter',\n",
    "    'PromptTemplateManager', \n",
    "    'FileValidator',\n",
    "    'InputValidator',\n",
    "    'PerformanceMonitor'\n",
    "]\n",
    "\n",
    "# utils/text_splitter.py\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class SmartTextSplitter:\n",
    "    \"\"\"\n",
    "    Découpeur de texte intelligent avec optimisations pour différents types de documents\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.separators = {\n",
    "            'default': [\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"],\n",
    "            'academic': [\"\\n\\n\", \"\\n\", \". \", \"; \", \", \", \" \", \"\"],\n",
    "            'technical': [\"\\n\\n\", \"\\n\", \".\\n\", \". \", \":\\n\", \": \", \" \", \"\"],\n",
    "            'legal': [\"\\n\\n\", \"\\n\", \". \", \"; \", \" - \", \" \", \"\"]\n",
    "        }\n",
    "    \n",
    "    def detect_document_type(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Détecte le type de document pour optimiser le découpage\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Mots-clés pour différents types\n",
    "        academic_keywords = ['abstract', 'résumé', 'introduction', 'conclusion', 'références', 'bibliographie']\n",
    "        technical_keywords = ['api', 'fonction', 'class', 'def ', 'import', 'documentation', 'manuel']\n",
    "        legal_keywords = ['article', 'clause', 'alinéa', 'considérant', 'attendu', 'arrêté']\n",
    "        \n",
    "        # Compter les occurrences\n",
    "        academic_score = sum(1 for keyword in academic_keywords if keyword in text_lower)\n",
    "        technical_score = sum(1 for keyword in technical_keywords if keyword in text_lower)\n",
    "        legal_score = sum(1 for keyword in legal_keywords if keyword in text_lower)\n",
    "        \n",
    "        # Déterminer le type\n",
    "        scores = {\n",
    "            'academic': academic_score,\n",
    "            'technical': technical_score,\n",
    "            'legal': legal_score\n",
    "        }\n",
    "        \n",
    "        max_score = max(scores.values())\n",
    "        if max_score >= 2:  # Seuil minimum\n",
    "            return max(scores, key=scores.get)\n",
    "        \n",
    "        return 'default'\n",
    "    \n",
    "    def create_splitter(self, \n",
    "                       chunk_size: int = 1000,\n",
    "                       chunk_overlap: int = 200,\n",
    "                       document_type: str = 'auto') -> RecursiveCharacterTextSplitter:\n",
    "        \"\"\"\n",
    "        Crée un text splitter optimisé selon le type de document\n",
    "        \"\"\"\n",
    "        if document_type == 'auto':\n",
    "            # Le type sera détecté lors du split_text\n",
    "            separators = self.separators['default']\n",
    "        else:\n",
    "            separators = self.separators.get(document_type, self.separators['default'])\n",
    "        \n",
    "        return RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=separators,\n",
    "            keep_separator=True\n",
    "        )\n",
    "    \n",
    "    def smart_split(self, \n",
    "                   text: str, \n",
    "                   chunk_size: int = 1000,\n",
    "                   chunk_overlap: int = 200,\n",
    "                   preserve_structure: bool = True) -> List[str]:\n",
    "        \"\"\"\n",
    "        Découpage intelligent qui préserve la structure du document\n",
    "        \"\"\"\n",
    "        # Détection automatique du type\n",
    "        doc_type = self.detect_document_type(text)\n",
    "        \n",
    "        # Préprocessing pour préserver la structure\n",
    "        if preserve_structure:\n",
    "            text = self._preprocess_text(text)\n",
    "        \n",
    "        # Création du splitter adapté\n",
    "        splitter = self.create_splitter(chunk_size, chunk_overlap, doc_type)\n",
    "        \n",
    "        # Découpage\n",
    "        chunks = splitter.split_text(text)\n",
    "        \n",
    "        # Post-processing pour nettoyer les chunks\n",
    "        cleaned_chunks = [self._clean_chunk(chunk) for chunk in chunks]\n",
    "        \n",
    "        return [chunk for chunk in cleaned_chunks if chunk.strip()]\n",
    "    \n",
    "    def split_documents_with_metadata(self,\n",
    "                                    text: str,\n",
    "                                    filename: str,\n",
    "                                    chunk_size: int = 1000,\n",
    "                                    chunk_overlap: int = 200) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Découpe le texte et crée des documents avec métadonnées enrichies\n",
    "        \"\"\"\n",
    "        chunks = self.smart_split(text, chunk_size, chunk_overlap)\n",
    "        doc_type = self.detect_document_type(text)\n",
    "        \n",
    "        documents = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Métadonnées enrichies\n",
    "            metadata = {\n",
    "                \"filename\": filename,\n",
    "                \"chunk_id\": i,\n",
    "                \"total_chunks\": len(chunks),\n",
    "                \"document_type\": doc_type,\n",
    "                \"chunk_size\": len(chunk),\n",
    "                \"chunk_position\": \"start\" if i == 0 else (\"end\" if i == len(chunks) - 1 else \"middle\"),\n",
    "                \"contains_structure\": self._has_structure_markers(chunk)\n",
    "            }\n",
    "            \n",
    "            # Ajout de mots-clés pour ce chunk\n",
    "            keywords = self._extract_keywords(chunk)\n",
    "            if keywords:\n",
    "                metadata[\"keywords\"] = keywords\n",
    "            \n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Préprocessing pour améliorer le découpage\"\"\"\n",
    "        # Normaliser les espaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Préserver les sauts de ligne importants\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        \n",
    "        # Améliorer la détection des phrases\n",
    "        text = re.sub(r'([.!?])\\s*([A-Z])', r'\\1\\n\\2', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _clean_chunk(self, chunk: str) -> str:\n",
    "        \"\"\"Nettoie un chunk après découpage\"\"\"\n",
    "        # Supprimer les espaces en début/fin\n",
    "        chunk = chunk.strip()\n",
    "        \n",
    "        # Supprimer les lignes vides multiples\n",
    "        chunk = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', chunk)\n",
    "        \n",
    "        # S'assurer qu'on ne commence pas par un séparateur\n",
    "        chunk = re.sub(r'^[.!?;:,\\s]+', '', chunk)\n",
    "        \n",
    "        return chunk\n",
    "    \n",
    "    def _has_structure_markers(self, chunk: str) -> bool:\n",
    "        \"\"\"Détecte si le chunk contient des marqueurs structurels\"\"\"\n",
    "        structure_patterns = [\n",
    "            r'^\\d+\\.',  # Numérotation\n",
    "            r'^[A-Z][.]',  # Sections A., B., etc.\n",
    "            r'^-\\s',  # Listes à puces\n",
    "            r'^\\*\\s',  # Listes étoiles\n",
    "            r'^\\w+:\\s',  # Titre: contenu\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, chunk, re.MULTILINE) for pattern in structure_patterns)\n",
    "    \n",
    "    def _extract_keywords(self, chunk: str, max_keywords: int = 5) -> List[str]:\n",
    "        \"\"\"Extrait les mots-clés principaux d'un chunk\"\"\"\n",
    "        # Mots vides français et anglais\n",
    "        stop_words = {\n",
    "            'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'et', 'ou', 'à', 'dans', 'sur', 'pour', 'par',\n",
    "            'avec', 'sans', 'sous', 'que', 'qui', 'quoi', 'dont', 'où', 'ce', 'cette', 'ces', 'est', 'sont',\n",
    "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are'\n",
    "        }\n",
    "        \n",
    "        # Extraction des mots\n",
    "        words = re.findall(r'\\b[a-zA-ZàâäéèêëïîôöùûüÿçÀÂÄÉÈÊËÏÎÔÖÙÛÜŸÇ]{3,}\\b', chunk.lower())\n",
    "        \n",
    "        # Filtrage et comptage\n",
    "        word_freq = {}\n",
    "        for word in words:\n",
    "            if word not in stop_words and len(word) > 2:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        # Retourner les mots les plus fréquents\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [word for word, _ in sorted_words[:max_keywords]]\n",
    "\n",
    "# utils/prompt_templates.py\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Dict, List\n",
    "\n",
    "class PromptTemplateManager:\n",
    "    \"\"\"\n",
    "    Gestionnaire de templates de prompts optimisés pour différents cas d'usage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.templates = self._initialize_templates()\n",
    "    \n",
    "    def _initialize_templates(self) -> Dict[str, PromptTemplate]:\n",
    "        \"\"\"Initialise les différents templates de prompts\"\"\"\n",
    "        \n",
    "        templates = {}\n",
    "        \n",
    "        # Template général (par défaut)\n",
    "        templates['general'] = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "            template=\"\"\"Tu es un assistant IA spécialisé dans l'analyse de documents. \n",
    "Réponds aux questions en te basant UNIQUEMENT sur le contexte fourni.\n",
    "\n",
    "Contexte des documents:\n",
    "{context}\n",
    "\n",
    "Historique de la conversation:\n",
    "{chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Réponds de manière précise et concise\n",
    "- Si l'information n'est pas dans le contexte, dis clairement \"Je ne trouve pas cette information dans le document fourni\"\n",
    "- Cite des passages spécifiques quand c'est pertinent\n",
    "- Reste factuel et objectif\n",
    "\n",
    "Réponse:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Template pour documents académiques\n",
    "        templates['academic'] = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "            template=\"\"\"Tu es un assistant de recherche académique spécialisé dans l'analyse de publications scientifiques.\n",
    "Analyse le contexte fourni pour répondre à la question de manière rigoureuse.\n",
    "\n",
    "Contexte du document académique:\n",
    "{context}\n",
    "\n",
    "Historique de la conversation:\n",
    "{chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Fournis une réponse académique rigoureuse\n",
    "- Cite les auteurs et sections spécifiques si mentionnés\n",
    "- Distingue clairement les faits des hypothèses\n",
    "- Si des données ou statistiques sont présentes, inclus-les dans ta réponse\n",
    "- Indique les limites de l'information disponible\n",
    "- Si l'information n'est pas dans le document, dis \"Cette information n'est pas présente dans le document analysé\"\n",
    "\n",
    "Réponse académique:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Template pour documents techniques\n",
    "        templates['technical'] = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "            template=\"\"\"Tu es un assistant technique expert en documentation et manuels.\n",
    "Analyse la documentation technique pour fournir une réponse précise et applicable.\n",
    "\n",
    "Documentation technique:\n",
    "{context}\n",
    "\n",
    "Historique de la conversation:\n",
    "{chat_history}\n",
    "\n",
    "Question technique: {question}\n",
    "\n",
    "Instructions:\n",
    "- Fournis une réponse technique précise et actionnable\n",
    "- Inclus les étapes, commandes ou configurations si pertinentes\n",
    "- Mentionne les prérequis ou limitations\n",
    "- Utilise la terminologie technique appropriée\n",
    "- Si des exemples de code sont présents, inclus-les\n",
    "- Si l'information technique n'est pas disponible, dis \"Cette information technique n'est pas documentée dans ce manuel\"\n",
    "\n",
    "Réponse technique:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Template pour documents légaux\n",
    "        templates['legal'] = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "            template=\"\"\"Tu es un assistant spécialisé dans l'analyse de documents juridiques.\n",
    "Analyse le texte légal pour répondre de manière précise et structurée.\n",
    "\n",
    "Texte légal:\n",
    "{context}\n",
    "\n",
    "Historique de la conversation:\n",
    "{chat_history}\n",
    "\n",
    "Question juridique: {question}\n",
    "\n",
    "Instructions:\n",
    "- Fournis une réponse structurée et précise\n",
    "- Cite les articles, clauses ou sections spécifiques\n",
    "- Respecte la terminologie juridique\n",
    "- Distingue les obligations, droits et procédures\n",
    "- Ne fournis pas de conseil juridique, seulement l'analyse du texte\n",
    "- Si l'information n'est pas dans le document, dis \"Cette disposition n'est pas couverte dans ce texte\"\n",
    "\n",
    "Analyse juridique:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Template pour résumé\n",
    "        templates['summary'] = PromptTemplate(\n",
    "            input_variables=[\"context\"],\n",
    "            template=\"\"\"Tu dois créer un résumé structuré et complet du document fourni.\n",
    "\n",
    "Contenu du document:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- Crée un résumé structuré avec des sections claires\n",
    "- Inclus les points clés et informations importantes\n",
    "- Utilise des puces pour les listes d'éléments\n",
    "- Maintiens l'objectivité et la précision\n",
    "- Indique s'il s'agit d'un résumé partiel si le document semble incomplet\n",
    "\n",
    "Structure suggérée:\n",
    "## Résumé du document\n",
    "\n",
    "### Points principaux\n",
    "- [Point 1]\n",
    "- [Point 2]\n",
    "- [etc.]\n",
    "\n",
    "### Informations clés\n",
    "- [Information 1]\n",
    "- [Information 2]\n",
    "- [etc.]\n",
    "\n",
    "### Conclusion\n",
    "[Synthèse globale]\n",
    "\n",
    "Résumé:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Template pour extraction d'informations spécifiques\n",
    "        templates['extraction'] = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"Tu dois extraire des informations spécifiques du document selon la demande.\n",
    "\n",
    "Document:\n",
    "{context}\n",
    "\n",
    "Demande d'extraction: {question}\n",
    "\n",
    "Instructions:\n",
    "- Extrais UNIQUEMENT les informations demandées\n",
    "- Présente les résultats de manière structurée\n",
    "- Si l'information n'est pas présente, indique \"Information non trouvée\"\n",
    "- Utilise des listes ou tableaux si approprié\n",
    "- Reste fidèle au texte original sans interpréter\n",
    "\n",
    "Extraction:\"\"\"\n",
    "        )\n",
    "        \n",
    "        return templates\n",
    "    \n",
    "    def get_template(self, template_type: str = 'general') -> PromptTemplate:\n",
    "        \"\"\"\n",
    "        Retourne le template demandé\n",
    "        \"\"\"\n",
    "        return self.templates.get(template_type, self.templates['general'])\n",
    "    \n",
    "    def get_available_templates(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retourne la liste des templates disponibles\n",
    "        \"\"\"\n",
    "        return list(self.templates.keys())\n",
    "    \n",
    "    def create_custom_template(self, \n",
    "                             template_name: str,\n",
    "                             template_content: str,\n",
    "                             input_variables: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Ajoute un template personnalisé\n",
    "        \"\"\"\n",
    "        self.templates[template_name] = PromptTemplate(\n",
    "            input_variables=input_variables,\n",
    "            template=template_content\n",
    "        )\n",
    "    \n",
    "    def get_optimized_template(self, document_type: str, query_type: str = 'general') -> PromptTemplate:\n",
    "        \"\"\"\n",
    "        Retourne le template le mieux adapté selon le type de document et de requête\n",
    "        \"\"\"\n",
    "        # Logique de sélection du template optimal\n",
    "        if query_type == 'summary':\n",
    "            return self.get_template('summary')\n",
    "        elif query_type == 'extraction':\n",
    "            return self.get_template('extraction')\n",
    "        elif document_type in ['academic', 'technical', 'legal']:\n",
    "            return self.get_template(document_type)\n",
    "        else:\n",
    "            return self.get_template('general')\n",
    "\n",
    "# utils/validators.py\n",
    "import os\n",
    "import magic\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, List\n",
    "import re\n",
    "\n",
    "class FileValidator:\n",
    "    \"\"\"\n",
    "    Validateur pour les fichiers uploadés\n",
    "    \"\"\"\n",
    "    \n",
    "    SUPPORTED_EXTENSIONS = {'.pdf', '.docx', '.txt', '.doc'}\n",
    "    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\n",
    "    MIN_FILE_SIZE = 100  # 100 bytes\n",
    "    \n",
    "    @classmethod\n",
    "    def validate_file(cls, file_obj) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Valide un fichier uploadé\n",
    "        Returns: (is_valid, error_message)\n",
    "        \"\"\"\n",
    "        if file_obj is None:\n",
    "            return False, \"Aucun fichier fourni\"\n",
    "        \n",
    "        # Vérification de l'extension\n",
    "        if hasattr(file_obj, 'name'):\n",
    "            file_extension = Path(file_obj.name).suffix.lower()\n",
    "            if file_extension not in cls.SUPPORTED_EXTENSIONS:\n",
    "                return False, f\"Format non supporté. Formats acceptés: {', '.join(cls.SUPPORTED_EXTENSIONS)}\"\n",
    "        \n",
    "        # Vérification de la taille\n",
    "        try:\n",
    "            if hasattr(file_obj, 'size'):\n",
    "                file_size = file_obj.size\n",
    "            else:\n",
    "                # Fallback pour les objets sans attribut size\n",
    "                content = file_obj.read() if hasattr(file_obj, 'read') else file_obj\n",
    "                file_size = len(content)\n",
    "                # Reset du pointeur si possible\n",
    "                if hasattr(file_obj, 'seek'):\n",
    "                    file_obj.seek(0)\n",
    "            \n",
    "            if file_size > cls.MAX_FILE_SIZE:\n",
    "                return False, f\"Fichier trop volumineux (max {cls.MAX_FILE_SIZE // (1024*1024)}MB)\"\n",
    "            \n",
    "            if file_size < cls.MIN_FILE_SIZE:\n",
    "                return False, \"Fichier trop petit ou vide\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return False, f\"Erreur lors de la vérification de la taille: {str(e)}\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    @classmethod\n",
    "    def detect_file_type(cls, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Détecte le type MIME d'un fichier\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mime_type = magic.from_file(file_path, mime=True)\n",
    "            return mime_type\n",
    "        except:\n",
    "            # Fallback basé sur l'extension\n",
    "            extension = Path(file_path).suffix.lower()\n",
    "            mime_mapping = {\n",
    "                '.pdf': 'application/pdf',\n",
    "                '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
    "                '.doc': 'application/msword',\n",
    "                '.txt': 'text/plain'\n",
    "            }\n",
    "            return mime_mapping.get(extension, 'unknown')\n",
    "\n",
    "class InputValidator:\n",
    "    \"\"\"\n",
    "    Validateur pour les entrées utilisateur\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_api_key(api_key: str, provider: str = 'mistral') -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Valide une clé API\n",
    "        \"\"\"\n",
    "        if not api_key or not api_key.strip():\n",
    "            return False, \"Clé API manquante\"\n",
    "        \n",
    "        api_key = api_key.strip()\n",
    "        \n",
    "        if provider.lower() == 'mistral':\n",
    "            # Format attendu pour Mistral: commence généralement par des caractères spécifiques\n",
    "            if len(api_key) < 20:\n",
    "                return False, \"Clé API Mistral trop courte\"\n",
    "            \n",
    "            # Vérification basique du format\n",
    "            if not re.match(r'^[a-zA-Z0-9_-]+$', api_key):\n",
    "                return False, \"Format de clé API invalide\"\n",
    "        \n",
    "        elif provider.lower() == 'openai':\n",
    "            # Format OpenAI: sk-...\n",
    "            if not api_key.startswith('sk-'):\n",
    "                return False, \"Clé API OpenAI doit commencer par 'sk-'\"\n",
    "            \n",
    "            if len(api_key) < 45:\n",
    "                return False, \"Clé API OpenAI trop courte\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_chunk_size(chunk_size: int) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Valide la taille des chunks\n",
    "        \"\"\"\n",
    "        if not isinstance(chunk_size, int):\n",
    "            return False, \"La taille des chunks doit être un nombre entier\"\n",
    "        \n",
    "        if chunk_size < 100:\n",
    "            return False, \"Taille des chunks trop petite (minimum 100)\"\n",
    "        \n",
    "        if chunk_size > 4000:\n",
    "            return False, \"Taille des chunks trop grande (maximum 4000)\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_k_documents(k: int) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Valide le nombre de documents à récupérer\n",
    "        \"\"\"\n",
    "        if not isinstance(k, int):\n",
    "            return False, \"Le nombre de documents doit être un entier\"\n",
    "        \n",
    "        if k < 1:\n",
    "            return False, \"Nombre de documents minimum: 1\"\n",
    "        \n",
    "        if k > 20:\n",
    "            return False, \"Nombre de documents maximum: 20\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_question(question: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Valide une question utilisateur\n",
    "        \"\"\"\n",
    "        if not question or not question.strip():\n",
    "            return False, \"Question vide\"\n",
    "        \n",
    "        question = question.strip()\n",
    "        \n",
    "        if len(question) < 3:\n",
    "            return False, \"Question trop courte (minimum 3 caractères)\"\n",
    "        \n",
    "        if len(question) > 1000:\n",
    "            return False, \"Question trop longue (maximum 1000 caractères)\"\n",
    "        \n",
    "        # Vérification de caractères suspects\n",
    "        suspicious_patterns = [\n",
    "            r'<script',\n",
    "            r'javascript:',\n",
    "            r'eval\\(',\n",
    "            r'exec\\('\n",
    "        ]\n",
    "        \n",
    "        for pattern in suspicious_patterns:\n",
    "            if re.search(pattern, question, re.IGNORECASE):\n",
    "                return False, \"Question contient des éléments suspects\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def sanitize_input(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Nettoie et sécurise une entrée texte\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Supprime les caractères de contrôle\n",
    "        text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
    "        \n",
    "        # Normalise les espaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Supprime les espaces en début/fin\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "# utils/performance.py\n",
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "from typing import Dict, List, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Structure pour stocker les métriques de performance\"\"\"\n",
    "    timestamp: datetime\n",
    "    operation: str\n",
    "    duration: float\n",
    "    memory_usage: float\n",
    "    cpu_usage: float\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Moniteur de performance pour l'application\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_history: int = 100):\n",
    "        self.metrics_history: List[PerformanceMetrics] = []\n",
    "        self.max_history = max_history\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def measure_performance(self, operation_name: str):\n",
    "        \"\"\"\n",
    "        Décorateur pour mesurer les performances d'une fonction\n",
    "        \"\"\"\n",
    "        def decorator(func: Callable):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                start_time = time.time()\n",
    "                start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "                start_cpu = psutil.cpu_percent()\n",
    "                \n",
    "                success = True\n",
    "                error_message = None\n",
    "                result = None\n",
    "                \n",
    "                try:\n",
    "                    result = func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    success = False\n",
    "                    error_message = str(e)\n",
    "                    raise\n",
    "                finally:\n",
    "                    end_time = time.time()\n",
    "                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "                    duration = end_time - start_time\n",
    "                    \n",
    "                    # Mesure CPU pendant l'opération (approximation)\n",
    "                    cpu_usage = psutil.cpu_percent()\n",
    "                    \n",
    "                    metrics = PerformanceMetrics(\n",
    "                        timestamp=datetime.now(),\n",
    "                        operation=operation_name,\n",
    "                        duration=duration,\n",
    "                        memory_usage=end_memory - start_memory,\n",
    "                        cpu_usage=cpu_usage,\n",
    "                        success=success,\n",
    "                        error_message=error_message\n",
    "                    )\n",
    "                    \n",
    "                    self._add_metrics(metrics)\n",
    "                \n",
    "                return result\n",
    "            return wrapper\n",
    "        return decorator\n",
    "    \n",
    "    def _add_metrics(self, metrics: PerformanceMetrics):\n",
    "        \"\"\"Ajoute des métriques à l'historique\"\"\"\n",
    "        with self._lock:\n",
    "            self.metrics_history.append(metrics)\n",
    "            \n",
    "            # Limiter la taille de l'historique\n",
    "            if len(self.metrics_history) > self.max_history:\n",
    "                self.metrics_history = self.metrics_history[-self.max_history:]\n",
    "    \n",
    "    def get_metrics_summary(self, \n",
    "                           operation: Optional[str] = None,\n",
    "                           last_n_minutes: Optional[int] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Retourne un résumé des métriques\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            filtered_metrics = self.metrics_history.copy()\n",
    "        \n",
    "        # Filtrage par opération\n",
    "        if operation:\n",
    "            filtered_metrics = [m for m in filtered_metrics if m.operation == operation]\n",
    "        \n",
    "        # Filtrage temporel\n",
    "        if last_n_minutes:\n",
    "            cutoff_time = datetime.now() - timedelta(minutes=last_n_minutes)\n",
    "            filtered_metrics = [m for m in filtered_metrics if m.timestamp >= cutoff_time]\n",
    "        \n",
    "        if not filtered_metrics:\n",
    "            return {\"message\": \"Aucune métrique disponible\"}\n",
    "        \n",
    "        # Calculs statistiques\n",
    "        durations = [m.duration for m in filtered_metrics]\n",
    "        memory_usages = [m.memory_usage for m in filtered_metrics]\n",
    "        success_count = sum(1 for m in filtered_metrics if m.success)\n",
    "        \n",
    "        summary = {\n",
    "            \"total_operations\": len(filtered_metrics),\n",
    "            \"success_rate\": success_count / len(filtered_metrics) * 100,\n",
    "            \"avg_duration\": sum(durations) / len(durations),\n",
    "            \"max_duration\": max(durations),\n",
    "            \"min_duration\": min(durations),\n",
    "            \"avg_memory_usage\": sum(memory_usages) / len(memory_usages),\n",
    "            \"max_memory_usage\": max(memory_usages),\n",
    "            \"recent_errors\": [\n",
    "                {\"operation\": m.operation, \"error\": m.error_message, \"timestamp\": m.timestamp}\n",
    "                for m in filtered_metrics[-5:] if not m.success\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def get_system_info(self) -> Dict:\n",
    "        \"\"\"Retourne les informations système actuelles\"\"\"\n",
    "        return {\n",
    "            \"cpu_percent\": psutil.cpu_percent(),\n",
    "            \"memory_percent\": psutil.virtual_memory().percent,\n",
    "            \"memory_available_gb\": psutil.virtual_memory().available / 1024 / 1024 / 1024,\n",
    "            \"disk_usage_percent\": psutil.disk_usage('/').percent,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def export_metrics(self, filepath: str):\n",
    "        \"\"\"Exporte les métriques vers un fichier JSON\"\"\"\n",
    "        import json\n",
    "        \n",
    "        with self._lock:\n",
    "            data = {\n",
    "                \"export_timestamp\": datetime.now().isoformat(),\n",
    "                \"metrics\": [\n",
    "                    {\n",
    "                        \"timestamp\": m.timestamp.isoformat(),\n",
    "                        \"operation\": m.operation,\n",
    "                        \"duration\": m.duration,\n",
    "                        \"memory_usage\": m.memory_usage,\n",
    "                        \"cpu_usage\": m.cpu_usage,\n",
    "                        \"success\": m.success,\n",
    "                        \"error_message\": m.error_message\n",
    "                    }\n",
    "                    for m in self.metrics_history\n",
    "                ]\n",
    "            }\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def clear_metrics(self):\n",
    "        \"\"\"Efface l'historique des métriques\"\"\"\n",
    "        with self._lock:\n",
    "            self.metrics_history.clear()\n",
    "\n",
    "# Exemple d'utilisation du moniteur de performance\n",
    "def create_performance_monitor():\n",
    "    \"\"\"Factory function pour créer un moniteur de performance\"\"\"\n",
    "    return PerformanceMonitor(max_history=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📋 Récapitulatif des scripts créés :\n",
    "1. text_splitter.py - Découpage intelligent\n",
    "\n",
    "SmartTextSplitter : Détection automatique du type de document\n",
    "Types supportés : académique, technique, légal, général\n",
    "Optimisations : préservation de structure, extraction de mots-clés\n",
    "Métadonnées enrichies pour chaque chunk\n",
    "\n",
    "2. prompt_templates.py - Templates de prompts optimisés\n",
    "\n",
    "6 templates spécialisés : général, académique, technique, légal, résumé, extraction\n",
    "Sélection automatique selon le type de document\n",
    "Templates personnalisables\n",
    "Optimisation contextuelle\n",
    "\n",
    "3. validators.py - Validation et sécurité\n",
    "\n",
    "FileValidator : validation fichiers (taille, format, MIME)\n",
    "InputValidator : validation des entrées (API keys, paramètres, questions)\n",
    "Sécurisation : détection de patterns suspects, nettoyage\n",
    "Support multi-providers (Mistral, OpenAI)\n",
    "\n",
    "4. performance.py - Monitoring des performances\n",
    "\n",
    "PerformanceMonitor : mesure automatique des performances\n",
    "Métriques : temps, mémoire, CPU, taux de succès\n",
    "Décorateur pour instrumenter les fonctions\n",
    "Export et historique des métriques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .env.example :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .env.example\n",
    "MISTRAL_API_KEY=your_mistral_api_key_here\n",
    "\n",
    "# Installation et lancement\n",
    "# 1. Créer un environnement virtuel\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # Linux/Mac\n",
    "# ou\n",
    "venv\\Scripts\\activate  # Windows\n",
    "\n",
    "# 2. Installer les dépendances\n",
    "pip install gradio>=4.0.0 langchain>=0.1.0 langchain-community langchain-mistralai chromadb>=0.4.0 mistralai>=0.1.0 PyPDF2>=3.0.0 python-docx>=0.8.11 sentence-transformers>=2.2.0 python-dotenv>=1.0.0 tiktoken>=0.5.0\n",
    "\n",
    "# 3. Créer le fichier .env avec votre clé API\n",
    "cp .env.example .env\n",
    "# Puis éditer .env avec votre vraie clé API Mistral\n",
    "\n",
    "# 4. Lancer l'application\n",
    "python app/main.py\n",
    "\n",
    "# Structure des dossiers à créer\n",
    "mkdir -p app/components\n",
    "mkdir -p data/uploads\n",
    "mkdir -p data/vectorstore\n",
    "mkdir -p utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-récap :\n",
    "\n",
    "### ✨ Fonctionnalités principales :\n",
    "- **Interface Gradio moderne** avec 3 onglets (Configuration, Chat, Aide)\n",
    "- **Mistral AI intégré** (mistral-tiny pour commencer, gratuit)\n",
    "- **Embeddings gratuits** avec sentence-transformers (multilingue FR/EN)\n",
    "- **Support PDF, DOCX, TXT** avec extraction robuste\n",
    "- **ChromaDB local** pour la persistance\n",
    "- **Historique de conversation** avec sources\n",
    "- **Configuration flexible** des paramètres\n",
    "\n",
    "### 🚀 Pour lancer l'application :\n",
    "\n",
    "1. **Créez la structure** :\n",
    "```bash\n",
    "mkdir document-chatbot && cd document-chatbot\n",
    "mkdir -p app/components data/uploads data/vectorstore\n",
    "```\n",
    "\n",
    "2. **Copiez le code** dans les fichiers appropriés\n",
    "3. **Installez les dépendances** avec pip\n",
    "4. **Obtenez votre clé API** Mistral (gratuite) sur [console.mistral.ai](https://console.mistral.ai/)\n",
    "5. **Lancez** : `python app/main.py`\n",
    "\n",
    "### 🔧 Points forts de cette implémentation :\n",
    "\n",
    "- **100% gratuit** (sauf limite API Mistral gratuite)\n",
    "- **Multilingue** (français/anglais optimisé)\n",
    "- **Robuste** avec gestion d'erreurs complète\n",
    "- **Modulaire** et extensible\n",
    "- **Interface intuitive** avec aide intégrée\n",
    "\n",
    "L'application sera accessible sur `http://localhost:7860` et ressemblera à SecInsights.ai avec des fonctionnalités similaires !\n",
    "\n",
    "Voulez-vous que je vous aide avec l'installation ou que j'ajoute des fonctionnalités spécifiques ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok NEW GAME :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Guide d'installation et utilisation complète\n",
    "\n",
    "## 📦 Installation\n",
    "\n",
    "### 1. Créer la structure du projet\n",
    "```bash\n",
    "mkdir document-chatbot\n",
    "cd document-chatbot\n",
    "\n",
    "# Créer l'arborescence\n",
    "mkdir -p app/components\n",
    "mkdir -p utils\n",
    "mkdir -p data/uploads\n",
    "mkdir -p data/vectorstore\n",
    "mkdir -p data/exports\n",
    "```\n",
    "\n",
    "### 2. Installer les dépendances\n",
    "```bash\n",
    "# Créer un environnement virtuel\n",
    "python -m venv venv\n",
    "\n",
    "# Activer l'environnement\n",
    "# Linux/Mac:\n",
    "source venv/bin/activate\n",
    "# Windows:\n",
    "venv\\Scripts\\activate\n",
    "\n",
    "# Installer les packages\n",
    "pip install gradio>=4.0.0\n",
    "pip install langchain>=0.1.0\n",
    "pip install langchain-community\n",
    "pip install langchain-mistralai\n",
    "pip install chromadb>=0.4.0\n",
    "pip install mistralai>=0.1.0\n",
    "pip install PyPDF2>=3.0.0\n",
    "pip install python-docx>=0.8.11\n",
    "pip install sentence-transformers>=2.2.0\n",
    "pip install python-dotenv>=1.0.0\n",
    "pip install tiktoken>=0.5.0\n",
    "pip install psutil>=5.9.0\n",
    "pip install python-magic>=0.4.27\n",
    "\n",
    "# Alternative pour python-magic sur Windows :\n",
    "# pip install python-magic-bin==0.4.14\n",
    "```\n",
    "\n",
    "### 3. Configuration\n",
    "```bash\n",
    "# Créer le fichier d'environnement\n",
    "cp .env.example .env\n",
    "\n",
    "# Éditer .env avec votre clé API Mistral\n",
    "echo \"MISTRAL_API_KEY=votre_cle_api_ici\" > .env\n",
    "```\n",
    "\n",
    "## 🎯 Utilisation\n",
    "\n",
    "### Version Simple (originale)\n",
    "```bash\n",
    "python app/main.py\n",
    "```\n",
    "\n",
    "### Version Avancée (avec tous les utilitaires)\n",
    "```bash\n",
    "python app/main_enhanced.py\n",
    "```\n",
    "\n",
    "## 🆕 Nouvelles fonctionnalités de la version améliorée\n",
    "\n",
    "### 1. **Découpage intelligent de documents**\n",
    "- **Détection automatique** du type de document\n",
    "- **Types supportés** : \n",
    "  - 📚 Académique (articles, thèses, rapports de recherche)\n",
    "  - 🔧 Technique (manuels, documentation, guides)\n",
    "  - ⚖️ Légal (contrats, règlements, lois)\n",
    "  - 📄 Général (tous autres documents)\n",
    "\n",
    "- **Optimisations par type** :\n",
    "  - Séparateurs spécialisés\n",
    "  - Préservation de la structure\n",
    "  - Extraction de mots-clés\n",
    "  - Métadonnées enrichies\n",
    "\n",
    "### 2. **Templates de prompts optimisés**\n",
    "- **Sélection automatique** du template selon le document\n",
    "- **Templates disponibles** :\n",
    "  - `general` : Usage standard\n",
    "  - `academic` : Analyse rigoureuse, citations\n",
    "  - `technical` : Réponses pratiques, étapes\n",
    "  - `legal` : Analyse structurée, articles\n",
    "  - `summary` : Résumés structurés\n",
    "  - `extraction` : Extraction d'informations\n",
    "\n",
    "### 3. **Validation et sécurité renforcées**\n",
    "- **Validation des fichiers** :\n",
    "  - Vérification du format et de la taille\n",
    "  - Détection du type MIME\n",
    "  - Protection contre les fichiers malveillants\n",
    "\n",
    "- **Validation des entrées** :\n",
    "  - Clés API (format et longueur)\n",
    "  - Paramètres (plages valides)\n",
    "  - Questions (nettoyage et sécurisation)\n",
    "\n",
    "### 4. **Monitoring de performance**\n",
    "- **Métriques en temps réel** :\n",
    "  - Temps de traitement\n",
    "  - Utilisation mémoire et CPU\n",
    "  - Taux de succès des opérations\n",
    "\n",
    "- **Historique et export** :\n",
    "  - Sauvegarde des métriques\n",
    "  - Export des sessions\n",
    "  - Analyse des performances\n",
    "\n",
    "## 🎛️ Interface utilisateur améliorée\n",
    "\n",
    "### Onglet \"Configuration Avancée\"\n",
    "1. **Clé API** avec validation automatique\n",
    "2. **Upload de fichier** avec vérification en temps réel\n",
    "3. **Paramètres avancés** :\n",
    "   - Taille des chunks (optimisée automatiquement)\n",
    "   - Nombre de documents récupérés\n",
    "   - **Type de template** (auto-détection ou manuel)\n",
    "4. **Informations détaillées** :\n",
    "   - Type de document détecté\n",
    "   - Nombre de chunks créés\n",
    "   - Mots-clés extraits\n",
    "   - Métriques de performance\n",
    "\n",
    "### Onglet \"Chat Intelligent\"\n",
    "1. **Conversation améliorée** avec :\n",
    "   - Sources détaillées avec aperçu\n",
    "   - Mots-clés des chunks utilisés\n",
    "   - Template de prompt utilisé\n",
    "2. **Métadonnées en temps réel** :\n",
    "   - Nombre de sources consultées\n",
    "   - Temps de réponse\n",
    "   - Template utilisé\n",
    "3. **Validation automatique** des questions\n",
    "\n",
    "### Onglet \"Monitoring\"\n",
    "1. **Statut du système** :\n",
    "   - Configuration actuelle\n",
    "   - Ressources système (CPU, mémoire, disque)\n",
    "   - Templates disponibles\n",
    "2. **Export de session** :\n",
    "   - Historique des conversations\n",
    "   - Métriques de performance\n",
    "   - Configuration utilisée\n",
    "\n",
    "## 🔍 Exemples d'utilisation\n",
    "\n",
    "### Document académique\n",
    "```\n",
    "Document : Article de recherche sur l'IA\n",
    "Type détecté : academic\n",
    "Template utilisé : academic\n",
    "Réponse : \"Selon l'étude présentée dans ce document, les auteurs démontrent que...\"\n",
    "```\n",
    "\n",
    "### Documentation technique\n",
    "```\n",
    "Document : Manuel d'installation logiciel\n",
    "Type détecté : technical\n",
    "Template utilisé : technical\n",
    "Réponse : \"Pour installer le logiciel, suivez ces étapes : 1. Télécharger...\"\n",
    "```\n",
    "\n",
    "### Document légal\n",
    "```\n",
    "Document : Contrat de travail\n",
    "Type détecté : legal\n",
    "Template utilisé : legal\n",
    "Réponse : \"L'article 3.2 du contrat stipule que...\"\n",
    "```\n",
    "\n",
    "## 📊 Métriques et performance\n",
    "\n",
    "### Métriques collectées\n",
    "- **Temps de traitement** par opération\n",
    "- **Utilisation mémoire** (avant/après)\n",
    "- **CPU** pendant le traitement\n",
    "- **Taux de succès** des opérations\n",
    "- **Erreurs** avec détails\n",
    "\n",
    "### Export des données\n",
    "```json\n",
    "{\n",
    "  \"document\": \"mon_document.pdf\",\n",
    "  \"system_status\": {\n",
    "    \"template_used\": \"academic\",\n",
    "    \"conversation_length\": 5,\n",
    "    \"performance_summary\": {...}\n",
    "  },\n",
    "  \"conversation_history\": [...],\n",
    "  \"performance_metrics\": {...}\n",
    "}\n",
    "```\n",
    "\n",
    "## 🎯 Avantages de la version améliorée\n",
    "\n",
    "### 🚀 Performance\n",
    "- **Découpage optimisé** selon le type de document\n",
    "- **Templates spécialisés** pour de meilleures réponses\n",
    "- **Monitoring en temps réel** des performances\n",
    "\n",
    "### 🔒 Sécurité\n",
    "- **Validation robuste** des entrées\n",
    "- **Nettoyage automatique** des données\n",
    "- **Protection** contre les injections\n",
    "\n",
    "### 🧠 Intelligence\n",
    "- **Détection automatique** du contexte\n",
    "- **Adaptation dynamique** des réponses\n",
    "- **Métadonnées enrichies** pour plus de précision\n",
    "\n",
    "### 📈 Observabilité\n",
    "- **Métriques détaillées** en temps réel\n",
    "- **Historique complet** des performances\n",
    "- **Export facile** des données\n",
    "\n",
    "## 🔧 Personnalisation\n",
    "\n",
    "### Ajouter un nouveau template\n",
    "```python\n",
    "from utils.prompt_templates import PromptTemplateManager\n",
    "\n",
    "manager = PromptTemplateManager()\n",
    "manager.create_custom_template(\n",
    "    template_name=\"medical\",\n",
    "    template_content=\"Tu es un assistant médical...\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "```\n",
    "\n",
    "### Personnaliser la détection de type\n",
    "```python\n",
    "from utils.text_splitter import SmartTextSplitter\n",
    "\n",
    "splitter = SmartTextSplitter()\n",
    "# Ajouter des mots-clés pour un nouveau type\n",
    "splitter.separators['medical'] = [\"\\n\\n\", \"\\n\", \". \", \"; \", \" \", \"\"]\n",
    "```\n",
    "\n",
    "## 🐛 Dépannage\n",
    "\n",
    "### Problèmes courants\n",
    "\n",
    "1. **Erreur d'installation de python-magic** :\n",
    "   ```bash\n",
    "   # Linux\n",
    "   sudo apt-get install libmagic1\n",
    "   \n",
    "   # macOS\n",
    "   brew install libmagic\n",
    "   \n",
    "   # Windows\n",
    "   pip install python-magic-bin\n",
    "   ```\n",
    "\n",
    "2. **Erreur de clé API Mistral** :\n",
    "   - Vérifiez que la clé commence par le bon format\n",
    "   - Consultez https://console.mistral.ai/ pour obtenir une clé\n",
    "\n",
    "3. **Problème de mémoire** :\n",
    "   - Réduisez la taille des chunks\n",
    "   - Diminuez le nombre de documents récupérés\n",
    "   - Surveillez l'onglet Monitoring\n",
    "\n",
    "4. **Documents non traités** :\n",
    "   - Vérifiez le format (PDF, DOCX, TXT)\n",
    "   - Assurez-vous que le fichier contient du texte\n",
    "   - Vérifiez la taille (max 10MB)\n",
    "\n",
    "## 🎉 Fonctionnalités futures\n",
    "\n",
    "- 🌐 **Support multilingue** avancé\n",
    "- 🔗 **Intégration d'APIs** externes\n",
    "- 📱 **Interface mobile** responsive\n",
    "- 🤖 **Agent autonome** pour l'analyse\n",
    "- 📊 **Tableaux de bord** avancés\n",
    "- 💾 **Base de données** persistante\n",
    "- 🔄 **Synchronisation** multi-utilisateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 Structure finale du projet :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "document-chatbot/\n",
    "├── app/\n",
    "│   ├── main.py                    # Version originale\n",
    "│   ├── main_enhanced.py           # Version améliorée avec utilitaires\n",
    "│   ├── config.py\n",
    "│   └── components/\n",
    "│       ├── file_processor.py\n",
    "│       ├── vectorstore.py         # Version originale\n",
    "│       ├── vectorstore_enhanced.py # Version améliorée\n",
    "│       ├── chat_engine.py         # Version originale\n",
    "│       ├── chat_engine_enhanced.py # Version améliorée\n",
    "│       └── memory.py\n",
    "├── utils/\n",
    "│   ├── __init__.py               # ✅ Créé\n",
    "│   ├── text_splitter.py          # ✅ Créé\n",
    "│   ├── prompt_templates.py       # ✅ Créé\n",
    "│   ├── validators.py             # ✅ Créé\n",
    "│   └── performance.py            # ✅ Créé\n",
    "├── data/\n",
    "│   ├── uploads/\n",
    "│   ├── vectorstore/\n",
    "│   └── exports/                  # Nouveau : exports de session\n",
    "├── requirements.txt\n",
    "├── .env.example\n",
    "└── README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app/components/vectorstore_enhanced.py\n",
    "# Version améliorée avec les utilitaires\n",
    "\n",
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "from app.config import Config\n",
    "from utils.text_splitter import SmartTextSplitter\n",
    "from utils.validators import InputValidator\n",
    "from utils.performance import PerformanceMonitor\n",
    "\n",
    "class EnhancedVectorStoreManager:\n",
    "    \"\"\"Version améliorée du gestionnaire de base vectorielle avec utilitaires\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = None\n",
    "        self.vectorstore = None\n",
    "        self.text_splitter = SmartTextSplitter()\n",
    "        self.performance_monitor = PerformanceMonitor()\n",
    "        self._setup_embeddings()\n",
    "    \n",
    "    def _setup_embeddings(self):\n",
    "        \"\"\"Configure les embeddings avec monitoring\"\"\"\n",
    "        @self.performance_monitor.measure_performance(\"setup_embeddings\")\n",
    "        def _setup():\n",
    "            model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs={'device': 'cpu'},\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "        \n",
    "        _setup()\n",
    "    \n",
    "    @property\n",
    "    def performance_metrics(self):\n",
    "        \"\"\"Accès aux métriques de performance\"\"\"\n",
    "        return self.performance_monitor.get_metrics_summary()\n",
    "    \n",
    "    def initialize_vectorstore(self, collection_name: str = Config.COLLECTION_NAME):\n",
    "        \"\"\"Initialise la base vectorielle avec monitoring\"\"\"\n",
    "        @self.performance_monitor.measure_performance(\"initialize_vectorstore\")\n",
    "        def _initialize():\n",
    "            os.makedirs(Config.CHROMADB_PATH, exist_ok=True)\n",
    "            client = chromadb.PersistentClient(path=Config.CHROMADB_PATH)\n",
    "            \n",
    "            try:\n",
    "                client.delete_collection(collection_name)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "            self.vectorstore = Chroma(\n",
    "                client=client,\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=self.embeddings,\n",
    "            )\n",
    "        \n",
    "        _initialize()\n",
    "    \n",
    "    def add_documents_enhanced(self, \n",
    "                             text: str, \n",
    "                             filename: str, \n",
    "                             chunk_size: int, \n",
    "                             chunk_overlap: int) -> Tuple[int, dict]:\n",
    "        \"\"\"\n",
    "        Version améliorée d'ajout de documents avec validation et monitoring\n",
    "        Returns: (nombre_chunks, informations_détaillées)\n",
    "        \"\"\"\n",
    "        # Validation des paramètres\n",
    "        valid_chunk, chunk_error = InputValidator.validate_chunk_size(chunk_size)\n",
    "        if not valid_chunk:\n",
    "            raise ValueError(f\"Chunk size invalide: {chunk_error}\")\n",
    "        \n",
    "        @self.performance_monitor.measure_performance(\"add_documents\")\n",
    "        def _add_documents():\n",
    "            if not self.vectorstore:\n",
    "                raise ValueError(\"VectorStore non initialisé\")\n",
    "            \n",
    "            # Découpage intelligent avec métadonnées enrichies\n",
    "            documents = self.text_splitter.split_documents_with_metadata(\n",
    "                text, filename, chunk_size, chunk_overlap\n",
    "            )\n",
    "            \n",
    "            # Ajout à la base vectorielle\n",
    "            self.vectorstore.add_documents(documents)\n",
    "            \n",
    "            # Informations détaillées sur le traitement\n",
    "            doc_info = {\n",
    "                \"total_chunks\": len(documents),\n",
    "                \"document_type\": documents[0].metadata.get(\"document_type\", \"unknown\") if documents else \"unknown\",\n",
    "                \"average_chunk_size\": sum(len(doc.page_content) for doc in documents) / len(documents) if documents else 0,\n",
    "                \"keywords_extracted\": any(\"keywords\" in doc.metadata for doc in documents),\n",
    "                \"structure_preserved\": sum(1 for doc in documents if doc.metadata.get(\"contains_structure\", False))\n",
    "            }\n",
    "            \n",
    "            return len(documents), doc_info\n",
    "        \n",
    "        return _add_documents()\n",
    "    \n",
    "    def search_with_metadata(self, query: str, k: int = Config.DEFAULT_K_DOCUMENTS) -> List[dict]:\n",
    "        \"\"\"Recherche avec métadonnées détaillées\"\"\"\n",
    "        # Validation de la requête\n",
    "        valid_query, query_error = InputValidator.validate_question(query)\n",
    "        if not valid_query:\n",
    "            raise ValueError(f\"Requête invalide: {query_error}\")\n",
    "        \n",
    "        @self.performance_monitor.measure_performance(\"search_documents\")\n",
    "        def _search():\n",
    "            if not self.vectorstore:\n",
    "                return []\n",
    "            \n",
    "            documents = self.vectorstore.similarity_search(query, k=k)\n",
    "            \n",
    "            # Enrichissement des résultats avec métadonnées\n",
    "            enriched_results = []\n",
    "            for doc in documents:\n",
    "                result = {\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata,\n",
    "                    \"relevance_score\": getattr(doc, 'relevance_score', None),\n",
    "                    \"chunk_info\": {\n",
    "                        \"position\": doc.metadata.get(\"chunk_position\", \"unknown\"),\n",
    "                        \"total_chunks\": doc.metadata.get(\"total_chunks\", 0),\n",
    "                        \"chunk_id\": doc.metadata.get(\"chunk_id\", 0),\n",
    "                        \"has_structure\": doc.metadata.get(\"contains_structure\", False),\n",
    "                        \"keywords\": doc.metadata.get(\"keywords\", [])\n",
    "                    }\n",
    "                }\n",
    "                enriched_results.append(result)\n",
    "            \n",
    "            return enriched_results\n",
    "        \n",
    "        return _search()\n",
    "\n",
    "# app/components/chat_engine_enhanced.py\n",
    "# Version améliorée du moteur de chat\n",
    "\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from app.components.vectorstore_enhanced import EnhancedVectorStoreManager\n",
    "from app.components.memory import ConversationMemory\n",
    "from utils.prompt_templates import PromptTemplateManager\n",
    "from utils.validators import InputValidator\n",
    "from utils.performance import PerformanceMonitor\n",
    "\n",
    "class EnhancedChatEngine:\n",
    "    \"\"\"Moteur de conversation amélioré avec utilitaires avancés\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.vectorstore_manager = EnhancedVectorStoreManager()\n",
    "        self.memory = ConversationMemory()\n",
    "        self.prompt_manager = PromptTemplateManager()\n",
    "        self.performance_monitor = PerformanceMonitor()\n",
    "        self.chain = None\n",
    "        self.current_template_type = 'general'\n",
    "    \n",
    "    def setup_llm(self, api_key: str, model: str = \"mistral-tiny\"):\n",
    "        \"\"\"Configuration du LLM avec validation\"\"\"\n",
    "        # Validation de la clé API\n",
    "        valid_key, key_error = InputValidator.validate_api_key(api_key, 'mistral')\n",
    "        if not valid_key:\n",
    "            raise ValueError(f\"Clé API invalide: {key_error}\")\n",
    "        \n",
    "        @self.performance_monitor.measure_performance(\"setup_llm\")\n",
    "        def _setup():\n",
    "            self.llm = ChatMistralAI(\n",
    "                mistral_api_key=api_key,\n",
    "                model=model,\n",
    "                temperature=0.1,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "        \n",
    "        _setup()\n",
    "    \n",
    "    def setup_chain_with_template(self, \n",
    "                                 k_documents: int = 3, \n",
    "                                 template_type: str = 'auto'):\n",
    "        \"\"\"Configuration de la chaîne avec template intelligent\"\"\"\n",
    "        if not self.llm:\n",
    "            raise ValueError(\"LLM non configuré\")\n",
    "        \n",
    "        if not self.vectorstore_manager.vectorstore:\n",
    "            raise ValueError(\"VectorStore non initialisé\")\n",
    "        \n",
    "        # Validation des paramètres\n",
    "        valid_k, k_error = InputValidator.validate_k_documents(k_documents)\n",
    "        if not valid_k:\n",
    "            raise ValueError(f\"Paramètre k invalide: {k_error}\")\n",
    "        \n",
    "        @self.performance_monitor.measure_performance(\"setup_chain\")\n",
    "        def _setup():\n",
    "            # Détection automatique du type de template si nécessaire\n",
    "            if template_type == 'auto':\n",
    "                # Récupérer le type de document depuis les métadonnées\n",
    "                sample_docs = self.vectorstore_manager.search_with_metadata(\"test\", k=1)\n",
    "                if sample_docs:\n",
    "                    document_type = sample_docs[0]['metadata'].get('document_type', 'general')\n",
    "                    template_type_final = document_type\n",
    "                else:\n",
    "                    template_type_final = 'general'\n",
    "            else:\n",
    "                template_type_final = template_type\n",
    "            \n",
    "            # Sélection du template optimisé\n",
    "            prompt_template = self.prompt_manager.get_optimized_template(\n",
    "                document_type=template_type_final,\n",
    "                query_type='general'\n",
    "            )\n",
    "            \n",
    "            self.current_template_type = template_type_final\n",
    "            \n",
    "            # Configuration de la mémoire\n",
    "            memory = ConversationBufferMemory(\n",
    "                memory_key=\"chat_history\",\n",
    "                return_messages=True,\n",
    "                output_key=\"answer\"\n",
    "            )\n",
    "            \n",
    "            # Création de la chaîne\n",
    "            self.chain = ConversationalRetrievalChain.from_llm(\n",
    "                llm=self.llm,\n",
    "                retriever=self.vectorstore_manager.get_retriever(k=k_documents),\n",
    "                memory=memory,\n",
    "                return_source_documents=True,\n",
    "                combine_docs_chain_kwargs={\"prompt\": prompt_template}\n",
    "            )\n",
    "        \n",
    "        _setup()\n",
    "    \n",
    "    def process_question_enhanced(self, question: str) -> Tuple[str, List[str], Dict]:\n",
    "        \"\"\"\n",
    "        Traitement avancé des questions avec métadonnées enrichies\n",
    "        Returns: (answer, sources, metadata)\n",
    "        \"\"\"\n",
    "        # Validation et nettoyage de la question\n",
    "        valid_question, question_error = InputValidator.validate_question(question)\n",
    "        if not valid_question:\n",
    "            return f\"Question invalide: {question_error}\", [], {}\n",
    "        \n",
    "        question = InputValidator.sanitize_input(question)\n",
    "        \n",
    "        @self.performance_monitor.measure_performance(\"process_question\")\n",
    "        def _process():\n",
    "            if not self.chain:\n",
    "                return \"Erreur: Système non configuré\", [], {}\n",
    "            \n",
    "            try:\n",
    "                # Exécution de la chaîne\n",
    "                result = self.chain({\"question\": question})\n",
    "                \n",
    "                answer = result.get(\"answer\", \"Pas de réponse générée\")\n",
    "                \n",
    "                # Extraction enrichie des sources\n",
    "                sources = []\n",
    "                source_metadata = []\n",
    "                \n",
    "                if \"source_documents\" in result:\n",
    "                    for doc in result[\"source_documents\"]:\n",
    "                        filename = doc.metadata.get(\"filename\", \"Document\")\n",
    "                        chunk_id = doc.metadata.get(\"chunk_id\", 0)\n",
    "                        chunk_position = doc.metadata.get(\"chunk_position\", \"unknown\")\n",
    "                        keywords = doc.metadata.get(\"keywords\", [])\n",
    "                        \n",
    "                        source_label = f\"{filename} (section {chunk_id + 1})\"\n",
    "                        sources.append(source_label)\n",
    "                        \n",
    "                        source_metadata.append({\n",
    "                            \"filename\": filename,\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"position\": chunk_position,\n",
    "                            \"keywords\": keywords,\n",
    "                            \"content_preview\": doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "                        })\n",
    "                \n",
    "                # Métadonnées de la réponse\n",
    "                response_metadata = {\n",
    "                    \"template_used\": self.current_template_type,\n",
    "                    \"sources_count\": len(sources),\n",
    "                    \"source_details\": source_metadata,\n",
    "                    \"performance\": self.performance_monitor.get_metrics_summary(\"process_question\", last_n_minutes=1)\n",
    "                }\n",
    "                \n",
    "                # Sauvegarde dans l'historique\n",
    "                self.memory.add_exchange(question, answer, sources)\n",
    "                \n",
    "                return answer, sources, response_metadata\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Erreur lors du traitement: {str(e)}\"\n",
    "                return error_msg, [], {\"error\": str(e)}\n",
    "        \n",
    "        return _process()\n",
    "    \n",
    "    def get_system_status(self) -> Dict:\n",
    "        \"\"\"Retourne le statut détaillé du système\"\"\"\n",
    "        return {\n",
    "            \"llm_configured\": self.llm is not None,\n",
    "            \"vectorstore_ready\": self.vectorstore_manager.vectorstore is not None,\n",
    "            \"chain_ready\": self.chain is not None,\n",
    "            \"current_template\": self.current_template_type,\n",
    "            \"conversation_length\": len(self.memory.conversation_history),\n",
    "            \"performance_summary\": self.performance_monitor.get_metrics_summary(),\n",
    "            \"system_info\": self.performance_monitor.get_system_info(),\n",
    "            \"available_templates\": self.prompt_manager.get_available_templates()\n",
    "        }\n",
    "    \n",
    "    def switch_template(self, new_template_type: str) -> bool:\n",
    "        \"\"\"Change le template de prompt en cours d'exécution\"\"\"\n",
    "        try:\n",
    "            available_templates = self.prompt_manager.get_available_templates()\n",
    "            if new_template_type not in available_templates:\n",
    "                return False\n",
    "            \n",
    "            # Reconfigurer la chaîne avec le nouveau template\n",
    "            if self.chain:\n",
    "                k_current = getattr(self.chain.retriever, 'search_kwargs', {}).get('k', 3)\n",
    "                self.setup_chain_with_template(k_current, new_template_type)\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "# app/main_enhanced.py\n",
    "# Version améliorée de l'application principale\n",
    "\n",
    "import gradio as gr\n",
    "import os\n",
    "from typing import Optional, Tuple, List\n",
    "import json\n",
    "\n",
    "from app.components.file_processor import FileProcessor\n",
    "from app.components.chat_engine_enhanced import EnhancedChatEngine\n",
    "from app.config import Config\n",
    "from utils.validators import FileValidator, InputValidator\n",
    "from utils.performance import PerformanceMonitor\n",
    "\n",
    "class EnhancedDocumentChatBot:\n",
    "    \"\"\"Application principale améliorée avec tous les utilitaires\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chat_engine = EnhancedChatEngine()\n",
    "        self.global_monitor = PerformanceMonitor()\n",
    "        self.current_document = None\n",
    "        self.document_processed = False\n",
    "        self.system_status = {}\n",
    "    \n",
    "    def process_document_enhanced(self, \n",
    "                                api_key: str, \n",
    "                                file_obj, \n",
    "                                chunk_size: int, \n",
    "                                k_documents: int,\n",
    "                                template_type: str = 'auto') -> Tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Version améliorée du traitement de documents\n",
    "        Returns: (status_message, document_info, performance_info)\n",
    "        \"\"\"\n",
    "        @self.global_monitor.measure_performance(\"full_document_processing\")\n",
    "        def _process():\n",
    "            try:\n",
    "                # Validation du fichier\n",
    "                file_valid, file_error = FileValidator.validate_file(file_obj)\n",
    "                if not file_valid:\n",
    "                    return f\"❌ {file_error}\", \"\", \"\"\n",
    "                \n",
    "                # Validation de la clé API\n",
    "                api_valid, api_error = InputValidator.validate_api_key(api_key.strip(), 'mistral')\n",
    "                if not api_valid:\n",
    "                    return f\"❌ {api_error}\", \"\", \"\"\n",
    "                \n",
    "                # Configuration du LLM\n",
    "                self.chat_engine.setup_llm(api_key.strip())\n",
    "                \n",
    "                # Traitement du fichier\n",
    "                text_content, filename = FileProcessor.process_uploaded_file(file_obj)\n",
    "                \n",
    "                # Initialisation de la base vectorielle\n",
    "                self.chat_engine.vectorstore_manager.initialize_vectorstore()\n",
    "                \n",
    "                # Ajout des documents avec métadonnées enrichies\n",
    "                chunk_overlap = max(50, chunk_size // 5)\n",
    "                num_chunks, doc_details = self.chat_engine.vectorstore_manager.add_documents_enhanced(\n",
    "                    text_content, filename, chunk_size, chunk_overlap\n",
    "                )\n",
    "                \n",
    "                # Configuration de la chaîne avec template intelligent\n",
    "                self.chat_engine.setup_chain_with_template(k_documents, template_type)\n",
    "                \n",
    "                # Mise à jour de l'état\n",
    "                self.current_document = filename\n",
    "                self.document_processed = True\n",
    "                self.system_status = self.chat_engine.get_system_status()\n",
    "                \n",
    "                # Messages de statut\n",
    "                status = f\"✅ Document traité avec succès!\"\n",
    "                \n",
    "                doc_info = f\"\"\"\n",
    "### 📄 Informations du document\n",
    "- **Fichier**: {filename}\n",
    "- **Type détecté**: {doc_details['document_type']}\n",
    "- **Chunks créés**: {num_chunks}\n",
    "- **Taille moyenne des chunks**: {doc_details['average_chunk_size']:.0f} caractères\n",
    "- **Structure préservée**: {doc_details['structure_preserved']} sections\n",
    "- **Mots-clés extraits**: {'✅' if doc_details['keywords_extracted'] else '❌'}\n",
    "\n",
    "### ⚙️ Configuration\n",
    "- **Template utilisé**: {self.chat_engine.current_template_type}\n",
    "- **Documents récupérés**: {k_documents}\n",
    "- **Modèle**: Mistral AI (mistral-tiny)\n",
    "                \"\"\".strip()\n",
    "                \n",
    "                # Informations de performance\n",
    "                perf_summary = self.global_monitor.get_metrics_summary()\n",
    "                system_info = self.global_monitor.get_system_info()\n",
    "                \n",
    "                perf_info = f\"\"\"\n",
    "### 📊 Performance\n",
    "- **Temps de traitement**: {perf_summary.get('avg_duration', 0):.2f}s\n",
    "- **Utilisation mémoire**: {system_info['memory_percent']:.1f}%\n",
    "- **CPU**: {system_info['cpu_percent']:.1f}%\n",
    "- **Taux de succès**: {perf_summary.get('success_rate', 100):.1f}%\n",
    "                \"\"\".strip()\n",
    "                \n",
    "                return status, doc_info, perf_info\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"❌ Erreur: {str(e)}\"\n",
    "                return error_msg, \"\", \"\"\n",
    "        \n",
    "        return _process()\n",
    "    \n",
    "    def chat_enhanced(self, \n",
    "                     message: str, \n",
    "                     history: List[List[str]]) -> Tuple[str, List[List[str]], str]:\n",
    "        \"\"\"\n",
    "        Version améliorée du chat avec métadonnées\n",
    "        Returns: (\"\", updated_history, metadata_info)\n",
    "        \"\"\"\n",
    "        if not self.document_processed:\n",
    "            history.append([message, \"⚠️ Veuillez d'abord traiter un document dans l'onglet Configuration.\"])\n",
    "            return \"\", history, \"\"\n",
    "        \n",
    "        if not message.strip():\n",
    "            return \"\", history, \"\"\n",
    "        \n",
    "        # Traitement amélioré de la question\n",
    "        answer, sources, metadata = self.chat_engine.process_question_enhanced(message)\n",
    "        \n",
    "        # Formatage de la réponse avec sources détaillées\n",
    "        if sources:\n",
    "            source_details = []\n",
    "            for i, source in enumerate(sources):\n",
    "                if i < len(metadata.get('source_details', [])):\n",
    "                    detail = metadata['source_details'][i]\n",
    "                    keywords_str = ', '.join(detail['keywords'][:3]) if detail['keywords'] else 'N/A'\n",
    "                    source_details.append(f\"📄 {source} | 🏷️ Mots-clés: {keywords_str}\")\n",
    "                else:\n",
    "                    source_details.append(f\"📄 {source}\")\n",
    "            \n",
    "            formatted_answer = f\"{answer}\\n\\n**Sources consultées:**\\n\" + \"\\n\".join(source_details)\n",
    "        else:\n",
    "            formatted_answer = answer\n",
    "        \n",
    "        # Informations de métadonnées pour affichage\n",
    "        metadata_info = f\"\"\"\n",
    "**Template**: {metadata.get('template_used', 'N/A')} | **Sources**: {metadata.get('sources_count', 0)} | **Performance**: {metadata.get('performance', {}).get('avg_duration', 0):.2f}s\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        # Mise à jour de l'historique\n",
    "        history.append([message, formatted_answer])\n",
    "        \n",
    "        return \"\", history, metadata_info\n",
    "    \n",
    "    def get_detailed_status(self) -> str:\n",
    "        \"\"\"Retourne un statut détaillé du système\"\"\"\n",
    "        if not self.document_processed:\n",
    "            return \"🔴 **Statut**: Aucun document traité\"\n",
    "        \n",
    "        status = self.chat_engine.get_system_status()\n",
    "        \n",
    "        status_text = f\"\"\"\n",
    "### 🟢 Système opérationnel\n",
    "\n",
    "**Configuration actuelle:**\n",
    "- Template: {status['current_template']}\n",
    "- Conversations: {status['conversation_length']} échanges\n",
    "- Performance moyenne: {status['performance_summary'].get('avg_duration', 0):.2f}s\n",
    "\n",
    "**Ressources système:**\n",
    "- Mémoire: {status['system_info']['memory_percent']:.1f}%\n",
    "- CPU: {status['system_info']['cpu_percent']:.1f}%\n",
    "- Espace disque: {status['system_info']['disk_usage_percent']:.1f}%\n",
    "\n",
    "**Templates disponibles:** {', '.join(status['available_templates'])}\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        return status_text\n",
    "    \n",
    "    def export_session_data(self) -> str:\n",
    "        \"\"\"Exporte les données de la session\"\"\"\n",
    "        if not self.document_processed:\n",
    "            return \"Aucune donnée à exporter\"\n",
    "        \n",
    "        try:\n",
    "            # Données à exporter\n",
    "            session_data = {\n",
    "                \"document\": self.current_document,\n",
    "                \"system_status\": self.chat_engine.get_system_status(),\n",
    "                \"conversation_history\": self.chat_engine.memory.conversation_history,\n",
    "                \"performance_metrics\": self.global_monitor.get_metrics_summary()\n",
    "            }\n",
    "            \n",
    "            # Sauvegarde dans un fichier\n",
    "            filename = f\"session_export_{self.current_document}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(session_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "            \n",
    "            return f\"✅ Session exportée: {filename}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"❌ Erreur d'export: {str(e)}\"\n",
    "\n",
    "# Integration dans l'interface Gradio\n",
    "def create_enhanced_interface():\n",
    "    \"\"\"Crée l'interface Gradio améliorée\"\"\"\n",
    "    \n",
    "    app = EnhancedDocumentChatBot()\n",
    "    \n",
    "    # CSS amélioré\n",
    "    css = \"\"\"\n",
    "    .gradio-container {\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        max-width: 1200px;\n",
    "        margin: 0 auto;\n",
    "    }\n",
    "    .tab-nav button {\n",
    "        font-size: 16px;\n",
    "        font-weight: 500;\n",
    "    }\n",
    "    .performance-info {\n",
    "        background: #f0f0f0;\n",
    "        padding: 10px;\n",
    "        border-radius: 5px;\n",
    "        font-size: 12px;\n",
    "        color: #666;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(css=css, title=\"ChatBot Documentaire Avancé - Mistral AI\", theme=gr.themes.Soft()) as interface:\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        # 📄 ChatBot Documentaire Avancé avec Mistral AI\n",
    "        \n",
    "        **Nouvelle version** avec découpage intelligent, templates optimisés et monitoring de performance !\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            # Onglet Configuration Avancé\n",
    "            with gr.Tab(\"⚙️ Configuration Avancée\", id=\"config\"):\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        api_key_input = gr.Textbox(\n",
    "                            label=\"🔑 Clé API Mistral AI\",\n",
    "                            placeholder=\"Saisissez votre clé API Mistral...\",\n",
    "                            type=\"password\",\n",
    "                            info=\"Validation automatique de la clé\"\n",
    "                        )\n",
    "                        \n",
    "                        file_upload = gr.File(\n",
    "                            label=\"📁 Fichier à analyser\",\n",
    "                            file_types=[\".pdf\", \".docx\", \".txt\"],\n",
    "                            type=\"binary\"\n",
    "                        )\n",
    "                        \n",
    "                        with gr.Row():\n",
    "                            chunk_size_slider = gr.Slider(\n",
    "                                minimum=200,\n",
    "                                maximum=2000,\n",
    "                                value=Config.DEFAULT_CHUNK_SIZE,\n",
    "                                step=100,\n",
    "                                label=\"📏 Taille des chunks\",\n",
    "                                info=\"Découpage intelligent automatique\"\n",
    "                            )\n",
    "                            \n",
    "                            k_documents_slider = gr.Slider(\n",
    "                                minimum=1,\n",
    "                                maximum=10,\n",
    "                                value=Config.DEFAULT_K_DOCUMENTS,\n",
    "                                step=1,\n",
    "                                label=\"🔍 Documents récupérés\"\n",
    "                            )\n",
    "                        \n",
    "                        template_dropdown = gr.Dropdown(\n",
    "                            choices=['auto', 'general', 'academic', 'technical', 'legal'],\n",
    "                            value='auto',\n",
    "                            label=\"🎯 Type de template\",\n",
    "                            info=\"Auto = détection automatique\"\n",
    "                        )\n",
    "                        \n",
    "                        process_btn = gr.Button(\"🚀 Traiter le document\", variant=\"primary\", size=\"lg\")\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        status_output = gr.Textbox(\n",
    "                            label=\"📊 Statut\",\n",
    "                            interactive=False,\n",
    "                            lines=2\n",
    "                        )\n",
    "                        \n",
    "                        doc_info_output = gr.Markdown(\n",
    "                            label=\"ℹ️ Informations détaillées\"\n",
    "                        )\n",
    "                        \n",
    "                        perf_info_output = gr.Markdown(\n",
    "                            label=\"⚡ Performance\",\n",
    "                            elem_classes=[\"performance-info\"]\n",
    "                        )\n",
    "            \n",
    "            # Onglet Chat Amélioré\n",
    "            with gr.Tab(\"💬 Chat Intelligent\", id=\"chat\"):\n",
    "                \n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"Conversation avec intelligence augmentée\",\n",
    "                    height=500,\n",
    "                    show_copy_button=True\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    msg_input = gr.Textbox(\n",
    "                        label=\"Votre question\",\n",
    "                        placeholder=\"Question intelligente avec validation automatique...\",\n",
    "                        scale=4\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        send_btn = gr.Button(\"📤 Envoyer\", variant=\"primary\")\n",
    "                        clear_btn = gr.Button(\"🗑️ Effacer\", variant=\"secondary\")\n",
    "                \n",
    "                # Informations de métadonnées en temps réel\n",
    "                metadata_display = gr.Textbox(\n",
    "                    label=\"🔍 Métadonnées de la réponse\",\n",
    "                    interactive=False,\n",
    "                    lines=1,\n",
    "                    elem_classes=[\"performance-info\"]\n",
    "                )\n",
    "            \n",
    "            # Onglet Monitoring\n",
    "            with gr.Tab(\"📊 Monitoring\", id=\"monitoring\"):\n",
    "                \n",
    "                with gr.Row():\n",
    "                    refresh_btn = gr.Button(\"🔄 Actualiser\", variant=\"secondary\")\n",
    "                    export_btn = gr.Button(\"💾 Exporter Session\", variant=\"primary\")\n",
    "                \n",
    "                system_status_display = gr.Markdown(\n",
    "                    label=\"Statut du système\",\n",
    "                    value=\"Aucune donnée disponible\"\n",
    "                )\n",
    "                \n",
    "                export_result_display = gr.Textbox(\n",
    "                    label=\"Résultat d'export\",\n",
    "                    interactive=False\n",
    "                )\n",
    "        \n",
    "        # Événements\n",
    "        process_btn.click(\n",
    "            fn=app.process_document_enhanced,\n",
    "            inputs=[api_key_input, file_upload, chunk_size_slider, k_documents_slider, template_dropdown],\n",
    "            outputs=[status_output, doc_info_output, perf_info_output]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            fn=app.chat_enhanced,\n",
    "            inputs=[msg_input, chatbot],\n",
    "            outputs=[msg_input, chatbot, metadata_display]\n",
    "        )\n",
    "        \n",
    "        msg_input.submit(\n",
    "            fn=app.chat_enhanced,\n",
    "            inputs=[msg_input, chatbot],\n",
    "            outputs=[msg_input, chatbot, metadata_display]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            fn=lambda: ([], \"\"),\n",
    "            outputs=[chatbot, metadata_display]\n",
    "        )\n",
    "        \n",
    "        refresh_btn.click(\n",
    "            fn=app.get_detailed_status,\n",
    "            outputs=[system_status_display]\n",
    "        )\n",
    "        \n",
    "        export_btn.click(\n",
    "            fn=app.export_session_data,\n",
    "            outputs=[export_result_display]\n",
    "        )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from datetime import datetime\n",
    "    os.makedirs(\"./data/uploads\", exist_ok=True)\n",
    "    os.makedirs(\"./data/vectorstore\", exist_ok=True)\n",
    "    \n",
    "    interface = create_enhanced_interface()\n",
    "    interface.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "        share=False,\n",
    "        debug=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚀 Intégration complète des utilitaires !\n",
    "Voici comment les utilitaires sont intégrés dans votre application :\n",
    "✨ Nouvelles fonctionnalités ajoutées :\n",
    "1. Découpage intelligent (SmartTextSplitter)\n",
    "\n",
    "Détection automatique du type de document (académique, technique, légal)\n",
    "Préservation de structure avec mots-clés extraits\n",
    "Métadonnées enrichies pour chaque chunk\n",
    "\n",
    "2. Templates de prompts optimisés (PromptTemplateManager)\n",
    "\n",
    "6 templates spécialisés selon le type de contenu\n",
    "Sélection automatique ou manuelle\n",
    "Optimisation contextuelle des réponses\n",
    "\n",
    "3. Validation robuste (Validators)\n",
    "\n",
    "Validation des fichiers : taille, format, sécurité\n",
    "Validation des entrées : clés API, paramètres, questions\n",
    "Nettoyage automatique des inputs utilisateur\n",
    "\n",
    "4. Monitoring de performance (PerformanceMonitor)\n",
    "\n",
    "Métriques en temps réel : temps, mémoire, CPU\n",
    "Historique des performances avec export\n",
    "Surveillance système automatique\n",
    "\n",
    "🆕 Interface améliorée :\n",
    "Nouveaux onglets :\n",
    "\n",
    "Configuration Avancée : sélection de templates, validation en temps réel\n",
    "Chat Intelligent : métadonnées des réponses, validation des questions\n",
    "Monitoring : statut système, export de session\n",
    "\n",
    "Nouvelles informations affichées :\n",
    "\n",
    "Type de document détecté automatiquement\n",
    "Template de prompt utilisé\n",
    "Mots-clés extraits des chunks\n",
    "Métriques de performance en temps réel\n",
    "Sources détaillées avec prévisualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "document-chatbot/\n",
    "├── app/\n",
    "│   ├── main.py                    # Version originale\n",
    "│   ├── main_enhanced.py           # Version améliorée avec utilitaires\n",
    "│   ├── config.py\n",
    "│   └── components/\n",
    "│       ├── file_processor.py\n",
    "│       ├── vectorstore.py         # Version originale\n",
    "│       ├── vectorstore_enhanced.py # Version améliorée\n",
    "│       ├── chat_engine.py         # Version originale\n",
    "│       ├── chat_engine_enhanced.py # Version améliorée\n",
    "│       └── memory.py\n",
    "├── utils/\n",
    "│   ├── __init__.py               # ✅ Créé\n",
    "│   ├── text_splitter.py          # ✅ Créé\n",
    "│   ├── prompt_templates.py       # ✅ Créé\n",
    "│   ├── validators.py             # ✅ Créé\n",
    "│   └── performance.py            # ✅ Créé\n",
    "├── data/\n",
    "│   ├── uploads/\n",
    "│   ├── vectorstore/\n",
    "│   └── exports/                  # Nouveau : exports de session\n",
    "├── requirements.txt\n",
    "├── .env.example\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "## 🔧 **Installation et utilisation :**## 🎯 **Résumé de votre projet complet**\n",
    "\n",
    "Vous avez maintenant **deux versions** de votre ChatBot documentaire :\n",
    "\n",
    "### 📱 **Version Simple** (`main.py`)\n",
    "- Interface Gradio basique\n",
    "- Fonctionnalités essentielles\n",
    "- Facile à déployer et utiliser\n",
    "\n",
    "### 🚀 **Version Avancée** (`main_enhanced.py`)\n",
    "- **Découpage intelligent** avec détection automatique du type\n",
    "- **Templates optimisés** pour chaque contexte\n",
    "- **Validation robuste** et sécurité renforcée\n",
    "- **Monitoring complet** des performances\n",
    "- **Interface enrichie** avec 3 onglets spécialisés\n",
    "\n",
    "## 🔥 **Points forts de la version avancée :**\n",
    "\n",
    "1. **🧠 Intelligence augmentée** : détection automatique + templates spécialisés\n",
    "2. **⚡ Performance optimisée** : monitoring en temps réel + métriques détaillées  \n",
    "3. **🔒 Sécurité renforcée** : validation complète + nettoyage automatique\n",
    "4. **📊 Observabilité totale** : export de sessions + historique complet\n",
    "5. **🎯 Réponses de qualité** : adaptation contextuelle + sources enrichies\n",
    "\n",
    "## 🚀 **Pour commencer :**\n",
    "\n",
    "1. **Choisissez votre version** (simple ou avancée)\n",
    "2. **Installez les dépendances** avec pip\n",
    "3. **Obtenez votre clé Mistral AI** (gratuite)\n",
    "4. **Lancez l'application** : `python app/main_enhanced.py`\n",
    "5. **Accédez à** : `http://localhost:7860`\n",
    "\n",
    "Votre ChatBot documentaire est maintenant **prêt à rivaliser avec SecInsights.ai** ! 🎉\n",
    "\n",
    "Avez-vous des questions sur l'installation ou voulez-vous que j'ajoute d'autres fonctionnalités spécifiques ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
